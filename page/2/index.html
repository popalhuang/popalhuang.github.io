<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Popal&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="技術之路學習歷程">
<meta property="og:type" content="website">
<meta property="og:title" content="Popal's Blog">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Popal's Blog">
<meta property="og:description" content="技術之路學習歷程">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Popal's Blog">
<meta name="twitter:description" content="技術之路學習歷程">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Popal&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">邊做邊學</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首頁</a>
        
          <a class="main-nav-link" href="/archives">文章</a>
        
          <a class="main-nav-link" href="/categories/hadoop">Hadoop</a>
        
          <a class="main-nav-link" href="/about">關於我</a>
        
      </nav>
      <nav id="sub-nav">
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-[hadoop]hadoop3.0.0_nfs_gateway" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/08/[hadoop]hadoop3.0.0_nfs_gateway/" class="article-date">
  <time datetime="2018-01-07T16:00:00.000Z" itemprop="datePublished">2018-01-08</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/hadoop/">hadoop</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/08/[hadoop]hadoop3.0.0_nfs_gateway/">Hadoop3.0.0 NFS Gateway</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
<div id="toc">
  <ol class="toc"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop3-0-0-NFS-Gateway-setting-amp-start"><span class="toc-text">Hadoop3.0.0 NFS Gateway setting & start</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop3-0-0-NFS-Gateway-modify-mount-point"><span class="toc-text">Hadoop3.0.0 NFS Gateway modify mount point</span></a></li></ol>
</div>

        <h5 id="Hadoop3-0-0-NFS-Gateway-setting-amp-start"><a href="#Hadoop3-0-0-NFS-Gateway-setting-amp-start" class="headerlink" title="Hadoop3.0.0 NFS Gateway setting &amp; start"></a>Hadoop3.0.0 NFS Gateway setting &amp; start</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ service nfs-kernel-server stop</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ start-dfs.sh</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ start-yarn.sh</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs --daemon start nfs3</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ rpcinfo -p</div><div class="line">   program vers proto   port  service</div><div class="line">    100000    4   tcp    111  portmapper</div><div class="line">    100000    3   tcp    111  portmapper</div><div class="line">    100000    2   tcp    111  portmapper</div><div class="line">    100000    4   udp    111  portmapper</div><div class="line">    100000    3   udp    111  portmapper</div><div class="line">    100000    2   udp    111  portmapper</div><div class="line">    100005    1   udp  46340  mountd</div><div class="line">    100005    1   tcp  33399  mountd</div><div class="line">    100005    2   udp  56322  mountd</div><div class="line">    100005    2   tcp  43065  mountd</div><div class="line">    100005    3   udp  52402  mountd</div><div class="line">    100005    3   tcp  49301  mountd</div><div class="line">    100003    2   tcp   2049  nfs</div><div class="line">    100003    3   tcp   2049  nfs</div><div class="line">    100003    4   tcp   2049  nfs</div><div class="line">    100227    2   tcp   2049</div><div class="line">    100227    3   tcp   2049</div><div class="line">    100003    2   udp   2049  nfs</div><div class="line">    100003    3   udp   2049  nfs</div><div class="line">    100003    4   udp   2049  nfs</div><div class="line">    100227    2   udp   2049</div><div class="line">    100227    3   udp   2049</div><div class="line">    100021    1   udp  50043  nlockmgr</div><div class="line">    100021    3   udp  50043  nlockmgr</div><div class="line">    100021    4   udp  50043  nlockmgr</div><div class="line">    100021    1   tcp  33247  nlockmgr</div><div class="line">    100021    3   tcp  33247  nlockmgr</div><div class="line">    100021    4   tcp  33247  nlockmgr</div><div class="line">	</div><div class="line">hadoop@hadoop-master:~$ showmount -e</div><div class="line">Export list for hadoop-master:</div><div class="line">/ *</div><div class="line">	</div><div class="line">hadoop@hadoop-master:~$ netstat -tnl</div><div class="line">Active Internet connections (only servers)</div><div class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State</div><div class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8088       0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8030       0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:50079           0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8031       0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8032       0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:2049            0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8033       0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:50090      0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:5355            0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:9870            0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:4242            0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8020       0.0.0.0:*               LISTEN</div><div class="line">tcp6       0      0 :::22                   :::*                    LISTEN</div><div class="line">tcp6       0      0 :::5355                 :::*                    LISTEN</div><div class="line">tcp6       0      0 :::111                  :::*                    LISTEN</div><div class="line"></div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ jps</div><div class="line">4116 Nfs3</div><div class="line">2392 ResourceManager</div><div class="line">2108 SecondaryNameNode</div><div class="line">4157 Jps</div><div class="line">1837 NameNode</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ mkdir -p /opt/hdfs</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ sudo mount -t nfs -o vers=3,proto=tcp,nolock,sync 192.168.51.4:/  /opt/hdfs</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ ls -la /opt/hdfs/user</div><div class="line">total 4</div><div class="line">drwxr-xr-x  6 hadoop 2584148964  192 Jan  4 16:35 .</div><div class="line">drwxr-xr-x  4 hadoop 2584148964  128 Dec 26 15:25 ..</div><div class="line">drwxr-xr-x 41 hadoop 2584148964 1312 Jan  8 17:00 hadoop</div><div class="line">drwxr-xr-x  3 hadoop 2584148964   96 Dec 26 18:44 hive</div><div class="line">drwxr-xr-x  2 hadoop 2584148964   64 Jan  3 17:30 snapshot</div><div class="line">drwxr-xr-x  2 hadoop 2584148964   64 Jan  4 16:35 vagrant</div><div class="line"></div><div class="line">hadoop@hadoop-master:/home$ sudo umount /opt/hdfs</div><div class="line">hadoop@hadoop-master:/home$ ls -la /opt/hdfs</div><div class="line">total 8</div><div class="line">drwxrwxr-x 2 hadoop hadoop 4096 Jan  8 17:32 .</div><div class="line">drwxr-xr-x 4 hadoop hadoop 4096 Jan  8 17:32 ..</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="Hadoop3-0-0-NFS-Gateway-modify-mount-point"><a href="#Hadoop3-0-0-NFS-Gateway-modify-mount-point" class="headerlink" title="Hadoop3.0.0 NFS Gateway modify mount point"></a>Hadoop3.0.0 NFS Gateway modify mount point</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">vi /bgdt/hadoop-3.0.0/etc/hadoop/hdfs-site.xml</div><div class="line"></div><div class="line">##在設定檔中加入以下參數(nfs.export.point)</div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;nfs.export.point&lt;/name&gt;</div><div class="line">  &lt;value&gt;/user&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line"></div><div class="line">##重新啟動HDFS</div><div class="line">hadoop@hadoop-master:~$start-dfs.sh</div><div class="line"></div><div class="line">##重新啟動NFS3</div><div class="line">hadoop@hadoop-master:~$hdfs --daemon start nfs3</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$showmount -e</div><div class="line">Export list for hadoop-master:</div><div class="line">/user *</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ sudo mount -t nfs -o vers=3,proto=tcp,nolock,sync 192.168.51.4:/user  /opt/hdfs</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ ls -la /opt/hdfs</div><div class="line">total 8</div><div class="line">drwxr-xr-x  6 hadoop 2584148964  192 Jan  4 16:35 .</div><div class="line">drwxr-xr-x  4 hadoop hadoop     4096 Jan  8 17:32 ..</div><div class="line">drwxr-xr-x 41 hadoop 2584148964 1312 Jan  8 17:00 hadoop</div><div class="line">drwxr-xr-x  3 hadoop 2584148964   96 Dec 26 18:44 hive</div><div class="line">drwxr-xr-x  2 hadoop 2584148964   64 Jan  3 17:30 snapshot</div><div class="line">drwxr-xr-x  2 hadoop 2584148964   64 Jan  4 16:35 vagrant</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ sudo umount /opt/hdfs</div></pre></td></tr></table></figure>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/01/08/[hadoop]hadoop3.0.0_nfs_gateway/" data-id="ckcx1jzcf000nnsh48dmj9p0a" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-[hadoop]hadoop_snapshot_cmd" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/04/[hadoop]hadoop_snapshot_cmd/" class="article-date">
  <time datetime="2018-01-03T16:00:00.000Z" itemprop="datePublished">2018-01-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/hadoop/">hadoop</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/04/[hadoop]hadoop_snapshot_cmd/">hdfs snapshot command Memo</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
<div id="toc">
  <ol class="toc"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop-Snapshot機制用到的相關語法"><span class="toc-text">Hadoop Snapshot機制用到的相關語法</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Snapshot機制操作說明"><span class="toc-text">Snapshot機制操作說明</span></a></li></ol></li></ol>
</div>

        <h5 id="Hadoop-Snapshot機制用到的相關語法"><a href="#Hadoop-Snapshot機制用到的相關語法" class="headerlink" title="Hadoop Snapshot機制用到的相關語法"></a>Hadoop Snapshot機制用到的相關語法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">hdfs lsSnapshottableDir</div><div class="line">hdfs snapshotDiff</div><div class="line">hdfs dfsadmin -allowSnapshot</div><div class="line">hdfs dfsadmin -disallowSnapshot</div><div class="line">hdfs dfs -createSnapshot</div><div class="line">hdfs dfs -deleteSnapshot</div><div class="line">hdfs dfs -renameSnapshot</div><div class="line">hdfs fsck -includeSnapshots</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="Snapshot機制操作說明"><a href="#Snapshot機制操作說明" class="headerlink" title="Snapshot機制操作說明"></a>Snapshot機制操作說明</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir /user/hadoop/snapshot</div><div class="line"></div><div class="line">##選定一個目錄做為Snapshot的起點</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -allowSnapshot /user/hadoop/snapshot</div><div class="line">Allowing snaphot on /user/hadoop/snapshot succeeded</div><div class="line"></div><div class="line">##列出所有要做Snapshot目錄</div><div class="line">hadoop@hadoop-master:~$ hdfs lsSnapshottableDir</div><div class="line">drwxr-xr-x 0 hadoop supergroup 0 2018-01-03 16:50 1 65536 /user/hadoop/snapshot</div><div class="line"></div><div class="line">##建立Snapshot</div><div class="line">hadoop@hadoop-master:~$  hadoop fs -createSnapshot /user/hadoop/snapshot snapshot</div><div class="line">Created snapshot /user/hadoop/snapshot/.snapshot/snapshot</div><div class="line"></div><div class="line">##在Snapshot目錄中新增一個檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/snapshot/test1.txt</div><div class="line">11111</div><div class="line">2222</div><div class="line">33333</div><div class="line">44444</div><div class="line">55555</div><div class="line"></div><div class="line">##再做一次Snapshot</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -createSnapshot /user/hadoop/snapshot snapshot_201801031654</div><div class="line">Created snapshot /user/hadoop/snapshot/.snapshot/snapshot_201801031654</div><div class="line"></div><div class="line">##比較兩個Snapshot之間的差異</div><div class="line">hadoop@hadoop-master:~$ hdfs snapshotDiff /user/hadoop/snapshot snapshot snapshot_201801031654</div><div class="line">Difference between snapshot snapshot and snapshot snapshot_201801031654 under directory /user/hadoop/snapshot:</div><div class="line">M       .</div><div class="line">+       ./test1.txt</div><div class="line"></div><div class="line">##以下為復原Snapshot方式</div><div class="line">hadoop@hadoop-master:~$hadoop fs -rm -r -skipTrash /user/hadoop/snapshot/*</div><div class="line">hadoop@hadoop-master:~$hadoop fs -cp /user/hadoop/snapshot/.snapshot/snapshot/* /user/hadoop/snapshot</div><div class="line"></div><div class="line"></div><div class="line">##刪除Snapshot</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir /tmp/important-data</div><div class="line">hadoop@hadoop-master:~$ echo &quot;important data&quot; | hdfs dfs -put - /tmp/important-dir/important-file.txt</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -allowSnapshot  /tmp/important-dir</div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -createSnapshot /tmp/important-dir first-snapshot</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs lsSnapshottableDir</div><div class="line">drwxr-xr-x 0 hadoop supergroup 0 2018-01-04 11:12 1 65536 /tmp/important-dir</div><div class="line">drwxr-xr-x 0 hadoop supergroup 0 2018-01-04 11:59 3 65536 /user/hadoop/snapshot</div><div class="line"></div><div class="line">##此目錄下尚有其他Snapshot存在無法操作disallowSnapshot</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -disallowSnapshot /tmp/important-dir</div><div class="line">disallowSnapshot: The directory /tmp/important-dir has snapshot(s). Please redo the operation after removing all the snapshots.</div><div class="line"></div><div class="line">##須將所有Snapshot刪除後,才能操作disallowSnapshot</div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -deleteSnapshot /tmp/important-dir first-snapshot</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -disallowSnapshot /tmp/important-dir</div><div class="line">Disallowing snaphot on /tmp/important-dir succeeded</div><div class="line">hadoop@hadoop-master:~$ hdfs lsSnapshottableDir</div><div class="line">drwxr-xr-x 0 hadoop supergroup 0 2018-01-04 11:59 3 65536 /user/hadoop/snapshot</div><div class="line"></div><div class="line">##更改Snapshot名稱,並顯示snapshottable下所有的Snapshot</div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -renameSnapshot /user/hadoop/snapshot snapshot snapshot_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/snapshot/.snapshot</div><div class="line">Found 3 items</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-03 17:01 /user/hadoop/snapshot/.snapshot/snapshot_1</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-03 17:01 /user/hadoop/snapshot/.snapshot/snapshot_201801031654</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-03 18:01 /user/hadoop/snapshot/.snapshot/snapshot_201801031737</div></pre></td></tr></table></figure>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/01/04/[hadoop]hadoop_snapshot_cmd/" data-id="ckcx1jzcy0016nsh4mz1dpgax" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-[hadoop]hadoop_archive_cmd" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/04/[hadoop]hadoop_archive_cmd/" class="article-date">
  <time datetime="2018-01-03T16:00:00.000Z" itemprop="datePublished">2018-01-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/hadoop/">hadoop</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/04/[hadoop]hadoop_archive_cmd/">hadoop archive command Memo</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
<div id="toc">
  <ol class="toc"><li class="toc-item toc-level-5"><a class="toc-link" href="#hadoop壓縮機制"><span class="toc-text">hadoop壓縮機制</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#hadoop-archive操作方式"><span class="toc-text">hadoop archive操作方式</span></a></li></ol>
</div>

        <h5 id="hadoop壓縮機制"><a href="#hadoop壓縮機制" class="headerlink" title="hadoop壓縮機制"></a>hadoop壓縮機制</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">usage: archive &lt;-archiveName &lt;NAME&gt;.har&gt; &lt;-p &lt;parent path&gt;&gt; [-r &lt;replication factor&gt;] &lt;src&gt;* &lt;dest&gt;</div><div class="line"> -archiveName &lt;arg&gt;   Name of the Archive. This is mandatory option</div><div class="line"> -help                Show the usage</div><div class="line"> -p &lt;arg&gt;             Parent path of sources. This is mandatory option</div><div class="line"> -r &lt;arg&gt;             Replication factor archive files</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hadoop-archive操作方式"><a href="#hadoop-archive操作方式" class="headerlink" title="hadoop archive操作方式"></a>hadoop archive操作方式</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/test1.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/test2.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/test3.txt</div><div class="line"></div><div class="line">##壓縮/user/hadoop/example目錄下所有檔案和目錄</div><div class="line">hadoop@hadoop-master:~$ hadoop archive -archiveName example1.har -p /user/hadoop/example -r 3 /user/hadoop</div><div class="line"></div><div class="line">##壓縮/user/hadoop/example目錄下所有txt檔</div><div class="line">hadoop@hadoop-master:~$ hadoop archive -archiveName example2.har -p /user/hadoop/example/ -r 3 *.txt /user/hadoop</div><div class="line"></div><div class="line">## 查詢壓縮檔的內容</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R har:///user/hadoop/example1.har</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-04 18:12 har:///user/hadoop/example1.har/lab_2</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-04 18:13 har:///user/hadoop/example1.har/lab_2/example1.har</div><div class="line">-rw-r--r--   3 hadoop supergroup        373 2018-01-03 15:41 har:///user/hadoop/example1.har/lab_2/test9.tar.gz</div><div class="line">-rw-r--r--   3 hadoop supergroup         23 2018-01-04 18:09 har:///user/hadoop/example1.har/test1.txt</div><div class="line">-rw-r--r--   3 hadoop supergroup         30 2018-01-04 18:10 har:///user/hadoop/example1.har/test2.txt</div><div class="line">-rw-r--r--   3 hadoop supergroup         15 2018-01-04 18:10 har:///user/hadoop/example1.har/test3.txt</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R har:///user/hadoop/example2.har</div><div class="line">-rw-r--r--   3 hadoop supergroup         23 2018-01-04 18:09 har:///user/hadoop/example2.har/test1.txt</div><div class="line">-rw-r--r--   3 hadoop supergroup         30 2018-01-04 18:10 har:///user/hadoop/example2.har/test2.txt</div><div class="line">-rw-r--r--   3 hadoop supergroup         15 2018-01-04 18:10 har:///user/hadoop/example2.har/test3.txt</div><div class="line"></div><div class="line">##解壓縮使用cp命令</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir -p /user/hadoop/example/lab_3</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cp har:///user/hadoop/example2.har/* hdfs:/user/hadoop/example/lab_3</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R /user/hadoop/example/lab_3</div><div class="line">-rw-r--r--   1 hadoop supergroup         23 2018-01-04 18:34 /user/hadoop/example/lab_3/test1.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup         30 2018-01-04 18:34 /user/hadoop/example/lab_3/test2.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup         15 2018-01-04 18:34 /user/hadoop/example/lab_3/test3.txt</div></pre></td></tr></table></figure>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/01/04/[hadoop]hadoop_archive_cmd/" data-id="ckcx1jzcr000znsh4exbkexto" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-[hadoop]hdfs_ec_cmd" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/03/[hadoop]hdfs_ec_cmd/" class="article-date">
  <time datetime="2018-01-02T16:00:00.000Z" itemprop="datePublished">2018-01-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/hadoop/">hadoop</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/03/[hadoop]hdfs_ec_cmd/">hdfs ec command Memo</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
<div id="toc">
  <ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#hdfs-ec-使用方式-以XOR-2-1-1024k-Policy做為測試"><span class="toc-text">hdfs ec 使用方式(以XOR-2-1-1024k Policy做為測試)</span></a></li></ol>
</div>

        <h4 id="hdfs-ec-使用方式-以XOR-2-1-1024k-Policy做為測試"><a href="#hdfs-ec-使用方式-以XOR-2-1-1024k-Policy做為測試" class="headerlink" title="hdfs ec 使用方式(以XOR-2-1-1024k Policy做為測試)"></a>hdfs ec 使用方式(以XOR-2-1-1024k Policy做為測試)</h4><blockquote>
<p>EC相關架構的介紹可以參考網路相關的文章<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div></pre></td><td class="code"><pre><div class="line">Usage: bin/hdfs ec [COMMAND]</div><div class="line">          [-listPolicies]</div><div class="line">          [-addPolicies -policyFile &lt;file&gt;]</div><div class="line">          [-getPolicy -path &lt;path&gt;]</div><div class="line">          [-removePolicy -policy &lt;policy&gt;]</div><div class="line">          [-setPolicy -path &lt;path&gt; [-policy &lt;policy&gt;] [-replicate]]</div><div class="line">          [-unsetPolicy -path &lt;path&gt;]</div><div class="line">          [-listCodecs]</div><div class="line">          [-enablePolicy -policy &lt;policy&gt;]</div><div class="line">          [-disablePolicy -policy &lt;policy&gt;]</div><div class="line">          [-help &lt;command-name&gt;]</div><div class="line"></div><div class="line">##列出所有Erasure Coding可用的相關Policies</div><div class="line">hadoop@hadoop-master:~$ hdfs ec -listPolicies</div><div class="line">Erasure Coding Policies:</div><div class="line">ErasureCodingPolicy=[Name=RS-10-4-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=10, numParityUnits=4]], CellSize=1048576, Id=5], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-3-2-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=3, numParityUnits=2]], CellSize=1048576, Id=2], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-LEGACY-6-3-1024k, Schema=[ECSchema=[Codec=rs-legacy, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=3], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=XOR-2-1-1024k, Schema=[ECSchema=[Codec=xor, numDataUnits=2, numParityUnits=1]], CellSize=1048576, Id=4], State=DISABLED</div><div class="line"></div><div class="line">##列出所有Erasure Coding Codec列表</div><div class="line">hadoop@hadoop-master:~$ hdfs ec -listCodecs</div><div class="line">Erasure Coding Codecs: Codec [Coder List]</div><div class="line">        RS [RS_NATIVE, RS_JAVA]</div><div class="line">        RS-LEGACY [RS-LEGACY_JAVA]</div><div class="line">        XOR [XOR_NATIVE, XOR_JAVA]</div><div class="line"></div><div class="line">##Enable XOR-2-1-1024k的EC Policy		</div><div class="line">hadoop@hadoop-master:~$ hdfs ec -enablePolicy -policy XOR-2-1-1024k</div><div class="line">Erasure coding policy XOR-2-1-1024k is enabled</div><div class="line">hadoop@hadoop-master:~$ hdfs ec -listPolicies</div><div class="line">Erasure Coding Policies:</div><div class="line">ErasureCodingPolicy=[Name=RS-10-4-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=10, numParityUnits=4]], CellSize=1048576, Id=5], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-3-2-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=3, numParityUnits=2]], CellSize=1048576, Id=2], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-LEGACY-6-3-1024k, Schema=[ECSchema=[Codec=rs-legacy, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=3], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=XOR-2-1-1024k, Schema=[ECSchema=[Codec=xor, numDataUnits=2, numParityUnits=1]], CellSize=1048576, Id=4], State=ENABLED</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir -p /user/hadoop/example/lab_2</div><div class="line"></div><div class="line">##將目錄設定為XOR-2-1-1024k的EC Policy</div><div class="line">hadoop@hadoop-master:~$ hdfs ec -setPolicy -path /user/hadoop/example/lab_2 -policy XOR-2-1-1024k</div><div class="line">Set erasure coding policy XOR-2-1-1024k on /user/hadoop/example/lab_2</div><div class="line"></div><div class="line">##上傳檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put ~/test9.tar.gz /user/hadoop/example/lab_2</div><div class="line">2018-01-03 15:41:12,678 WARN erasurecode.ErasureCodeNative: ISA-L support is not available in your platform... using builtin-java codec where applicable</div><div class="line">2018-01-03 15:41:12,741 WARN hdfs.DFSOutputStream: Cannot allocate parity block(index=2, policy=XOR-2-1-1024k). Not enough datanodes? Exclude nodes=[]</div><div class="line">2018-01-03 15:41:12,953 WARN hdfs.DFSOutputStream: Block group &lt;1&gt; has 1 corrupt blocks. It&apos;s at high risk of losing data.</div><div class="line"></div><div class="line">##觀察上傳檔案的複本數</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_2</div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   1 hadoop supergroup        373 2018-01-03 15:41 /user/hadoop/example/lab_2/test9.tar.gz</div><div class="line"></div><div class="line">##以下為使用fsck指令觀察該檔案Block相關訊息</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/example/lab_2/test9.tar.gz -files -blocks -locations</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;locations=1&amp;path=%2Fuser%2Fhadoop%2Fexample%2Flab_2%2Ftest9.tar.gz</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/example/lab_2/test9.tar.gz at Wed Jan 03 15:46:48 CST 2018</div><div class="line">/user/hadoop/example/lab_2/test9.tar.gz 373 bytes, erasure-coded: policy=XOR-2-1-1024k, 1 block(s):  OK</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_-9223372036854775776_2346 len=373 Live_repl=2  </div><div class="line">[blk_-9223372036854775776:DatanodeInfoWithStorage[192.168.51.5:9866,DS-f59711c6-801d-4ebc-b55c-228f225117b8,DISK], </div><div class="line"> blk_-9223372036854775774:DatanodeInfoWithStorage[192.168.51.6:9866,DS-482194d9-aa70-4ab8-8253-907739d5b1a1,DISK]]</div><div class="line"></div><div class="line"></div><div class="line">Status: HEALTHY</div><div class="line"> Number of data-nodes:  2</div><div class="line"> Number of racks:               1</div><div class="line"> Total dirs:                    0</div><div class="line"> Total symlinks:                0</div><div class="line"></div><div class="line">Replicated Blocks:</div><div class="line"> Total size:    0 B</div><div class="line"> Total files:   0</div><div class="line"> Total blocks (validated):      0</div><div class="line"> Minimally replicated blocks:   0</div><div class="line"> Over-replicated blocks:        0</div><div class="line"> Under-replicated blocks:       0</div><div class="line"> Mis-replicated blocks:         0</div><div class="line"> Default replication factor:    1</div><div class="line"> Average block replication:     0.0</div><div class="line"> Missing blocks:                0</div><div class="line"> Corrupt blocks:                0</div><div class="line"> Missing replicas:              0</div><div class="line"></div><div class="line">Erasure Coded Block Groups:</div><div class="line"> Total size:    373 B</div><div class="line"> Total files:   1</div><div class="line"> Total block groups (validated):        1 (avg. block group size 373 B)</div><div class="line"> Minimally erasure-coded block groups:  1 (100.0 %)</div><div class="line"> Over-erasure-coded block groups:       0 (0.0 %)</div><div class="line"> Under-erasure-coded block groups:      0 (0.0 %)</div><div class="line"> Unsatisfactory placement block groups: 0 (0.0 %)</div><div class="line"> Average block group size:      2.0</div><div class="line"> Missing block groups:          0</div><div class="line"> Corrupt block groups:          0</div><div class="line"> Missing internal blocks:       0 (0.0 %)</div><div class="line">FSCK ended at Wed Jan 03 15:46:48 CST 2018 in 2 milliseconds</div><div class="line"></div><div class="line"></div><div class="line">The filesystem under path &apos;/user/hadoop/example/lab_2/test9.tar.gz&apos; is HEALTHY</div><div class="line"></div><div class="line">##以blockId觀察複本狀況</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck -blockId blk_-9223372036854775776</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;blockId=blk_-9223372036854775776+&amp;path=%2F</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 at Wed Jan 03 15:49:12 CST 2018</div><div class="line"></div><div class="line">Block Id: blk_-9223372036854775776</div><div class="line">Block belongs to: /user/hadoop/example/lab_2/test9.tar.gz</div><div class="line">No. of Expected Replica: 2</div><div class="line">No. of live Replica: 2</div><div class="line">No. of excess Replica: 0</div><div class="line">No. of stale Replica: 0</div><div class="line">No. of decommissioned Replica: 0</div><div class="line">No. of decommissioning Replica: 0</div><div class="line">No. of corrupted Replica: 0</div><div class="line">null</div><div class="line"></div><div class="line"></div><div class="line">Fsck on blockId &apos;blk_-9223372036854775776</div><div class="line"></div><div class="line">##以下為模擬shutdown 一台Datanode,資料是否還會存在??</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -shutdownDatanode hadoop-slave2:9867</div><div class="line">Submitted a shutdown request to datanode hadoop-slave2:9867</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/example/lab_2/test9.tar.gz -files -blocks -locations</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;locations=1&amp;path=%2Fuser%2Fhadoop%2Fexample%2Flab_2%2Ftest9.tar.gz</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/example/lab_2/test9.tar.gz at Wed Jan 03 16:11:01 CST 2018</div><div class="line">/user/hadoop/example/lab_2/test9.tar.gz 373 bytes, erasure-coded: policy=XOR-2-1-1024k, 1 block(s):  </div><div class="line">Under replicated BP-1139418417-192.168.51.4-1514272982687:blk_-9223372036854775776_2346. </div><div class="line">Target Replicas is 2 but found 1 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_-9223372036854775776_2346 len=373 Live_repl=1  </div><div class="line">[blk_-9223372036854775776:DatanodeInfoWithStorage[192.168.51.5:9866,DS-f59711c6-801d-4ebc-b55c-228f225117b8,DISK]]</div><div class="line"></div><div class="line"></div><div class="line">Status: HEALTHY</div><div class="line"> Number of data-nodes:  1</div><div class="line"> Number of racks:               1</div><div class="line"> Total dirs:                    0</div><div class="line"> Total symlinks:                0</div><div class="line"></div><div class="line">Replicated Blocks:</div><div class="line"> Total size:    0 B</div><div class="line"> Total files:   0</div><div class="line"> Total blocks (validated):      0</div><div class="line"> Minimally replicated blocks:   0</div><div class="line"> Over-replicated blocks:        0</div><div class="line"> Under-replicated blocks:       0</div><div class="line"> Mis-replicated blocks:         0</div><div class="line"> Default replication factor:    1</div><div class="line"> Average block replication:     0.0</div><div class="line"> Missing blocks:                0</div><div class="line"> Corrupt blocks:                0</div><div class="line"> Missing replicas:              0</div><div class="line"></div><div class="line">Erasure Coded Block Groups:</div><div class="line"> Total size:    373 B</div><div class="line"> Total files:   1</div><div class="line"> Total block groups (validated):        1 (avg. block group size 373 B)</div><div class="line"> Minimally erasure-coded block groups:  1 (100.0 %)</div><div class="line"> Over-erasure-coded block groups:       0 (0.0 %)</div><div class="line"> Under-erasure-coded block groups:      1 (100.0 %)</div><div class="line"> Unsatisfactory placement block groups: 0 (0.0 %)</div><div class="line"> Average block group size:      1.0</div><div class="line"> Missing block groups:          0</div><div class="line"> Corrupt block groups:          0</div><div class="line"> Missing internal blocks:       1 (50.0 %)</div><div class="line">FSCK ended at Wed Jan 03 16:11:01 CST 2018 in 3 milliseconds</div><div class="line"></div><div class="line"></div><div class="line">The filesystem under path &apos;/user/hadoop/example/lab_2/test9.tar.gz&apos; is HEALTHY</div><div class="line"></div><div class="line">雖然複本數是設定為1,但因為使用的是EC策略,資料仍然是可下載而且資料內容也是正確的</div></pre></td></tr></table></figure></p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/01/03/[hadoop]hdfs_ec_cmd/" data-id="ckcx1jzd00018nsh48lirnsoo" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-[hadoop]hadoop_fsck_cmd" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/03/[hadoop]hadoop_fsck_cmd/" class="article-date">
  <time datetime="2018-01-02T16:00:00.000Z" itemprop="datePublished">2018-01-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/hadoop/">hadoop</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/03/[hadoop]hadoop_fsck_cmd/">hdfs fsck command Memo</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
<div id="toc">
  <ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#hadoop-fsck使用方式"><span class="toc-text">hadoop fsck使用方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hdfs-fsck-相關參數操作說明"><span class="toc-text">hdfs fsck 相關參數操作說明</span></a></li></ol>
</div>

        <h4 id="hadoop-fsck使用方式"><a href="#hadoop-fsck使用方式" class="headerlink" title="hadoop fsck使用方式"></a>hadoop fsck使用方式</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:15:13 CST 2018</div><div class="line"></div><div class="line">Status: HEALTHY</div><div class="line"> Number of data-nodes:  2</div><div class="line"> Number of racks:               1</div><div class="line"> Total dirs:                    0</div><div class="line"> Total symlinks:                0</div><div class="line"></div><div class="line">Replicated Blocks:</div><div class="line"> Total size:    133 B</div><div class="line"> Total files:   1</div><div class="line"> Total blocks (validated):      1 (avg. block size 133 B)</div><div class="line"> Minimally replicated blocks:   1 (100.0 %)</div><div class="line"> Over-replicated blocks:        0 (0.0 %)</div><div class="line"> Under-replicated blocks:       0 (0.0 %)</div><div class="line"> Mis-replicated blocks:         0 (0.0 %)</div><div class="line"> Default replication factor:    2</div><div class="line"> Average block replication:     2.0</div><div class="line"> Missing blocks:                0</div><div class="line"> Corrupt blocks:                0</div><div class="line"> Missing replicas:              0 (0.0 %)</div><div class="line"></div><div class="line">Erasure Coded Block Groups:</div><div class="line"> Total size:    0 B</div><div class="line"> Total files:   0</div><div class="line"> Total block groups (validated):        0</div><div class="line"> Minimally erasure-coded block groups:  0</div><div class="line"> Over-erasure-coded block groups:       0</div><div class="line"> Under-erasure-coded block groups:      0</div><div class="line"> Unsatisfactory placement block groups: 0</div><div class="line"> Average block group size:      0.0</div><div class="line"> Missing block groups:          0</div><div class="line"> Corrupt block groups:          0</div><div class="line"> Missing internal blocks:       0</div><div class="line">FSCK ended at Wed Jan 03 13:15:13 CST 2018 in 1 milliseconds</div><div class="line"></div><div class="line"></div><div class="line">The filesystem under path &apos;/user/hadoop/test1.txt&apos; is HEALTHY</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="hdfs-fsck-相關參數操作說明"><a href="#hdfs-fsck-相關參數操作說明" class="headerlink" title="hdfs fsck 相關參數操作說明"></a>hdfs fsck 相關參數操作說明</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line">##檢查並列出所有文件的狀態</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:17:02 CST 2018</div><div class="line">/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s):  OK</div><div class="line"></div><div class="line">##列出所有Blocks資訊</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files -blocks</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:18:46 CST 2018</div><div class="line">/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s):  OK</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_1073743138_2323 len=133 Live_repl=2</div><div class="line"></div><div class="line">##列出所有Blocks的位置訊息</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files -blocks -locations</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;locations=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:19:53 CST 2018</div><div class="line">/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s):  OK</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_1073743138_2323 len=133 Live_repl=2  </div><div class="line">[DatanodeInfoWithStorage[192.168.51.6:9866,DS-482194d9-aa70-4ab8-8253-907739d5b1a1,DISK], </div><div class="line"> DatanodeInfoWithStorage[192.168.51.5:9866,DS-f59711c6-801d-4ebc-b55c-228f225117b8,DISK]]</div><div class="line"></div><div class="line">##列出檔案所有的訊息包含Rack位置</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files -blocks -locations -racks</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;locations=1&amp;racks=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Thu Jan 04 18:43:12 CST 2018</div><div class="line">/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s):  OK</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_1073743138_2323 len=133 Live_repl=2  [/default-rack/192.168.51.5:9866, /default-rack/192.168.51.6:9866]</div><div class="line"></div><div class="line"></div><div class="line">##列出所有完整的replication的位置訊息</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files -blocks -replicaDetails</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;replicadetails=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:21:56 CST 2018</div><div class="line">/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s):  OK</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_1073743138_2323 len=133 Live_repl=2  </div><div class="line">[DatanodeInfoWithStorage[192.168.51.6:9866,DS-482194d9-aa70-4ab8-8253-907739d5b1a1,DISK](LIVE), </div><div class="line"> DatanodeInfoWithStorage[192.168.51.5:9866,DS-f59711c6-801d-4ebc-b55c-228f225117b8,DISK](LIVE)]</div><div class="line"></div><div class="line">##查看文件中損壞Blocks的狀況</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -list-corruptfileblocks</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;listcorruptfileblocks=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">The filesystem under path &apos;/user/hadoop/test1.txt&apos; has 0 CORRUPT files</div><div class="line"></div><div class="line">##列出指定的block的詳細資訊</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -blockId blk_1073743138</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;blockId=blk_1073743138+&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 at Wed Jan 03 13:34:40 CST 2018</div><div class="line"></div><div class="line">Block Id: blk_1073743138</div><div class="line">Block belongs to: /user/hadoop/test1.txt</div><div class="line">No. of Expected Replica: 2</div><div class="line">No. of live Replica: 2</div><div class="line">No. of excess Replica: 0</div><div class="line">No. of stale Replica: 0</div><div class="line">No. of decommissioned Replica: 0</div><div class="line">No. of decommissioning Replica: 0</div><div class="line">No. of corrupted Replica: 0</div><div class="line">Block replica on datanode/rack: hadoop-slave1/default-rack is HEALTHY</div><div class="line">Block replica on datanode/rack: hadoop-slave2/default-rack is HEALTHY</div></pre></td></tr></table></figure></blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/01/03/[hadoop]hadoop_fsck_cmd/" data-id="ckcx1jzcw0014nsh448tfnp8y" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-[hadoop]hdfs_other_cmd" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/03/[hadoop]hdfs_other_cmd/" class="article-date">
  <time datetime="2018-01-02T16:00:00.000Z" itemprop="datePublished">2018-01-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/hadoop/">hadoop</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/03/[hadoop]hdfs_other_cmd/">hdfs client command Memo</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
<div id="toc">
  <ol class="toc"><li class="toc-item toc-level-5"><a class="toc-link" href="#hdfs-getconf相關用法"><span class="toc-text">hdfs getconf相關用法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#hdfs-envvars相關用法"><span class="toc-text">hdfs envvars相關用法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#hdfs-classpath相關用法"><span class="toc-text">hdfs classpath相關用法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#hdfs-version相關用法"><span class="toc-text">hdfs version相關用法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#hdfs-groups相關用法"><span class="toc-text">hdfs groups相關用法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#hadoop-conftest"><span class="toc-text">hadoop conftest</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#hadoop-jnipath"><span class="toc-text">hadoop jnipath</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#hadoop-checknative"><span class="toc-text">hadoop checknative</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#hadoop-daemonlog"><span class="toc-text">hadoop daemonlog</span></a></li></ol></li></ol>
</div>

        <h5 id="hdfs-getconf相關用法"><a href="#hdfs-getconf相關用法" class="headerlink" title="hdfs getconf相關用法"></a>hdfs getconf相關用法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">Usage:</div><div class="line">hadoop getconf</div><div class="line">        [-namenodes]            gets list of namenodes in the cluster.</div><div class="line">        [-secondaryNameNodes]   gets list of secondary namenodes in the cluster.</div><div class="line">        [-backupNodes]          gets list of backup nodes in the cluster.</div><div class="line">        [-includeFile]          gets the include file path that defines the datanodes that can join the cluster.</div><div class="line">        [-excludeFile]          gets the exclude file path that defines the datanodes that need to decommissioned.</div><div class="line">        [-nnRpcAddresses]       gets the namenode rpc addresses</div><div class="line">        [-confKey [key]]		gets a specific key from the configuration</div><div class="line">---</div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -namenodes</div><div class="line">hadoop-master</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -secondaryNameNodes</div><div class="line">hadoop-master</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -backupNodes</div><div class="line">0.0.0.0</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -nnRpcAddresses</div><div class="line">hadoop-master:8020</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -includeFile</div><div class="line">Configuration dfs.hosts is missing.</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -excludeFile</div><div class="line">Configuration dfs.hosts.exclude is missing.</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -confKey &quot;fs.trash.interval&quot;</div><div class="line">1440</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hdfs-envvars相關用法"><a href="#hdfs-envvars相關用法" class="headerlink" title="hdfs envvars相關用法"></a>hdfs envvars相關用法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs envvars</div><div class="line">JAVA_HOME=&apos;/bgdt/java/jdk1.8.0_101&apos;</div><div class="line">HADOOP_HDFS_HOME=&apos;/bgdt/hadoop-3.0.0&apos;</div><div class="line">HDFS_DIR=&apos;share/hadoop/hdfs&apos;</div><div class="line">HDFS_LIB_JARS_DIR=&apos;share/hadoop/hdfs/lib&apos;</div><div class="line">HADOOP_CONF_DIR=&apos;/bgdt/hadoop-3.0.0/etc/hadoop&apos;</div><div class="line">HADOOP_TOOLS_HOME=&apos;/bgdt/hadoop-3.0.0&apos;</div><div class="line">HADOOP_TOOLS_DIR=&apos;share/hadoop/tools&apos;</div><div class="line">HADOOP_TOOLS_LIB_JARS_DIR=&apos;share/hadoop/tools/lib&apos;</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hdfs-classpath相關用法"><a href="#hdfs-classpath相關用法" class="headerlink" title="hdfs classpath相關用法"></a>hdfs classpath相關用法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs classpath</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop:/bgdt/hadoop-3.0.0/share/hadoop/common/lib/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/common/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/hdfs:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/hdfs/lib/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/hdfs/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/mapreduce/lib/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/mapreduce/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/yarn:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/yarn/lib/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/yarn/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hdfs-version相關用法"><a href="#hdfs-version相關用法" class="headerlink" title="hdfs version相關用法"></a>hdfs version相關用法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs version</div><div class="line">Hadoop 3.0.0</div><div class="line">Source code repository https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533</div><div class="line">Compiled by andrew on 2017-12-08T19:16Z</div><div class="line">Compiled with protoc 2.5.0</div><div class="line">From source with checksum 397832cb5529187dc8cd74ad54ff22</div><div class="line">This command was run using /bgdt/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hdfs-groups相關用法"><a href="#hdfs-groups相關用法" class="headerlink" title="hdfs groups相關用法"></a>hdfs groups相關用法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs groups</div><div class="line">hadoop : hadoop</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hadoop-conftest"><a href="#hadoop-conftest" class="headerlink" title="hadoop conftest"></a>hadoop conftest</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop conftest</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/hadoop-policy.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/yarn-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/hdfs-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/core-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/mapred-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/capacity-scheduler.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/kms-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/httpfs-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/kms-acls.xml: valid</div><div class="line">OK</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hadoop-jnipath"><a href="#hadoop-jnipath" class="headerlink" title="hadoop jnipath"></a>hadoop jnipath</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop jnipath</div><div class="line">/bgdt/hadoop-3.0.0/lib/native</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hadoop-checknative"><a href="#hadoop-checknative" class="headerlink" title="hadoop checknative"></a>hadoop checknative</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop checknative</div><div class="line">2018-01-04 17:18:04,659 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native</div><div class="line">2018-01-04 17:18:04,667 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</div><div class="line">2018-01-04 17:18:04,682 ERROR snappy.SnappyCompressor: failed to load SnappyCompressor</div><div class="line">java.lang.UnsatisfiedLinkError: Cannot load libsnappy.so.1 (libsnappy.so.1: cannot open shared object file: No such file or directory)!</div><div class="line">        at org.apache.hadoop.io.compress.snappy.SnappyCompressor.initIDs(Native Method)</div><div class="line">        at org.apache.hadoop.io.compress.snappy.SnappyCompressor.&lt;clinit&gt;(SnappyCompressor.java:57)</div><div class="line">        at org.apache.hadoop.io.compress.SnappyCodec.isNativeCodeLoaded(SnappyCodec.java:82)</div><div class="line">        at org.apache.hadoop.util.NativeLibraryChecker.main(NativeLibraryChecker.java:100)</div><div class="line">2018-01-04 17:18:04,691 WARN erasurecode.ErasureCodeNative: ISA-L support is not available in your platform... using builtin-java codec where applicable</div><div class="line">Native library checking:</div><div class="line">hadoop:  true /bgdt/hadoop-3.0.0/lib/native/libhadoop.so.1.0.0</div><div class="line">zlib:    true /lib/x86_64-linux-gnu/libz.so.1</div><div class="line">zstd  :  false</div><div class="line">snappy:  false</div><div class="line">lz4:     true revision:10301</div><div class="line">bzip2:   true /lib/x86_64-linux-gnu/libbz2.so.1</div><div class="line">openssl: false Cannot load libcrypto.so (libcrypto.so: cannot open shared object file: No such file or directory)!</div><div class="line">ISA-L:   false libhadoop was built without ISA-L support</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hadoop-daemonlog"><a href="#hadoop-daemonlog" class="headerlink" title="hadoop daemonlog"></a>hadoop daemonlog</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">##取得Class Log Level級別</div><div class="line">hadoop@hadoop-master:~$ hadoop daemonlog -getlevel hadoop-master:9870 org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Connecting to http://hadoop-master:9870/logLevel?log=org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Submitted Class Name: org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Log Class: org.apache.commons.logging.impl.Log4JLogger</div><div class="line">Effective Level: INFO</div><div class="line"></div><div class="line">##設定Class Log Level級別</div><div class="line">hadoop@hadoop-master:~$ hadoop daemonlog -setlevel  hadoop-master:9870 org.apache.hadoop.yarn.server.nodemanager.NodeManager DEBUG</div><div class="line">Connecting to http://hadoop-master:9870/logLevel?log=org.apache.hadoop.yarn.server.nodemanager.NodeManager&amp;level=DEBUG</div><div class="line">Submitted Class Name: org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Log Class: org.apache.commons.logging.impl.Log4JLogger</div><div class="line">Submitted Level: DEBUG</div><div class="line">Setting Level to DEBUG ...</div><div class="line">Effective Level: DEBUG</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop daemonlog -getlevel hadoop-master:9870 org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Connecting to http://hadoop-master:9870/logLevel?log=org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Submitted Class Name: org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Log Class: org.apache.commons.logging.impl.Log4JLogger</div><div class="line">Effective Level: DEBUG</div></pre></td></tr></table></figure></blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/01/03/[hadoop]hdfs_other_cmd/" data-id="ckcx1jzd6001cnsh40cc4uwcx" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-[hadoop]hdfs_dfsadmin_cmd" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/03/[hadoop]hdfs_dfsadmin_cmd/" class="article-date">
  <time datetime="2018-01-02T16:00:00.000Z" itemprop="datePublished">2018-01-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/hadoop/">hadoop</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/03/[hadoop]hdfs_dfsadmin_cmd/">hdfs dfsadmin command Memo</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
<div id="toc">
  <ol class="toc"><li class="toc-item toc-level-5"><a class="toc-link" href="#hdfs-dfsadmin相關語法"><span class="toc-text">hdfs dfsadmin相關語法</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#hdfs-dfsadmin-report"><span class="toc-text">hdfs dfsadmin -report</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#hdfs-dfsadmin-printTopology"><span class="toc-text">hdfs dfsadmin -printTopology</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#hdfs-dfsadmin-getDatanodeInfo"><span class="toc-text">hdfs dfsadmin -getDatanodeInfo</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#hdfs-dfsadmin-safemode"><span class="toc-text">hdfs dfsadmin -safemode</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#hdfs-dfsadmin-shutdownDatanode"><span class="toc-text">hdfs dfsadmin -shutdownDatanode</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#hdfs-dfsadmin-metasave"><span class="toc-text">hdfs dfsadmin -metasave</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#hdfs-dfsadmin-listOpenFiles"><span class="toc-text">hdfs dfsadmin -listOpenFiles</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#hdfs-dfsadmin-triggerBlockReport"><span class="toc-text">hdfs dfsadmin -triggerBlockReport</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#hdfs-dfsadmin-BalancerBandwidth"><span class="toc-text">hdfs dfsadmin BalancerBandwidth</span></a></li></ol></li></ol>
</div>

        <h5 id="hdfs-dfsadmin相關語法"><a href="#hdfs-dfsadmin相關語法" class="headerlink" title="hdfs dfsadmin相關語法"></a>hdfs dfsadmin相關語法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">Note: Administrative commands can only be run as the HDFS superuser.</div><div class="line">        [-report [-live] [-dead] [-decommissioning] [-enteringmaintenance] [-inmaintenance]]</div><div class="line">        [-safemode &lt;enter | leave | get | wait&gt;]</div><div class="line">        [-saveNamespace [-beforeShutdown]]</div><div class="line">        [-rollEdits]</div><div class="line">        [-restoreFailedStorage true|false|check]</div><div class="line">        [-refreshNodes]</div><div class="line">        [-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">        [-clrQuota &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">        [-setSpaceQuota &lt;quota&gt; [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">        [-clrSpaceQuota [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">        [-finalizeUpgrade]</div><div class="line">        [-rollingUpgrade [&lt;query|prepare|finalize&gt;]]</div><div class="line">        [-refreshServiceAcl]</div><div class="line">        [-refreshUserToGroupsMappings]</div><div class="line">        [-refreshSuperUserGroupsConfiguration]</div><div class="line">        [-refreshCallQueue]</div><div class="line">        [-refresh &lt;host:ipc_port&gt; &lt;key&gt; [arg1..argn]</div><div class="line">        [-reconfig &lt;namenode|datanode&gt; &lt;host:ipc_port&gt; &lt;start|status|properties&gt;]</div><div class="line">        [-printTopology]</div><div class="line">        [-refreshNamenodes datanode_host:ipc_port]</div><div class="line">        [-getVolumeReport datanode_host:ipc_port]</div><div class="line">        [-deleteBlockPool datanode_host:ipc_port blockpoolId [force]]</div><div class="line">        [-setBalancerBandwidth &lt;bandwidth in bytes per second&gt;]</div><div class="line">        [-getBalancerBandwidth &lt;datanode_host:ipc_port&gt;]</div><div class="line">        [-fetchImage &lt;local directory&gt;]</div><div class="line">        [-allowSnapshot &lt;snapshotDir&gt;]</div><div class="line">        [-disallowSnapshot &lt;snapshotDir&gt;]</div><div class="line">        [-shutdownDatanode &lt;datanode_host:ipc_port&gt; [upgrade]]</div><div class="line">        [-evictWriters &lt;datanode_host:ipc_port&gt;]</div><div class="line">        [-getDatanodeInfo &lt;datanode_host:ipc_port&gt;]</div><div class="line">        [-metasave filename]</div><div class="line">        [-triggerBlockReport [-incremental] &lt;datanode_host:ipc_port&gt;]</div><div class="line">        [-listOpenFiles]</div><div class="line">        [-help [cmd]]</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-report"><a href="#hdfs-dfsadmin-report" class="headerlink" title="hdfs dfsadmin -report"></a>hdfs dfsadmin -report</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -report</div><div class="line">Configured Capacity: 42002972672 (39.12 GB)</div><div class="line">Present Capacity: 26379444224 (24.57 GB)</div><div class="line">DFS Remaining: 26331783168 (24.52 GB)</div><div class="line">DFS Used: 47661056 (45.45 MB)</div><div class="line">DFS Used%: 0.18%</div><div class="line">Replicated Blocks:</div><div class="line">        Under replicated blocks: 967</div><div class="line">        Blocks with corrupt replicas: 0</div><div class="line">        Missing blocks: 0</div><div class="line">        Missing blocks (with replication factor 1): 0</div><div class="line">        Pending deletion blocks: 0</div><div class="line">Erasure Coded Block Groups:</div><div class="line">        Low redundancy block groups: 0</div><div class="line">        Block groups with corrupt internal blocks: 0</div><div class="line">        Missing block groups: 0</div><div class="line">        Pending deletion blocks: 0</div><div class="line"></div><div class="line">-------------------------------------------------</div><div class="line">Live datanodes (2):</div><div class="line"></div><div class="line">Name: 192.168.51.5:9866 (hadoop-slave1)</div><div class="line">Hostname: hadoop-slave1</div><div class="line">Decommission Status : Normal</div><div class="line">Configured Capacity: 21001486336 (19.56 GB)</div><div class="line">DFS Used: 23842816 (22.74 MB)</div><div class="line">Non DFS Used: 6787604480 (6.32 GB)</div><div class="line">DFS Remaining: 13099626496 (12.20 GB)</div><div class="line">DFS Used%: 0.11%</div><div class="line">DFS Remaining%: 62.37%</div><div class="line">Configured Cache Capacity: 0 (0 B)</div><div class="line">Cache Used: 0 (0 B)</div><div class="line">Cache Remaining: 0 (0 B)</div><div class="line">Cache Used%: 100.00%</div><div class="line">Cache Remaining%: 0.00%</div><div class="line">Xceivers: 1</div><div class="line">Last contact: Wed Jan 03 17:52:32 CST 2018</div><div class="line">Last Block Report: Wed Jan 03 15:42:49 CST 2018</div><div class="line"></div><div class="line"></div><div class="line">Name: 192.168.51.6:9866 (hadoop-slave2)</div><div class="line">Hostname: hadoop-slave2</div><div class="line">Decommission Status : Normal</div><div class="line">Configured Capacity: 21001486336 (19.56 GB)</div><div class="line">DFS Used: 23818240 (22.71 MB)</div><div class="line">Non DFS Used: 6655098880 (6.20 GB)</div><div class="line">DFS Remaining: 13232156672 (12.32 GB)</div><div class="line">DFS Used%: 0.11%</div><div class="line">DFS Remaining%: 63.01%</div><div class="line">Configured Cache Capacity: 0 (0 B)</div><div class="line">Cache Used: 0 (0 B)</div><div class="line">Cache Remaining: 0 (0 B)</div><div class="line">Cache Used%: 100.00%</div><div class="line">Cache Remaining%: 0.00%</div><div class="line">Xceivers: 1</div><div class="line">Last contact: Wed Jan 03 17:52:33 CST 2018</div><div class="line">Last Block Report: Wed Jan 03 16:33:35 CST 2018</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-printTopology"><a href="#hdfs-dfsadmin-printTopology" class="headerlink" title="hdfs dfsadmin -printTopology"></a>hdfs dfsadmin -printTopology</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -printTopology</div><div class="line">Rack: /default-rack</div><div class="line">   192.168.51.5:9866 (hadoop-slave1)</div><div class="line">   192.168.51.6:9866 (hadoop-slave2)</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-getDatanodeInfo"><a href="#hdfs-dfsadmin-getDatanodeInfo" class="headerlink" title="hdfs dfsadmin -getDatanodeInfo"></a>hdfs dfsadmin -getDatanodeInfo</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getDatanodeInfo 192.168.51.6:9867</div><div class="line">Uptime: 5100, Software version: 3.0.0, Config version: core-3.0.0,hdfs-1</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getDatanodeInfo 192.168.51.5:9867</div><div class="line">Uptime: 10709, Software version: 3.0.0, Config version: core-3.0.0,hdfs-1</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-safemode"><a href="#hdfs-dfsadmin-safemode" class="headerlink" title="hdfs dfsadmin -safemode"></a>hdfs dfsadmin -safemode</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -safemode</div><div class="line">Usage: hdfs dfsadmin [-safemode enter | leave | get | wait | forceExit]</div><div class="line"></div><div class="line">##取得safemode模式</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -safemode get</div><div class="line">Safe mode is OFF</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/snapshot/test3.txt</div><div class="line">3333</div><div class="line">4444</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/snapshot</div><div class="line">Found 3 items</div><div class="line">-rw-r--r--   1 hadoop supergroup         29 2018-01-03 16:53 /user/hadoop/snapshot/test1.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup          7 2018-01-03 17:36 /user/hadoop/snapshot/test2.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup         10 2018-01-03 18:04 /user/hadoop/snapshot/test3.txt</div><div class="line"></div><div class="line">##進入safemode模式</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -safemode enter</div><div class="line">Safe mode is ON</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -safemode get</div><div class="line">Safe mode is ON</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/snapshot/test4.txt</div><div class="line">put: Cannot create file/user/hadoop/snapshot/test4.txt._COPYING_. Name node is in safe mode.</div><div class="line"></div><div class="line">##離開safemode模式</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -safemode leave</div><div class="line">Safe mode is OFF</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/snapshot/test4.txt</div><div class="line">55555</div><div class="line">66666</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/snapshot</div><div class="line">Found 4 items</div><div class="line">-rw-r--r--   1 hadoop supergroup         29 2018-01-03 16:53 /user/hadoop/snapshot/test1.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup          7 2018-01-03 17:36 /user/hadoop/snapshot/test2.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup         10 2018-01-03 18:04 /user/hadoop/snapshot/test3.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup         12 2018-01-03 18:05 /user/hadoop/snapshot/test4.txt</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-shutdownDatanode"><a href="#hdfs-dfsadmin-shutdownDatanode" class="headerlink" title="hdfs dfsadmin -shutdownDatanode"></a>hdfs dfsadmin -shutdownDatanode</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">##停掉 Datanode</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -shutdownDatanode hadoop-slave2:9867</div><div class="line">Submitted a shutdown request to datanode hadoop-slave2:9867</div><div class="line"></div><div class="line">##觀察 Datanode Status</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -report -dead</div><div class="line">Safe mode is ON</div><div class="line">Configured Capacity: 21001486336 (19.56 GB)</div><div class="line">Present Capacity: 13123469312 (12.22 GB)</div><div class="line">DFS Remaining: 13099626496 (12.20 GB)</div><div class="line">DFS Used: 23842816 (22.74 MB)</div><div class="line">DFS Used%: 0.18%</div><div class="line">Replicated Blocks:</div><div class="line">        Under replicated blocks: 971</div><div class="line">        Blocks with corrupt replicas: 0</div><div class="line">        Missing blocks: 2</div><div class="line">        Missing blocks (with replication factor 1): 2</div><div class="line">        Pending deletion blocks: 0</div><div class="line">Erasure Coded Block Groups:</div><div class="line">        Low redundancy block groups: 1</div><div class="line">        Block groups with corrupt internal blocks: 0</div><div class="line">        Missing block groups: 0</div><div class="line">        Pending deletion blocks: 0</div><div class="line"></div><div class="line">-------------------------------------------------</div><div class="line">Dead datanodes (1):</div><div class="line"></div><div class="line">Name: 192.168.51.6:9866 (hadoop-slave2)</div><div class="line">Hostname: hadoop-slave2</div><div class="line">Decommission Status : Normal</div><div class="line">Configured Capacity: 21001486336 (19.56 GB)</div><div class="line">DFS Used: 23834624 (22.73 MB)</div><div class="line">Non DFS Used: 6655102976 (6.20 GB)</div><div class="line">DFS Remaining: 13232136192 (12.32 GB)</div><div class="line">DFS Used%: 0.11%</div><div class="line">DFS Remaining%: 63.01%</div><div class="line">Configured Cache Capacity: 0 (0 B)</div><div class="line">Cache Used: 0 (0 B)</div><div class="line">Cache Remaining: 0 (0 B)</div><div class="line">Cache Used%: 100.00%</div><div class="line">Cache Remaining%: 0.00%</div><div class="line">Xceivers: 0</div><div class="line">Last contact: Wed Jan 03 18:16:00 CST 2018</div><div class="line">Last Block Report: Wed Jan 03 16:33:35 CST 2018</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-metasave"><a href="#hdfs-dfsadmin-metasave" class="headerlink" title="hdfs dfsadmin -metasave"></a>hdfs dfsadmin -metasave</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">## 產生所有metadata資訊,產生後會存放於hdfs系統的Log資料夾內</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -metasave metasave.txt</div><div class="line">Created metasave file metasave.txt in the log directory of namenode hdfs://hadoop-master:8020</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-listOpenFiles"><a href="#hdfs-dfsadmin-listOpenFiles" class="headerlink" title="hdfs dfsadmin -listOpenFiles"></a>hdfs dfsadmin -listOpenFiles</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -listOpenFiles</div><div class="line">Client Host             Client Name             Open File Path</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-triggerBlockReport"><a href="#hdfs-dfsadmin-triggerBlockReport" class="headerlink" title="hdfs dfsadmin -triggerBlockReport"></a>hdfs dfsadmin -triggerBlockReport</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -triggerBlockReport 192.168.51.5:9867</div><div class="line">Triggering a full block report on 192.168.51.5:9867.</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-BalancerBandwidth"><a href="#hdfs-dfsadmin-BalancerBandwidth" class="headerlink" title="hdfs dfsadmin BalancerBandwidth"></a>hdfs dfsadmin BalancerBandwidth</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">##取得Namenode與各Datanode之間的IPC頻寬(目前設定為10M bytes/s)</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getBalancerBandwidth hadoop-slave1:9867</div><div class="line">Balancer bandwidth is 10485760 bytes per second.</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getBalancerBandwidth hadoop-slave2:9867</div><div class="line">Balancer bandwidth is 10485760 bytes per second.</div><div class="line"></div><div class="line">##設定Namenode與Datanode之間的頻寬大小</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -setBalancerBandwidth 15M</div><div class="line">Balancer bandwidth is set to 15728640</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getBalancerBandwidth hadoop-slave2:9867</div><div class="line">Balancer bandwidth is 15728640 bytes per second.</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getBalancerBandwidth hadoop-slave1:9867</div><div class="line">Balancer bandwidth is 15728640 bytes per second.</div></pre></td></tr></table></figure>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/01/03/[hadoop]hdfs_dfsadmin_cmd/" data-id="ckcx1jzd3001ansh4ujb8xiny" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-[hadoop]Hadoop_permission_memo" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/02/[hadoop]Hadoop_permission_memo/" class="article-date">
  <time datetime="2018-01-01T16:00:00.000Z" itemprop="datePublished">2018-01-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/hadoop/">hadoop</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/02/[hadoop]Hadoop_permission_memo/">Hadoop權限操作說明</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
<div id="toc">
  <ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#使用到的HDFS語法整理"><span class="toc-text">使用到的HDFS語法整理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#以下為操作驗證"><span class="toc-text">以下為操作驗證</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#使用hadoop語法查看目錄權限"><span class="toc-text">使用hadoop語法查看目錄權限</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#使用putty-以hadoop帳號-登入-hadoop-master主機建立一個新帳號"><span class="toc-text">使用putty(以hadoop帳號)登入,hadoop-master主機建立一個新帳號</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#建立一個文字檔"><span class="toc-text">建立一個文字檔</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#使用hadoop語法-上傳至HDFS時-出現錯誤訊息"><span class="toc-text">使用hadoop語法,上傳至HDFS時,出現錯誤訊息</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#將-user-hadoop權限變更"><span class="toc-text">將/user/hadoop權限變更</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#使用popal帳號再傳送一次檔案"><span class="toc-text">使用popal帳號再傳送一次檔案</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#觀察檔案是否有上傳至HDFS"><span class="toc-text">觀察檔案是否有上傳至HDFS</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#將原本權限復原"><span class="toc-text">將原本權限復原</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#將test1-txt的擁有者變更成hadoop"><span class="toc-text">將test1.txt的擁有者變更成hadoop</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#使用popal使用者變更test1-txt內容"><span class="toc-text">使用popal使用者變更test1.txt內容</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#將-user-hadoop-test1-txt的擁有者變更"><span class="toc-text">將/user/hadoop/test1.txt的擁有者變更</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#使用popal的帳號再變更-user-hadoop-test1-txt"><span class="toc-text">使用popal的帳號再變更/user/hadoop/test1.txt</span></a></li></ol></li></ol>
</div>

        <h4 id="使用到的HDFS語法整理"><a href="#使用到的HDFS語法整理" class="headerlink" title="使用到的HDFS語法整理"></a>使用到的HDFS語法整理</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hadoop fs -ls -d /user/hadoop</div><div class="line">hadoop fs -put ~/test1.txt /user/hadoop</div><div class="line">hadoop fs -chmod -R 757 /user/hadoop</div><div class="line">hadoop fs -ls  /user/hadoop</div><div class="line">hadoop fs -chown hadoop /user/hadoop/test1.txt</div><div class="line">hadoop fs -appendToFile - /user/hadoop/test1.txt</div><div class="line">hadoop fs -cat /user/hadoop/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="以下為操作驗證"><a href="#以下為操作驗證" class="headerlink" title="以下為操作驗證"></a>以下為操作驗證</h4><h5 id="使用hadoop語法查看目錄權限"><a href="#使用hadoop語法查看目錄權限" class="headerlink" title="使用hadoop語法查看目錄權限"></a>使用hadoop語法查看目錄權限</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$hadoop fs -ls -d /user/hadoop</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2017-12-27 18:39 /user/hadoop</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="使用putty-以hadoop帳號-登入-hadoop-master主機建立一個新帳號"><a href="#使用putty-以hadoop帳號-登入-hadoop-master主機建立一個新帳號" class="headerlink" title="使用putty(以hadoop帳號)登入,hadoop-master主機建立一個新帳號"></a>使用putty(以hadoop帳號)登入,hadoop-master主機建立一個新帳號</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$sudo useradd -m popal</div><div class="line">hadoop@hadoop-master:~$ls -la /home</div><div class="line">hadoop@hadoop-master:~$sudo passwd popal</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="建立一個文字檔"><a href="#建立一個文字檔" class="headerlink" title="建立一個文字檔"></a>建立一個文字檔</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$su - popal</div><div class="line">popal@hadoop-master:~$vi ~/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="使用hadoop語法-上傳至HDFS時-出現錯誤訊息"><a href="#使用hadoop語法-上傳至HDFS時-出現錯誤訊息" class="headerlink" title="使用hadoop語法,上傳至HDFS時,出現錯誤訊息"></a>使用hadoop語法,上傳至HDFS時,出現錯誤訊息</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -put ~/test1.txt /user/hadoop</div><div class="line">Error:&quot;put: Permission denied: user=popal, access=WRITE, inode=&quot;/user/hadoop&quot;:hadoop:supergroup:drwxr-xr-x&quot;</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="將-user-hadoop權限變更"><a href="#將-user-hadoop權限變更" class="headerlink" title="將/user/hadoop權限變更"></a>將/user/hadoop權限變更</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">popal@hadoop-master:~$su - hadoop</div><div class="line">hadoop@hadoop-master:~$hadoop fs -chmod -R 757 /user/hadoop</div><div class="line">hadoop@hadoop-master:~$hadoop fs -ls -d /user/hadoop</div><div class="line">drwxr-xrwx   - hadoop supergroup          0 2017-12-27 18:39 /user/hadoop</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="使用popal帳號再傳送一次檔案"><a href="#使用popal帳號再傳送一次檔案" class="headerlink" title="使用popal帳號再傳送一次檔案"></a>使用popal帳號再傳送一次檔案</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$su - popal</div><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -put ~/test1.txt /user/hadoop</div><div class="line">(沒有任何錯誤訊息,並傳送成功)</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="觀察檔案是否有上傳至HDFS"><a href="#觀察檔案是否有上傳至HDFS" class="headerlink" title="觀察檔案是否有上傳至HDFS"></a>觀察檔案是否有上傳至HDFS</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -ls  /user/hadoop</div><div class="line">-rw-r--r--   3 popal  supergroup         95 2018-01-02 10:43 /user/hadoop/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="將原本權限復原"><a href="#將原本權限復原" class="headerlink" title="將原本權限復原"></a>將原本權限復原</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">popal@hadoop-master:~$su - hadoop</div><div class="line">hadoop@hadoop-master:~$hadoop fs -chmod -R 755 /user/hadoop</div><div class="line">hadoop@hadoop-master:~$hadoop fs -ls /user/hadoop</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="將test1-txt的擁有者變更成hadoop"><a href="#將test1-txt的擁有者變更成hadoop" class="headerlink" title="將test1.txt的擁有者變更成hadoop"></a>將test1.txt的擁有者變更成hadoop</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$hadoop fs -chown hadoop /user/hadoop/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="使用popal使用者變更test1-txt內容"><a href="#使用popal使用者變更test1-txt內容" class="headerlink" title="使用popal使用者變更test1.txt內容"></a>使用popal使用者變更test1.txt內容</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$su - popal</div><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -appendToFile - /user/hadoop/test1.txt</div><div class="line">Error&quot;appendToFile: Permission denied: user=popal, access=WRITE, inode=&quot;/user/hadoop/test1.txt&quot;:hadoop:supergroup:-rwxr-xr-x&quot;</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="將-user-hadoop-test1-txt的擁有者變更"><a href="#將-user-hadoop-test1-txt的擁有者變更" class="headerlink" title="將/user/hadoop/test1.txt的擁有者變更"></a>將/user/hadoop/test1.txt的擁有者變更</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">popal@hadoop-master:~$su - hadoop</div><div class="line">hadoop@hadoop-master:~$hadoop fs -chown popal /user/hadoop/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="使用popal的帳號再變更-user-hadoop-test1-txt"><a href="#使用popal的帳號再變更-user-hadoop-test1-txt" class="headerlink" title="使用popal的帳號再變更/user/hadoop/test1.txt"></a>使用popal的帳號再變更/user/hadoop/test1.txt</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$su - popal</div><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -appendToFile - /user/hadoop/test1.txt</div><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -cat /user/hadoop/test1.txt</div></pre></td></tr></table></figure></blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/01/02/[hadoop]Hadoop_permission_memo/" data-id="ckcx1jzc8000gnsh4awnbjkog" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-[hadoop]hadoop_fs_cmd" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/02/[hadoop]hadoop_fs_cmd/" class="article-date">
  <time datetime="2018-01-01T16:00:00.000Z" itemprop="datePublished">2018-01-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/hadoop/">hadoop</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/02/[hadoop]hadoop_fs_cmd/">hadoop fs command Memo</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
<div id="toc">
  <ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#建立與刪除目錄"><span class="toc-text">建立與刪除目錄</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#檔案建立-複製-刪除-搬移"><span class="toc-text">檔案建立/複製/刪除/搬移</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#put-get操作檔案"><span class="toc-text">put/get操作檔案</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#copy-move操作檔案"><span class="toc-text">copy/move操作檔案</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#查看檔案內容"><span class="toc-text">查看檔案內容</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#檢視壓縮檔案內容-tar-gz"><span class="toc-text">檢視壓縮檔案內容(.tar.gz)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#檢視檔案內容-且資料append到檔案時-會馬上呈現"><span class="toc-text">檢視檔案內容,且資料append到檔案時,會馬上呈現</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#以下使用圖片說明"><span class="toc-text">以下使用圖片說明</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#建立空檔案vs-清空檔案內容"><span class="toc-text">建立空檔案vs.清空檔案內容</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用hadoop-fs-find找檔案"><span class="toc-text">使用hadoop fs -find找檔案</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用hadoop-fs-df-du查看檔案目錄的大小"><span class="toc-text">使用hadoop fs -df/-du查看檔案目錄的大小</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用hadoop-fs-ls查看檔案目錄相關資訊"><span class="toc-text">使用hadoop fs -ls查看檔案目錄相關資訊</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用hadoop-fs-count查看檔案目錄相關資訊"><span class="toc-text">使用hadoop fs -count查看檔案目錄相關資訊</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用hadoop-fs-stat查看檔案目錄相關資訊"><span class="toc-text">使用hadoop fs -stat查看檔案目錄相關資訊</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用hadoop-fs-test檔案測試"><span class="toc-text">使用hadoop fs -test檔案測試</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用hadoop-fs-setrep-設定檔案複本數"><span class="toc-text">使用hadoop fs -setrep 設定檔案複本數</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用hadoop-fs-rm刪除檔案"><span class="toc-text">使用hadoop fs -rm刪除檔案</span></a></li></ol>
</div>

        <h4 id="建立與刪除目錄"><a href="#建立與刪除目錄" class="headerlink" title="建立與刪除目錄"></a>建立與刪除目錄</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">##建立目錄但不會將上層目錄一併建立完成</div><div class="line">hadoop@hadoop-master:~$hadoop fs -mkdir /user/hadoop/example/lab_1</div><div class="line">mkdir: `/user/hadoop/example/lab_1&apos;: No such file or directory</div><div class="line"></div><div class="line">##建立目錄,會連同上層目錄一併建立</div><div class="line">hadoop@hadoop-master:~$hadoop fs -mkdir -p /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$hadoop fs ls -d /user/hadoop/example</div><div class="line"></div><div class="line">##刪除一個空目錄</div><div class="line">hadoop@hadoop-master:~$hadoop fs -rmdir /user/hadoop/example/lab_1</div><div class="line"></div><div class="line">##透過std in 建立一個test1.txt檔案</div><div class="line">hadoop@hadoop-master:~$hadoop fs -put -f - /user/hadoop/example/test1.txt</div><div class="line"></div><div class="line">##無法刪除,目錄下有檔案</div><div class="line">hadoop@hadoop-master:~$hadoop fs -rmdir  /user/hadoop/example</div><div class="line">&quot;rmdir: `/user/hadoop/example&apos;: Directory is not empty&quot;</div><div class="line"></div><div class="line">##將所有檔案刪除完成後,刪除目錄</div><div class="line">hadoop@hadoop-master:~$hadoop fs -rm -r -f  /user/hadoop/example			</div><div class="line"></div><div class="line">##建立目錄</div><div class="line">hadoop@hadoop-master:~$hadoop fs -mkdir -p /user/hadoop/example/lab_1</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="檔案建立-複製-刪除-搬移"><a href="#檔案建立-複製-刪除-搬移" class="headerlink" title="檔案建立/複製/刪除/搬移"></a>檔案建立/複製/刪除/搬移</h4><h5 id="put-get操作檔案"><a href="#put-get操作檔案" class="headerlink" title="put/get操作檔案"></a>put/get操作檔案</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$rm -rf  ~/get_hdfs_test2.txt</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$dd if=/dev/urandom of=test2.txt bs=1K count=1</div><div class="line">1+0 records in</div><div class="line">1+0 records out</div><div class="line">1024 bytes (1.0 kB, 1.0 KiB) copied, 0.00234966 s, 436 kB/s</div><div class="line"></div><div class="line">##使用PUT上傳檔案到HDFS</div><div class="line">hadoop@hadoop-master:~$hadoop fs -put ~/test2.txt /user/hadoop/example/</div><div class="line">hadoop@hadoop-master:~$hadoop fs -ls /user/hadoop/example</div><div class="line">Found 2 items</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 15:40 /user/hadoop/example/lab_1</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/test2.txt</div><div class="line"></div><div class="line">##使用GET下載檔案到Local</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -get /user/hadoop/example/test2.txt ~/get_hdfs_test2.txt</div><div class="line">hadoop@hadoop-master:~$ ls -la ~/get*</div><div class="line">-rw-r--r-- 1 hadoop hadoop 1024 Jan  2 15:44 /home/hadoop/get_hdfs_test2.txt</div><div class="line"></div><div class="line"></div><div class="line">##使用getmerge結合數個檔案內容,並下載至Local</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/lab_1/test1.txt</div><div class="line">1,&quot;test_001&quot;,10000</div><div class="line">2,&quot;test_002&quot;,20000</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/lab_1/test2.txt</div><div class="line">3,&quot;test_003&quot;,30000</div><div class="line">4,&quot;test_004&quot;,40000</div><div class="line">hadoop@hadoop-master:~$ rm -rf ~/merge_file.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -getmerge -nl /user/hadoop/example/lab_1/* ~/merge_file.txt</div><div class="line">hadoop@hadoop-master:~$ ls -la ~/merge_file.txt</div><div class="line">-rw-r--r-- 1 hadoop hadoop 78 Jan  2 15:53 /home/hadoop/merge_file.txt</div><div class="line">hadoop@hadoop-master:~$ cat ~/merge_file.txt</div><div class="line">1,&quot;test_001&quot;,10000</div><div class="line">2,&quot;test_002&quot;,20000</div><div class="line"></div><div class="line">3,&quot;test_003&quot;,30000</div><div class="line">4,&quot;test_004&quot;,40000</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="copy-move操作檔案"><a href="#copy-move操作檔案" class="headerlink" title="copy/move操作檔案"></a>copy/move操作檔案</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div></pre></td><td class="code"><pre><div class="line">## copyfromlocal/copytolocal操作</div><div class="line">hadoop@hadoop-master:~$ dd if=/dev/urandom of=test3.txt bs=1K count=1</div><div class="line">1+0 records in</div><div class="line">1+0 records out</div><div class="line">1024 bytes (1.0 kB, 1.0 KiB) copied, 0.00109903 s, 932 kB/s</div><div class="line">hadoop@hadoop-master:~$ rm -rf ~/cp_hdfs_tolocal_test3.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -copyFromLocal ~/test3.txt /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 3 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -copyToLocal  /user/hadoop/example/lab_1/test3.txt ~/cp_hdfs_tolocal_test3.txt</div><div class="line">hadoop@hadoop-master:~$ ls -la ~/cp_hdfs_tolocal_test3.txt</div><div class="line">-rw-r--r-- 1 hadoop hadoop 1024 Jan  2 16:01 /home/hadoop/cp_hdfs_tolocal_test3.txt</div><div class="line">hadoop@hadoop-master:~$</div><div class="line"></div><div class="line">## cp操作(同一個HDFS中資料複製)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 3 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cp /user/hadoop/example/test2.txt  /user/hadoop/example/lab_1/test4.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 4 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example</div><div class="line">Found 2 items</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 16:14 /user/hadoop/example/lab_1</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/test2.txt</div><div class="line"></div><div class="line">## cp操作(不同HDFS中資料複製)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cp hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test5.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 5 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test5.txt</div><div class="line">id,end_date,start_date,location</div><div class="line">1,2015-10-14 00:00:00,2015-09-14 00:00:00,CA-SF</div><div class="line">2,2015-10-15 01:00:20,2015-08-14 00:00:00,CA-SD</div><div class="line">3,2015-10-16 02:30:00,2015-01-14 00:00:00,NY-NY</div><div class="line">4,2015-10-17 03:00:20,2015-02-14 00:00:00,NY-NY</div><div class="line">5,2015-10-18 04:30:00,2014-04-14 00:00:00,CA-SD</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls hdfs://172.20.22.95:8020/user/hadoop/tmp</div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2017-11-22 17:09 hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv</div><div class="line"></div><div class="line">## moveFromLocal/moveToLocal</div><div class="line">hadoop@hadoop-master:~$ dd if=/dev/urandom of=test6.txt bs=1K count=1</div><div class="line">1+0 records in</div><div class="line">1+0 records out</div><div class="line">1024 bytes (1.0 kB, 1.0 KiB) copied, 0.00116886 s, 876 kB/s</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -moveFromLocal ~/test6.txt /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 6 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -moveToLocal /user/hadoop/example/test6.txt ~/mv_hdfs_tolocal_test6.txt</div><div class="line">moveToLocal: Option &apos;-moveToLocal&apos; is not implemented yet.</div><div class="line"></div><div class="line">## mv操作(同一個HDFS中資料複製),被搬移之後檔案會不見</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 6 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mv /user/hadoop/example/test2.txt /user/hadoop/example/lab_1/test7.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 7 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example</div><div class="line">Found 1 items</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 16:21 /user/hadoop/example/lab_1</div><div class="line"></div><div class="line">## mv操作(不同一個HDFS中資料複製),如果hadoop版本不一致無法使用mv搬移檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls hdfs://172.20.22.95:8020/user/hadoop/tmp</div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2017-11-22 17:09 hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls hdfs://192.168.51.4:8020/user/hadoop/example/lab_1</div><div class="line">Found 7 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test7.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mv hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test8.txt</div><div class="line">mv: `hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv&apos;: Does not match target filesystem</div><div class="line">hadoop@hadoop-master:~$  hadoop fs -cp hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv  hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test8.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls hdfs://192.168.51.4:8020/user/hadoop/example/lab_1</div><div class="line">Found 8 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test8.txt</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="查看檔案內容"><a href="#查看檔案內容" class="headerlink" title="查看檔案內容"></a>查看檔案內容</h4><h5 id="檢視壓縮檔案內容-tar-gz"><a href="#檢視壓縮檔案內容-tar-gz" class="headerlink" title="檢視壓縮檔案內容(.tar.gz)"></a>檢視壓縮檔案內容(.tar.gz)</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ tar -zcvf ~/test9.tar.gz &quot;test1.csv&quot; &quot;test2.csv&quot; &quot;test3.csv&quot;</div><div class="line">test1.csv</div><div class="line">test2.csv</div><div class="line">test3.csv</div><div class="line">hadoop@hadoop-master:~$ tar -ztvf ~/test9.tar.gz</div><div class="line">-rw-rw-r-- hadoop/hadoop   272 2017-11-08 15:49 test1.csv</div><div class="line">-rw-rw-r-- hadoop/hadoop   300 2017-11-06 19:14 test2.csv</div><div class="line">-rw-rw-r-- hadoop/hadoop   422 2017-08-21 16:30 test3.csv</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put ~/test9.tar.gz /user/hadoop/example/lab_1/</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 9 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">##可以使用hadoop fs -text來查看壓縮檔案內容</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -text /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line">test1.csv0000664000175100017510000000042013200533434012136 0ustar  hadoophadoop100,1,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.1&quot;</div><div class="line">200,2,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.2&quot;</div><div class="line">150,3,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.3&quot;</div><div class="line">300,4,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.4&quot;</div><div class="line">..........</div><div class="line">5,&quot;http://www.google.com&quot;;&quot;192.168.1.5&quot;;&quot;201&quot;,&quot;005&quot;;&quot;test1_005&quot;;&quot;500&quot;</div><div class="line">6,&quot;http://www.google.com&quot;;&quot;192.168.1.6&quot;;&quot;403&quot;,&quot;006&quot;;&quot;test1_006&quot;;&quot;1600&quot;</div><div class="line"></div><div class="line">##hadoop fs -cat查看壓縮檔時,會出現亂碼,需要配合zcat指令來查看</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test9.tar.gz | zcat</div><div class="line">test1.csv0000664000175100017510000000042013200533434012136 0ustar  hadoophadoop100,1,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.1&quot;</div><div class="line">200,2,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.2&quot;</div><div class="line">150,3,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.3&quot;</div><div class="line">300,4,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.4&quot;</div><div class="line">..........</div><div class="line">5,&quot;http://www.google.com&quot;;&quot;192.168.1.5&quot;;&quot;201&quot;,&quot;005&quot;;&quot;test1_005&quot;;&quot;500&quot;</div><div class="line">6,&quot;http://www.google.com&quot;;&quot;192.168.1.6&quot;;&quot;403&quot;,&quot;006&quot;;&quot;test1_006&quot;;&quot;1600&quot;</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="檢視檔案內容-且資料append到檔案時-會馬上呈現"><a href="#檢視檔案內容-且資料append到檔案時-會馬上呈現" class="headerlink" title="檢視檔案內容,且資料append到檔案時,會馬上呈現"></a>檢視檔案內容,且資料append到檔案時,會馬上呈現</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">##hadoop fs -tail指令的功能(與cat/text相同可以觀察檔案內容)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -tail /user/hadoop/example/lab_1/test1.txt</div><div class="line">1,&quot;test_001&quot;,10000</div><div class="line">2,&quot;test_002&quot;,20000</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="以下使用圖片說明"><a href="#以下使用圖片說明" class="headerlink" title="以下使用圖片說明"></a>以下使用圖片說明</h6><blockquote>
<p>hadoop fs -tail時,可以查看檔案完整內容<br><img src="/images/hadoop_fs_cmd/tail_1.jpg" alt=""><br>hadoop fs -tail -f 時,持續呈現檔案Append的相關內容<br><img src="/images/hadoop_fs_cmd/tail_2_1.jpg" alt=""><br><img src="/images/hadoop_fs_cmd/tail_2_2.jpg" alt=""></p>
</blockquote>
<h4 id="建立空檔案vs-清空檔案內容"><a href="#建立空檔案vs-清空檔案內容" class="headerlink" title="建立空檔案vs.清空檔案內容"></a>建立空檔案vs.清空檔案內容</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line">##使用 -touchz建立一個空的檔案,常用於建立時戳或者flag檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 9 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         76 2018-01-02 17:28 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -touchz /user/hadoop/example/lab_1/test10.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 10 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         76 2018-01-02 17:28 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test1.txt</div><div class="line">1,&quot;test_001&quot;,10000</div><div class="line">2,&quot;test_002&quot;,20000</div><div class="line">3,&quot;test_003&quot;,30000</div><div class="line">4,&quot;test_004&quot;,40000</div><div class="line"></div><div class="line">## -truncate -w 10,只留10 bytes資料</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -truncate -w 10 /user/hadoop/example/lab_1/test1.txt</div><div class="line">Waiting for /user/hadoop/example/lab_1/test1.txt ...</div><div class="line">Truncated /user/hadoop/example/lab_1/test1.txt to length: 10</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test1.txt</div><div class="line">1,&quot;test_00</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         10 2018-01-02 17:42 /user/hadoop/example/lab_1/test1.txt</div><div class="line"></div><div class="line">## -truncate -w 0,資料全部清空</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -truncate -w 0 /user/hadoop/example/lab_1/test1.txt</div><div class="line">Truncated /user/hadoop/example/lab_1/test1.txt to length: 0</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test1.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-find找檔案"><a href="#使用hadoop-fs-find找檔案" class="headerlink" title="使用hadoop fs -find找檔案"></a>使用hadoop fs -find找檔案</h4><blockquote>
<p>此部分可以參考Linux find指令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">說明:</div><div class="line">-iname pattern 所要查找的文檔名，不區分大小寫</div><div class="line">-name pattern  所要查找的文檔名，區分大小寫</div><div class="line">-print 換行列印</div><div class="line">-print0 連續列印</div><div class="line"></div><div class="line">##-name後,可接欲找尋的檔案名稱,&quot;*&quot;代表任何值</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -find / -name &quot;test*.txt&quot; -print</div><div class="line">/user/hadoop/example/lab_1/test1.txt</div><div class="line">/user/hadoop/example/lab_1/test10.txt</div><div class="line">/user/hadoop/example/lab_1/test2.txt</div><div class="line">/user/hadoop/example/lab_1/test3.txt</div><div class="line">/user/hadoop/example/lab_1/test4.txt</div><div class="line">/user/hadoop/example/lab_1/test5.txt</div><div class="line">/user/hadoop/example/lab_1/test6.txt</div><div class="line">/user/hadoop/example/lab_1/test7.txt</div><div class="line">/user/hadoop/example/lab_1/test8.txt</div><div class="line">/user/hadoop/test1.txt</div><div class="line">##-name後,可接正規式的相關參數</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -find / -name &quot;[A-Z,a-z]*.db&quot; -print</div><div class="line">/user/hive/warehouse/test1.db</div><div class="line">/user/hive/warehouse/test2.db</div><div class="line">/user/hive/warehouse/test3.db</div><div class="line">/user/hive/warehouse/test4.db</div><div class="line">/user/hive/warehouse/test5.db</div><div class="line">/user/hive/warehouse/test6.db</div></pre></td></tr></table></figure></p>
</blockquote>
<h4 id="使用hadoop-fs-df-du查看檔案目錄的大小"><a href="#使用hadoop-fs-df-du查看檔案目錄的大小" class="headerlink" title="使用hadoop fs -df/-du查看檔案目錄的大小"></a>使用hadoop fs -df/-du查看檔案目錄的大小</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">##後面如果都沒接任何參數,看各個檔案的大小(以byte為單位)及檔名</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -du /user/hadoop/example/lab_1</div><div class="line">0     0     /user/hadoop/example/lab_1/test1.txt</div><div class="line">0     0     /user/hadoop/example/lab_1/test10.txt</div><div class="line">38    76    /user/hadoop/example/lab_1/test2.txt</div><div class="line">1024  2048  /user/hadoop/example/lab_1/test3.txt</div><div class="line">1024  2048  /user/hadoop/example/lab_1/test4.txt</div><div class="line">272   544   /user/hadoop/example/lab_1/test5.txt</div><div class="line">1024  2048  /user/hadoop/example/lab_1/test6.txt</div><div class="line">1024  2048  /user/hadoop/example/lab_1/test7.txt</div><div class="line">272   544   /user/hadoop/example/lab_1/test8.txt</div><div class="line">373   746   /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">##加入&quot;-h&quot;,在顯示檔案大小時,會將單位標示出來</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -du -h /user/hadoop/example/lab_1</div><div class="line">0    0    /user/hadoop/example/lab_1/test1.txt</div><div class="line">0    0    /user/hadoop/example/lab_1/test10.txt</div><div class="line">38   76   /user/hadoop/example/lab_1/test2.txt</div><div class="line">1 K  2 K  /user/hadoop/example/lab_1/test3.txt</div><div class="line">1 K  2 K  /user/hadoop/example/lab_1/test4.txt</div><div class="line">272  544  /user/hadoop/example/lab_1/test5.txt</div><div class="line">1 K  2 K  /user/hadoop/example/lab_1/test6.txt</div><div class="line">1 K  2 K  /user/hadoop/example/lab_1/test7.txt</div><div class="line">272  544  /user/hadoop/example/lab_1/test8.txt</div><div class="line">373  746  /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">##加入&quot;-s&quot;,最主要是用來顯示該目錄下所有檔案的summary後的大小</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -du -h -s /user/hadoop/example/lab_1</div><div class="line">4.9 K  9.9 K  /user/hadoop/example/lab_1</div><div class="line"></div><div class="line">##hadoop fs -du 加入&quot;-v&quot;,可以看到每個欄位的標題</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -du -h -v  /user/hadoop/example/lab_1</div><div class="line">SIZE  DISK_SPACE_CONSUMED_WITH_ALL_REPLICAS  FULL_PATH_NAME</div><div class="line">0     0                                      /user/hadoop/example/lab_1/test1.txt</div><div class="line">0     0                                      /user/hadoop/example/lab_1/test10.txt</div><div class="line">38    76                                     /user/hadoop/example/lab_1/test2.txt</div><div class="line">1 K   2 K                                    /user/hadoop/example/lab_1/test3.txt</div><div class="line">1 K   2 K                                    /user/hadoop/example/lab_1/test4.txt</div><div class="line">272   544                                    /user/hadoop/example/lab_1/test5.txt</div><div class="line">1 K   2 K                                    /user/hadoop/example/lab_1/test6.txt</div><div class="line">1 K   2 K                                    /user/hadoop/example/lab_1/test7.txt</div><div class="line">272   544                                    /user/hadoop/example/lab_1/test8.txt</div><div class="line">373   746                                    /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">## hadoop fs -df 顯示HDFS系統(多個)剩餘空間</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -df hdfs://172.20.22.95:8020 hdfs://192.168.51.4:8020</div><div class="line">Filesystem                         Size         Used      Available  Use%</div><div class="line">hdfs://172.20.22.95:8020  2232839094272  29245636608  1911548899328    1%</div><div class="line">hdfs://192.168.51.4:8020    42002972672     47734784    26333745152    0%</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-ls查看檔案目錄相關資訊"><a href="#使用hadoop-fs-ls查看檔案目錄相關資訊" class="headerlink" title="使用hadoop fs -ls查看檔案目錄相關資訊"></a>使用hadoop fs -ls查看檔案目錄相關資訊</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">##</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example</div><div class="line">Found 2 items</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1</div><div class="line">-rw-r--r--   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">##-r  Reverse the order of the sort</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -r /user/hadoop/example</div><div class="line">Found 2 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1</div><div class="line"></div><div class="line">## 檢視&quot;-R&quot;遞回目錄下所有檔案及目錄</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R /user/hadoop/example</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line">-rw-r--r--   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">## &quot;-d&quot;只顯示目前只定的檔案或目錄</div><div class="line">hadoop fs -ls -d /user/hadoop/example</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 18:29 /user/hadoop/example</div><div class="line"></div><div class="line">## &quot;-e&quot; Display the erasure coding policy of files and directories.</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -e /user/hadoop/example/lab_1</div><div class="line">Found 10 items</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-count查看檔案目錄相關資訊"><a href="#使用hadoop-fs-count查看檔案目錄相關資訊" class="headerlink" title="使用hadoop fs -count查看檔案目錄相關資訊"></a>使用hadoop fs -count查看檔案目錄相關資訊</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">##計算指定的目錄或檔案的數量大小</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count &quot;/user/hadoop/example/*&quot;</div><div class="line">           1           10               5051 /user/hadoop/example/lab_1</div><div class="line">           0            1                 36 /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">## &quot;-q&quot;選項會多出以下欄位</div><div class="line">## QUOTA,REM_QUOTA,SPACE_QUOTA,REM_SPACE_QUOTA,DIR_COUNT,FILE_COUNT,CONTENT_SIZE,PATHNAME	   </div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -q &quot;/user/hadoop/example/*&quot;</div><div class="line">        none             inf            none             inf            1           10               5051 /user/hadoop/example/lab_1</div><div class="line">        none             inf            none             inf            0            1                 36 /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">## &quot;-u&quot; option shows the quota and the usage against the quota without the detailed content summary</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -u &quot;/user/hadoop/example/*&quot;</div><div class="line">        none             inf            none             inf /user/hadoop/example/lab_1</div><div class="line">        none             inf            none             inf /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">## &quot;-e&quot; option shows the erasure coding policy.</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -e &quot;/user/hadoop/example/*&quot;</div><div class="line">           1           10               5051 EC: /user/hadoop/example/lab_1</div><div class="line">           0            1                 36 Replicated /user/hadoop/example/test.txt</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-stat查看檔案目錄相關資訊"><a href="#使用hadoop-fs-stat查看檔案目錄相關資訊" class="headerlink" title="使用hadoop fs -stat查看檔案目錄相關資訊"></a>使用hadoop fs -stat查看檔案目錄相關資訊</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">##相關-stat Format說明</div><div class="line">%a: octal</div><div class="line">%A: symbolic</div><div class="line">%b: filesize in bytes</div><div class="line">%F: type</div><div class="line">%g: group name of owner</div><div class="line">%n: name</div><div class="line">%o: block size</div><div class="line">%r: replication</div><div class="line">%u: user name of owner</div><div class="line">%x,%X :access date</div><div class="line">%y,%Y :modification date</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -stat /user/*</div><div class="line">2018-01-02 09:05:22</div><div class="line">2017-12-26 10:44:41</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -stat &quot;%A,perm:%a,type:%F,%u:%g,size:%b,block size:%o,%r,mtime:%y atime:%x name:%n&quot; &quot;/user/hadoop/example/lab_1/*&quot;</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:0,block size:67108864,2,mtime:2018-01-02 09:45:22 atime:2018-01-02 09:18:30 name:test1.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:0,block size:67108864,2,mtime:2018-01-02 09:38:21 atime:2018-01-02 09:38:21 name:test10.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:38,block size:67108864,2,mtime:2018-01-02 07:51:41 atime:2018-01-02 07:51:16 name:test2.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:1024,block size:67108864,2,mtime:2018-01-02 07:59:28 atime:2018-01-02 07:59:28 name:test3.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:1024,block size:67108864,2,mtime:2018-01-02 08:07:12 atime:2018-01-02 08:07:12 name:test4.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:272,block size:67108864,2,mtime:2018-01-02 08:11:21 atime:2018-01-02 08:11:21 name:test5.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:1024,block size:67108864,2,mtime:2018-01-02 08:14:24 atime:2018-01-02 08:14:23 name:test6.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:1024,block size:67108864,2,mtime:2018-01-02 07:40:46 atime:2018-01-02 07:40:46 name:test7.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:272,block size:67108864,2,mtime:2018-01-02 08:31:28 atime:2018-01-02 08:31:28 name:test8.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:373,block size:67108864,2,mtime:2018-01-02 09:05:38 atime:2018-01-02 09:05:37 name:test9.tar.gz</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-test檔案測試"><a href="#使用hadoop-fs-test檔案測試" class="headerlink" title="使用hadoop fs -test檔案測試"></a>使用hadoop fs -test檔案測試</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line">##相關參數說明:</div><div class="line">-d  return 0 if &lt;path&gt; is a directory.</div><div class="line">-e  return 0 if &lt;path&gt; exists.</div><div class="line">-f  return 0 if &lt;path&gt; is a file.</div><div class="line">-s  return 0 if file &lt;path&gt; is greater than zero bytes in size.</div><div class="line">-w  return 0 if file &lt;path&gt; exists and write permission is granted.</div><div class="line">-r  return 0 if file &lt;path&gt; exists and read permission is granted.</div><div class="line">-z  return 0 if file &lt;path&gt; is zero bytes in size, else return 1.</div><div class="line">---</div><div class="line"></div><div class="line">## &quot;-e&quot; 判斷指定的路徑是否存在(0:是,1:否)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -e /user/hadoop</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -e /user/vagrant</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line"></div><div class="line">## &quot;-d&quot; 判斷是否為目錄(0:是,1:否)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -d /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -d /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line"></div><div class="line"></div><div class="line">## &quot;-f&quot; 判斷是否為目錄(0:是,1:否)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -f /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -f /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line"></div><div class="line">## &quot;-w&quot;判斷檔案是否有寫的權限</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/test.txt</div><div class="line">-rwxr-xr-x   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -w  /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line">hadoop@hadoop-master:~$ su - popal</div><div class="line">Password:</div><div class="line">popal@hadoop-master:~$ /bgdt/hadoop-3.0.0/bin/hadoop fs -test -w  /user/hadoop/example/test.txt</div><div class="line">popal@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -chmod 766 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/test.txt</div><div class="line">-rwxrw-rw-   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ su - popal</div><div class="line">Password:</div><div class="line">popal@hadoop-master:~$ /bgdt/hadoop-3.0.0/bin/hadoop fs -test -w /user/hadoop/example/test.txt</div><div class="line">popal@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line"></div><div class="line">## &quot;-r&quot;判斷檔案是否有讀的權限</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/test.txt</div><div class="line">-rwxrw-rw-   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ su - popal</div><div class="line">Password:</div><div class="line">popal@hadoop-master:~$ /bgdt/hadoop-3.0.0/bin/hadoop fs -test -r /user/hadoop/example/test.txt</div><div class="line">popal@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line">popal@hadoop-master:~$ exit</div><div class="line">logout</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -chmod 700 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/test.txt</div><div class="line">-rwx------   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ su - popal</div><div class="line">Password:</div><div class="line">popal@hadoop-master:~$ /bgdt/hadoop-3.0.0/bin/hadoop fs -test -r /user/hadoop/example/test.txt</div><div class="line">popal@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line"></div><div class="line">## &quot;-z&quot;判斷檔案是否為0 bytes</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 10 items</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -z /user/hadoop/example/lab_1/test1.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -z /user/hadoop/example/lab_1/test2.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line"></div><div class="line">## &quot;-s&quot;判斷檔案是否不為0 bytes</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -s /user/hadoop/example/lab_1/test1.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -s /user/hadoop/example/lab_1/test2.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-setrep-設定檔案複本數"><a href="#使用hadoop-fs-setrep-設定檔案複本數" class="headerlink" title="使用hadoop fs -setrep 設定檔案複本數"></a>使用hadoop fs -setrep 設定檔案複本數</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 10 items</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -setrep -w 1 /user/hadoop/example/lab_1/test1.txt</div><div class="line">Replication 1 set: /user/hadoop/example/lab_1/test1.txt</div><div class="line">Waiting for /user/hadoop/example/lab_1/test1.txt ... done</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 10 items</div><div class="line">-rw-r--r--   1 hadoop supergroup          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-rm刪除檔案"><a href="#使用hadoop-fs-rm刪除檔案" class="headerlink" title="使用hadoop fs -rm刪除檔案"></a>使用hadoop fs -rm刪除檔案</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line">## hadoop fs -rm指令,只能刪檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rm  /user/hadoop/example/lab_1/*</div><div class="line">Deleted /user/hadoop/example/lab_1/test1.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test10.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test2.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test3.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test4.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test5.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test6.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test7.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test8.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">## &quot;-rm&quot;無法刪目錄,如要刪除目錄需要用&quot;-rmdir or -rm -r&quot;</div><div class="line">hadoop fs -rm  /user/hadoop/example</div><div class="line">rm: `/user/hadoop/example&apos;: Is a directory</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rmdir  /user/hadoop/example</div><div class="line">rmdir: `/user/hadoop/example&apos;: Directory is not empty</div><div class="line"></div><div class="line">## &quot;-rmdir&quot;只能刪除空目錄</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rmdir /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">ls: `/user/hadoop/example/lab_1&apos;: No such file or directory</div><div class="line"></div><div class="line">## &quot;-rm -r&quot;,可以刪除所有檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -touchz /user/hadoop/example/lab_1/test1.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rmdir /user/hadoop/example/lab_1</div><div class="line">rmdir: `/user/hadoop/example/lab_1&apos;: Directory is not empty</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rm /user/hadoop/example/lab_1</div><div class="line">rm: `/user/hadoop/example/lab_1&apos;: Is a directory</div><div class="line"></div><div class="line">## &quot;-rm -r&quot;,會將刪除的檔案或目錄先丟到垃圾桶</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rm -r /user/hadoop/example/lab_1</div><div class="line">2018-01-03 11:38:37,066 INFO fs.TrashPolicyDefault: </div><div class="line">Moved: &apos;hdfs://hadoop-master:8020/user/hadoop/example/lab_1&apos; to trash </div><div class="line">at: hdfs://hadoop-master:8020/user/hadoop/.Trash/Current/user/hadoop/example/lab_1</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example</div><div class="line">Found 1 items</div><div class="line">-rwx------   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">##檢查垃圾桶</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R /user/hadoop/.Trash</div><div class="line">drwx------   - hadoop supergroup          0 2018-01-03 11:38 /user/hadoop/.Trash/Current</div><div class="line">drwx------   - hadoop supergroup          0 2018-01-03 11:38 /user/hadoop/.Trash/Current/user</div><div class="line">drwx------   - hadoop supergroup          0 2018-01-03 11:38 /user/hadoop/.Trash/Current/user/hadoop</div><div class="line">drwx------   - hadoop supergroup          0 2018-01-03 11:38 /user/hadoop/.Trash/Current/user/hadoop/example</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-03 11:37 /user/hadoop/.Trash/Current/user/hadoop/example/lab_1</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-03 11:37 /user/hadoop/.Trash/Current/user/hadoop/example/lab_1/test1.txt</div><div class="line"></div><div class="line">## &quot;-rm -r -skipTrash&quot;,會直接刪除目錄或檔案,並且不會進垃圾桶</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rm -r -skipTrash /user/hadoop/example/lab_1</div><div class="line">Deleted /user/hadoop/example/lab_1</div></pre></td></tr></table></figure>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/01/02/[hadoop]hadoop_fs_cmd/" data-id="ckcx1jzcu0012nsh4o5ky6qqc" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-[spark]spark2.2.1_Install" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/12/29/[spark]spark2.2.1_Install/" class="article-date">
  <time datetime="2017-12-28T16:00:00.000Z" itemprop="datePublished">2017-12-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/29/[spark]spark2.2.1_Install/">Spark2.2.1環境安裝說明</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
<div id="toc">
  <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#相關環境說明"><span class="toc-text">相關環境說明</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#環境準備"><span class="toc-text">環境準備</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Install-Linux-OS-略"><span class="toc-text">Install Linux OS(略)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Install-Linux-JDK8-略"><span class="toc-text">Install Linux JDK8(略)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Install-Linux-Hadoop3-0-0-略"><span class="toc-text">Install Linux Hadoop3.0.0(略)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Install-MariaDB-略"><span class="toc-text">Install MariaDB(略)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Install-Hive2-3-略"><span class="toc-text">Install Hive2.3(略)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Download-Spark2-2-1-tagz-file"><span class="toc-text">Download Spark2.2.1 tagz file</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#解壓縮下載的spark-2-2-1-bin-hadoop2-7-tgz-file-並更改目錄名稱-過長"><span class="toc-text">解壓縮下載的spark-2.2.1-bin-hadoop2.7.tgz file,並更改目錄名稱(過長)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#加入SPARK-HOME相關的環境變數"><span class="toc-text">加入SPARK_HOME相關的環境變數</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#執行-spark-shell-for-python"><span class="toc-text">執行 spark shell(for python)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#執行-spark-shell-for-scala"><span class="toc-text">執行 spark shell(for scala)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#執行spark-sql-Shell"><span class="toc-text">執行spark-sql Shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark2-2-1-amp-Hive2-3-2整合測試"><span class="toc-text">Spark2.2.1 & Hive2.3.2整合測試</span></a></li></ol>
</div>

        <h3 id="相關環境說明"><a href="#相關環境說明" class="headerlink" title="相關環境說明"></a>相關環境說明</h3><blockquote>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>HostName</th>
<th>角色</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.168.51.4</td>
<td>hadoop-master</td>
<td>NameNode(NN),SecondaryNameNode, HiveServer2</td>
</tr>
<tr>
<td>192.168.51.5</td>
<td>hadoop-slave1</td>
<td>DataNode(DN1)</td>
</tr>
<tr>
<td>192.168.51.6</td>
<td>hadoop-slave2</td>
<td>DataNode(DN2)</td>
</tr>
</tbody>
</table>
</blockquote>
<h3 id="環境準備"><a href="#環境準備" class="headerlink" title="環境準備"></a>環境準備</h3><h4 id="Install-Linux-OS-略"><a href="#Install-Linux-OS-略" class="headerlink" title="Install Linux OS(略)"></a>Install Linux OS(略)</h4><h4 id="Install-Linux-JDK8-略"><a href="#Install-Linux-JDK8-略" class="headerlink" title="Install Linux JDK8(略)"></a>Install Linux JDK8(略)</h4><h4 id="Install-Linux-Hadoop3-0-0-略"><a href="#Install-Linux-Hadoop3-0-0-略" class="headerlink" title="Install Linux Hadoop3.0.0(略)"></a>Install Linux Hadoop3.0.0(略)</h4><h4 id="Install-MariaDB-略"><a href="#Install-MariaDB-略" class="headerlink" title="Install MariaDB(略)"></a>Install MariaDB(略)</h4><h4 id="Install-Hive2-3-略"><a href="#Install-Hive2-3-略" class="headerlink" title="Install Hive2.3(略)"></a>Install Hive2.3(略)</h4><h3 id="Download-Spark2-2-1-tagz-file"><a href="#Download-Spark2-2-1-tagz-file" class="headerlink" title="Download Spark2.2.1 tagz file"></a>Download Spark2.2.1 tagz file</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wget -P /bgdt http://apache.stu.edu.tw/spark/spark-2.2.1/spark-2.2.1-bin-hadoop2.7.tgz</div><div class="line">wget -P /bgdt http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.39/mysql-connector-java-5.1.39.jar</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="解壓縮下載的spark-2-2-1-bin-hadoop2-7-tgz-file-並更改目錄名稱-過長"><a href="#解壓縮下載的spark-2-2-1-bin-hadoop2-7-tgz-file-並更改目錄名稱-過長" class="headerlink" title="解壓縮下載的spark-2.2.1-bin-hadoop2.7.tgz file,並更改目錄名稱(過長)"></a>解壓縮下載的spark-2.2.1-bin-hadoop2.7.tgz file,並更改目錄名稱(過長)</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tar -zxvf /bgdt/spark-2.2.1-bin-hadoop2.7.tgz -C /bgdt</div><div class="line">mv /bgdt/spark-2.2.1-bin-hadoop2.7 /bgdt/spark-2.2.1</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="加入SPARK-HOME相關的環境變數"><a href="#加入SPARK-HOME相關的環境變數" class="headerlink" title="加入SPARK_HOME相關的環境變數"></a>加入SPARK_HOME相關的環境變數</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">vi ~/.bashrc</div><div class="line"></div><div class="line">##Spark Segement</div><div class="line">export SPARK_HOME=/bgdt/spark-2.2.1</div><div class="line">export PATH=$PATH:$SPARK_HOME/sbin:$SPARK_HOME/bin</div><div class="line"></div><div class="line">source ~/.bashrc</div><div class="line"></div><div class="line">cp /bgdt/mysql-connector-java-5.1.39.jar /bgdt/spark-2.2.1/jars</div><div class="line">ln -s /bgdt/hive-2.3.2/conf/hive-site.xml /bgdt/spark-2.2.1/conf/hive-site.xml</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="執行-spark-shell-for-python"><a href="#執行-spark-shell-for-python" class="headerlink" title="執行 spark shell(for python)"></a>執行 spark shell(for python)</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ pyspark</div><div class="line">Python 2.7.13 (default, Jan 19 2017, 14:48:08)</div><div class="line">[GCC 6.3.0 20170118] on linux2</div><div class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</div><div class="line">Setting default log level to &quot;WARN&quot;.</div><div class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</div><div class="line">2017-12-29 11:34:53,515 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">2017-12-29 11:35:06,762 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</div><div class="line">Welcome to</div><div class="line">      ____              __</div><div class="line">     / __/__  ___ _____/ /__</div><div class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</div><div class="line">   /__ / .__/\_,_/_/ /_/\_\   version 2.2.1</div><div class="line">      /_/</div><div class="line"></div><div class="line">Using Python version 2.7.13 (default, Jan 19 2017 14:48:08)</div><div class="line">SparkSession available as &apos;spark&apos;.</div><div class="line">&gt;&gt;&gt; spark</div><div class="line">&lt;pyspark.sql.session.SparkSession object at 0x7f62b8e37610&gt;</div><div class="line">&gt;&gt;&gt; sc</div><div class="line">&lt;SparkContext master=local[*] appName=PySparkShell&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="執行-spark-shell-for-scala"><a href="#執行-spark-shell-for-scala" class="headerlink" title="執行 spark shell(for scala)"></a>執行 spark shell(for scala)</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ spark-shell</div><div class="line">Setting default log level to &quot;WARN&quot;.</div><div class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</div><div class="line">2017-12-29 11:44:58,352 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">Spark context Web UI available at http://192.168.51.4:4040</div><div class="line">Spark context available as &apos;sc&apos; (master = local[*], app id = local-1514519099800).</div><div class="line">Spark session available as &apos;spark&apos;.</div><div class="line">Welcome to</div><div class="line">      ____              __</div><div class="line">     / __/__  ___ _____/ /__</div><div class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</div><div class="line">   /___/ .__/\_,_/_/ /_/\_\   version 2.2.1</div><div class="line">      /_/</div><div class="line"></div><div class="line">Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_101)</div><div class="line">Type in expressions to have them evaluated.</div><div class="line">Type :help for more information.</div><div class="line"></div><div class="line">scala&gt; spark</div><div class="line">res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7afb9c93</div><div class="line"></div><div class="line">scala&gt; sc</div><div class="line">res1: org.apache.spark.SparkContext = org.apache.spark.SparkContext@14292d71</div><div class="line"></div><div class="line">scala&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="執行spark-sql-Shell"><a href="#執行spark-sql-Shell" class="headerlink" title="執行spark-sql Shell"></a>執行spark-sql Shell</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ spark-sql</div><div class="line">2017-12-29 18:04:01,165 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">2017-12-29 18:04:01,307 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</div><div class="line">2017-12-29 18:04:01,344 INFO metastore.ObjectStore: ObjectStore, initialize called</div><div class="line">2017-12-29 18:04:01,642 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored</div><div class="line">2017-12-29 18:04:01,642 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored</div><div class="line">2017-12-29 18:04:05,450 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=&quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order&quot;</div><div class="line">2017-12-29 18:04:07,495 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</div><div class="line">2017-12-29 18:04:07,497 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</div><div class="line">......</div><div class="line">......</div><div class="line">......</div><div class="line">2017-12-29 18:04:20,967 INFO metastore.HiveMetaStore: 0: get_database: global_temp</div><div class="line">2017-12-29 18:04:20,967 INFO HiveMetaStore.audit: ugi=hadoop    ip=unknown-ip-addr      cmd=get_database: global_temp</div><div class="line">2017-12-29 18:04:20,970 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</div><div class="line">2017-12-29 18:04:21,293 INFO session.SessionState: Created local directory: /tmp/d8b65bb8-f4f6-43f2-b544-39331b237673_resources</div><div class="line">2017-12-29 18:04:21,298 INFO session.SessionState: Created HDFS directory: /tmp/hive/hadoop/d8b65bb8-f4f6-43f2-b544-39331b237673</div><div class="line">2017-12-29 18:04:21,306 INFO session.SessionState: Created local directory: /tmp/hadoop/d8b65bb8-f4f6-43f2-b544-39331b237673</div><div class="line">2017-12-29 18:04:21,310 INFO session.SessionState: Created HDFS directory: /tmp/hive/hadoop/d8b65bb8-f4f6-43f2-b544-39331b237673/_tmp_space.db</div><div class="line">2017-12-29 18:04:21,311 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse</div><div class="line">2017-12-29 18:04:21,406 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint</div><div class="line"></div><div class="line">spark-sql&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="Spark2-2-1-amp-Hive2-3-2整合測試"><a href="#Spark2-2-1-amp-Hive2-3-2整合測試" class="headerlink" title="Spark2.2.1 &amp; Hive2.3.2整合測試"></a>Spark2.2.1 &amp; Hive2.3.2整合測試</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">###開啟第1個putty連線</div><div class="line">hadoop@hadoop-master:~$spark-sql</div><div class="line">spark-sql&gt;create database test1;</div><div class="line">..... INFO execution.SparkSqlParser: Parsing command: create database test1</div><div class="line">..... INFO metastore.HiveMetaStore: 0: create_database: Database(name:test1, description:, locationUri:hdfs://hadoop-master:8020/user/hive/warehouse/test1.db, parameters:&#123;&#125;)</div><div class="line">..... INFO HiveMetaStore.audit: ugi=hadoop    ip=unknown-ip-addr      cmd=create_database: Database(name:test1, description:, locationUri:hdfs://hadoop-master:8020/user/hive/warehouse/test1.db, parameters:&#123;&#125;)</div><div class="line">..... WARN metastore.ObjectStore: Failed to get database test1, returning NoSuchObjectException</div><div class="line">..... INFO common.FileUtils: Creating directory if it doesn&apos;t exist: hdfs://hadoop-master:8020/user/hive/warehouse/test1.db</div><div class="line">Time taken: 0.326 seconds</div><div class="line">..... INFO CliDriver: Time taken: 0.326 seconds</div><div class="line"></div><div class="line">spark-sql&gt;show databases;</div><div class="line">INFO execution.SparkSqlParser: Parsing command: show databases</div><div class="line">INFO metastore.HiveMetaStore: 0: get_databases: *</div><div class="line">HiveMetaStore.audit: ugi=hadoop    ip=unknown-ip-addr      cmd=get_databases: *</div><div class="line">INFO codegen.CodeGenerator: Code generated in 544.163987 ms</div><div class="line">default</div><div class="line">test1</div><div class="line"></div><div class="line">###開啟第2個putty連線</div><div class="line">hadoop@hadoop-master:~$hive</div><div class="line">hive&gt;show databases;</div><div class="line">OK</div><div class="line">default</div><div class="line">test1</div><div class="line">......</div><div class="line"></div><div class="line">hive&gt;create database test2;</div><div class="line">OK</div><div class="line">Time taken: 0.341 seconds</div><div class="line"></div><div class="line">hive&gt;show databases;</div><div class="line">OK</div><div class="line">default</div><div class="line">test1</div><div class="line">test2</div><div class="line">......</div><div class="line"></div><div class="line">spark-sql&gt;show databases;</div><div class="line">default</div><div class="line">test1</div><div class="line">test2</div><div class="line"></div><div class="line">###以上驗證由兩種不同方式所建立的資料庫是可以同步的</div></pre></td></tr></table></figure></blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/12/29/[spark]spark2.2.1_Install/" data-id="ckcx1jzdk001snsh4etv75vzb" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <a class="extend prev" rel="prev" href="/"><<前一頁</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/">下一頁>></a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分類</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Solace/">Solace</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Solace/Solace-Cache/">Solace Cache</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Solace/Solace-Install/">Solace Install</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/excel/">excel</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hadoop/">hadoop</a><span class="category-list-count">19</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hive/">hive</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/vagrant/">vagrant</a><span class="category-list-count">3</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">所有文章</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/07/22/[Linux]Linux-systemctl/">Linux - systemctl Command Usage</a>
          </li>
        
          <li>
            <a href="/2020/07/22/[Solace]SolaceCache/">Solace Cache</a>
          </li>
        
          <li>
            <a href="/2020/07/22/[Solace]SolaceforVirtualbox/">Solace Install for VitualBox</a>
          </li>
        
          <li>
            <a href="/2020/07/22/[solace]SolaceforDocker_Install/">Solace Install for docker</a>
          </li>
        
          <li>
            <a href="/2018/01/09/[hadoop]Hadoop3.0.0_doc/">Hadoop3.0.0 Document List</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">優質連結</h3>
    <div class="widget">
      <a href="http://yenyu-lovelan.blogspot.tw/" target="_blank">欲戴王冠 必先承其重~*</a><br>
	  <a href="http://tomkuo139.blogspot.tw/" target="_blank">昭佑 天翔</a><br>
	  <a href="http://rocksaying.tw/" target="_blank">石頭閒語</a><br>
	  <a href="https://blog.toright.com/" target="_blank">Soul & Shell Blog</a><br>	  
    </div>
  </div>


  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 ~ 2020 popal<br>
      Powered by <a href="http://popalhuang.github.io/" target="_blank">popal Blog</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首頁</a>
  
    <a href="/archives" class="mobile-nav-link">文章</a>
  
    <a href="/categories/hadoop" class="mobile-nav-link">Hadoop</a>
  
    <a href="/about" class="mobile-nav-link">關於我</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>