<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>青子的部落</title>
  <subtitle>邊做邊學</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-07-22T08:15:50.838Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>popal</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Solace Cache</title>
    <link href="http://yoursite.com/2020/07/22/%5BSolace%5DSolaceCache/"/>
    <id>http://yoursite.com/2020/07/22/[Solace]SolaceCache/</id>
    <published>2020-07-21T16:00:00.000Z</published>
    <updated>2020-07-22T08:15:50.838Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Installing-PubSub-Cache"><a href="#Installing-PubSub-Cache" class="headerlink" title="Installing PubSub+ Cache"></a>Installing PubSub+ Cache</h2><h3 id="Product-Key-Feature-Locking"><a href="#Product-Key-Feature-Locking" class="headerlink" title="Product Key Feature Locking"></a>Product Key Feature Locking</h3><h3 id="Downloading-PubSub-Cache"><a href="#Downloading-PubSub-Cache" class="headerlink" title="Downloading PubSub+ Cache"></a>Downloading PubSub+ Cache</h3><h3 id="Installing-PubSub-Cache-Instances-Installing-PubSub-Cache-Instances"><a href="#Installing-PubSub-Cache-Instances-Installing-PubSub-Cache-Instances" class="headerlink" title="Installing PubSub+ Cache Instances Installing PubSub+ Cache Instances"></a>Installing PubSub+ Cache Instances Installing PubSub+ Cache Instances</h3><h4 id="Installing-on-a-Linux-Server-with-systemd"><a href="#Installing-on-a-Linux-Server-with-systemd" class="headerlink" title="Installing on a Linux Server with systemd"></a>Installing on a Linux Server with systemd</h4><h4 id="Installing-on-a-Lunux-Server-with-chkconfig"><a href="#Installing-on-a-Lunux-Server-with-chkconfig" class="headerlink" title="Installing on a Lunux Server with chkconfig"></a>Installing on a Lunux Server with chkconfig</h4><h4 id="Installing-on-a-Linux-Server-without-chkconfig"><a href="#Installing-on-a-Linux-Server-without-chkconfig" class="headerlink" title="Installing on a Linux Server without chkconfig"></a>Installing on a Linux Server without chkconfig</h4><h3 id="Uninstalling-PubSub-Cache"><a href="#Uninstalling-PubSub-Cache" class="headerlink" title="Uninstalling PubSub+ Cache"></a>Uninstalling PubSub+ Cache</h3><h4 id="Uninstall-a-systemd-service"><a href="#Uninstall-a-systemd-service" class="headerlink" title="Uninstall a systemd service"></a>Uninstall a systemd service</h4><h4 id="Uninstall-a-chkconfig-service"><a href="#Uninstall-a-chkconfig-service" class="headerlink" title="Uninstall a chkconfig service"></a>Uninstall a chkconfig service</h4><h4 id="Uninstall-from-a-Linux-server-without-chkconfig"><a href="#Uninstall-from-a-Linux-server-without-chkconfig" class="headerlink" title="Uninstall from a Linux server without chkconfig"></a>Uninstall from a Linux server without chkconfig</h4><h3 id="Environment-Variables-Environment-Variables"><a href="#Environment-Variables-Environment-Variables" class="headerlink" title="Environment Variables Environment Variables"></a>Environment Variables Environment Variables</h3><h4 id="Advanced-Environment-Tuning"><a href="#Advanced-Environment-Tuning" class="headerlink" title="Advanced Environment Tuning"></a>Advanced Environment Tuning</h4><h2 id="Managing-PubSub-Cache-Managing-PubSub-Cache"><a href="#Managing-PubSub-Cache-Managing-PubSub-Cache" class="headerlink" title="Managing PubSub+ Cache Managing PubSub+ Cache"></a>Managing PubSub+ Cache Managing PubSub+ Cache</h2><h3 id="Operating-Limits"><a href="#Operating-Limits" class="headerlink" title="Operating Limits"></a>Operating Limits</h3><h3 id="Steps-to-Configure-PubSub-Cache"><a href="#Steps-to-Configure-PubSub-Cache" class="headerlink" title="Steps to Configure PubSub+ Cache"></a>Steps to Configure PubSub+ Cache</h3><h3 id="Configuring-Message-VPN-for-Caching"><a href="#Configuring-Message-VPN-for-Caching" class="headerlink" title="Configuring Message VPN for Caching"></a>Configuring Message VPN for Caching</h3><h3 id="Configuring-Distributed-Caches"><a href="#Configuring-Distributed-Caches" class="headerlink" title="Configuring Distributed Caches"></a>Configuring Distributed Caches</h3><h4 id="Configuring-Heartbeat-Intervals"><a href="#Configuring-Heartbeat-Intervals" class="headerlink" title="Configuring Heartbeat Intervals"></a>Configuring Heartbeat Intervals</h4><h4 id="Scheduling-Message-Deletes"><a href="#Scheduling-Message-Deletes" class="headerlink" title="Scheduling Message Deletes"></a>Scheduling Message Deletes</h4><h4 id="Enabling-Distributed-Caches"><a href="#Enabling-Distributed-Caches" class="headerlink" title="Enabling Distributed Caches"></a>Enabling Distributed Caches</h4><h3 id="Configuring-Cache-Clusters"><a href="#Configuring-Cache-Clusters" class="headerlink" title="Configuring Cache Clusters"></a>Configuring Cache Clusters</h3><h4 id="Enabling-Deliver-To-One-Overrides"><a href="#Enabling-Deliver-To-One-Overrides" class="headerlink" title="Enabling Deliver-To-One-Overrides"></a>Enabling Deliver-To-One-Overrides</h4><h4 id="Configuring-Event-Thresholds"><a href="#Configuring-Event-Thresholds" class="headerlink" title="Configuring Event Thresholds"></a>Configuring Event Thresholds</h4><h4 id="Configuring-Global-Caching"><a href="#Configuring-Global-Caching" class="headerlink" title="Configuring Global Caching"></a>Configuring Global Caching</h4><h4 id="Configuring-Max-Memory-for-PubSub-Cache-Instances"><a href="#Configuring-Max-Memory-for-PubSub-Cache-Instances" class="headerlink" title="Configuring Max Memory for PubSub+ Cache Instances"></a>Configuring Max Memory for PubSub+ Cache Instances</h4><h4 id="Configuring-Max-Number-of-Messages-Per-Topic"><a href="#Configuring-Max-Number-of-Messages-Per-Topic" class="headerlink" title="Configuring Max Number of Messages Per Topic"></a>Configuring Max Number of Messages Per Topic</h4><h4 id="Configuring-Max-Number-of-Topics"><a href="#Configuring-Max-Number-of-Topics" class="headerlink" title="Configuring Max Number of Topics"></a>Configuring Max Number of Topics</h4><h4 id="Configuring-Message-Lifetimes"><a href="#Configuring-Message-Lifetimes" class="headerlink" title="Configuring Message Lifetimes"></a>Configuring Message Lifetimes</h4><h4 id="Enabling-New-Topic-Advertisements"><a href="#Enabling-New-Topic-Advertisements" class="headerlink" title="Enabling New Topic Advertisements"></a>Enabling New Topic Advertisements</h4><h4 id="Configuring-Request-Queue-Depths"><a href="#Configuring-Request-Queue-Depths" class="headerlink" title="Configuring Request Queue Depths"></a>Configuring Request Queue Depths</h4><h4 id="Assigning-Topics-to-Cache-Clusters"><a href="#Assigning-Topics-to-Cache-Clusters" class="headerlink" title="Assigning Topics to Cache Clusters"></a>Assigning Topics to Cache Clusters</h4><h4 id="Enabling-Cache-Clusters"><a href="#Enabling-Cache-Clusters" class="headerlink" title="Enabling Cache Clusters"></a>Enabling Cache Clusters</h4><h3 id="Configuring-PubSub-Cache-Instances"><a href="#Configuring-PubSub-Cache-Instances" class="headerlink" title="Configuring PubSub+ Cache Instances"></a>Configuring PubSub+ Cache Instances</h3><h4 id="Enabling-Disabling-Auto-Start-Mode"><a href="#Enabling-Disabling-Auto-Start-Mode" class="headerlink" title="Enabling/Disabling Auto Start Mode"></a>Enabling/Disabling Auto Start Mode</h4><h4 id="Configuring-Stop-On-Lost-Message-Behavior"><a href="#Configuring-Stop-On-Lost-Message-Behavior" class="headerlink" title="Configuring Stop On Lost Message Behavior"></a>Configuring Stop On Lost Message Behavior</h4><h4 id="Enabling-Disabling-PubSub-Cache-Instances"><a href="#Enabling-Disabling-PubSub-Cache-Instances" class="headerlink" title="Enabling/Disabling PubSub+ Cache Instances"></a>Enabling/Disabling PubSub+ Cache Instances</h4><h3 id="Customizing-PubSub-Cache-Instance-Configuration-Files"><a href="#Customizing-PubSub-Cache-Instance-Configuration-Files" class="headerlink" title="Customizing PubSub+ Cache Instance Configuration Files"></a>Customizing PubSub+ Cache Instance Configuration Files</h3><h4 id="Changing-PubSub-Cache-Configuration-Files-That-Are-In-Use"><a href="#Changing-PubSub-Cache-Configuration-Files-That-Are-In-Use" class="headerlink" title="Changing PubSub+ Cache Configuration Files That Are In Use"></a>Changing PubSub+ Cache Configuration Files That Are In Use</h4><h3 id="Sample-PubSub-Cache-Configuration"><a href="#Sample-PubSub-Cache-Configuration" class="headerlink" title="Sample PubSub+ Cache Configuration"></a>Sample PubSub+ Cache Configuration</h3><h4 id="Configuration-Information"><a href="#Configuration-Information" class="headerlink" title="Configuration Information"></a>Configuration Information</h4><h4 id="Distributed-PubSub-Cache-Configuration"><a href="#Distributed-PubSub-Cache-Configuration" class="headerlink" title="Distributed PubSub+ Cache Configuration"></a>Distributed PubSub+ Cache Configuration</h4><h3 id="Performing-PubSub-Cache-Administrative-Tasks-Performing-PubSub-Cache-Administrative-Tasks"><a href="#Performing-PubSub-Cache-Administrative-Tasks-Performing-PubSub-Cache-Administrative-Tasks" class="headerlink" title="Performing PubSub+ Cache Administrative Tasks Performing PubSub+ Cache Administrative Tasks"></a>Performing PubSub+ Cache Administrative Tasks Performing PubSub+ Cache Administrative Tasks</h3><h4 id="Clearing-PubSub-Cache-Events"><a href="#Clearing-PubSub-Cache-Events" class="headerlink" title="Clearing PubSub+ Cache Events"></a>Clearing PubSub+ Cache Events</h4><h4 id="Deleting-Cached-Messages"><a href="#Deleting-Cached-Messages" class="headerlink" title="Deleting Cached Messages"></a>Deleting Cached Messages</h4><h4 id="Starting-PubSub-Cache-Instances"><a href="#Starting-PubSub-Cache-Instances" class="headerlink" title="Starting PubSub+ Cache Instances"></a>Starting PubSub+ Cache Instances</h4><h4 id="Backing-Up-Restoring-PubSub-Cache-Instance-Messages"><a href="#Backing-Up-Restoring-PubSub-Cache-Instance-Messages" class="headerlink" title="Backing Up/Restoring PubSub+ Cache Instance Messages"></a>Backing Up/Restoring PubSub+ Cache Instance Messages</h4><h5 id="Backing-up-Cached-Messages"><a href="#Backing-up-Cached-Messages" class="headerlink" title="Backing up Cached Messages"></a>Backing up Cached Messages</h5><h5 id="Restoring-Cached-Messages"><a href="#Restoring-Cached-Messages" class="headerlink" title="Restoring Cached Messages"></a>Restoring Cached Messages</h5><h4 id="Backing-Up-Cached-Messages"><a href="#Backing-Up-Cached-Messages" class="headerlink" title="Backing Up Cached Messages"></a>Backing Up Cached Messages</h4><h4 id="Restoring-Cached-Messages-1"><a href="#Restoring-Cached-Messages-1" class="headerlink" title="Restoring Cached Messages"></a>Restoring Cached Messages</h4><h2 id="Monitoring-PubSub-Cache"><a href="#Monitoring-PubSub-Cache" class="headerlink" title="Monitoring PubSub+ Cache"></a>Monitoring PubSub+ Cache</h2><h4 id="SolCache-Events"><a href="#SolCache-Events" class="headerlink" title="SolCache Events"></a>SolCache Events</h4><h4 id="Configuring-Cache-Cluster-Event-Thresholds-Configuring-Cache-Cluster-Event-Thresholds"><a href="#Configuring-Cache-Cluster-Event-Thresholds-Configuring-Cache-Cluster-Event-Thresholds" class="headerlink" title="Configuring Cache Cluster Event Thresholds Configuring Cache Cluster Event Thresholds"></a>Configuring Cache Cluster Event Thresholds Configuring Cache Cluster Event Thresholds</h4><h5 id="Data-Byte-Rate-Thresholds"><a href="#Data-Byte-Rate-Thresholds" class="headerlink" title="Data Byte Rate Thresholds"></a>Data Byte Rate Thresholds</h5><h5 id="Data-Message-Rate-Thresholds"><a href="#Data-Message-Rate-Thresholds" class="headerlink" title="Data Message Rate Thresholds"></a>Data Message Rate Thresholds</h5><h5 id="Max-Memory-Thresholds"><a href="#Max-Memory-Thresholds" class="headerlink" title="Max Memory Thresholds"></a>Max Memory Thresholds</h5><h5 id="Max-Topics-Thresholds"><a href="#Max-Topics-Thresholds" class="headerlink" title="Max Topics Thresholds"></a>Max Topics Thresholds</h5><h5 id="Request-Queue-Depth-Thresholds"><a href="#Request-Queue-Depth-Thresholds" class="headerlink" title="Request Queue Depth Thresholds"></a>Request Queue Depth Thresholds</h5><h5 id="Request-Rate-Thresholds"><a href="#Request-Rate-Thresholds" class="headerlink" title="Request Rate Thresholds"></a>Request Rate Thresholds</h5><h5 id="Response-Rate-Thresholds"><a href="#Response-Rate-Thresholds" class="headerlink" title="Response Rate Thresholds"></a>Response Rate Thresholds</h5><h4 id="Monitoring-SolCache-Configuration-and-Operations"><a href="#Monitoring-SolCache-Configuration-and-Operations" class="headerlink" title="Monitoring SolCache Configuration and Operations"></a>Monitoring SolCache Configuration and Operations</h4><h5 id="Show-Product-Key"><a href="#Show-Product-Key" class="headerlink" title="Show Product Key"></a>Show Product Key</h5><h5 id="Show-Distributed-Cache"><a href="#Show-Distributed-Cache" class="headerlink" title="Show Distributed Cache"></a>Show Distributed Cache</h5><h5 id="Show-Cache-Cluster"><a href="#Show-Cache-Cluster" class="headerlink" title="Show Cache Cluster"></a>Show Cache Cluster</h5><h5 id="Show-Cache-Instance"><a href="#Show-Cache-Instance" class="headerlink" title="Show Cache Instance"></a>Show Cache Instance</h5><h5 id="PubSub-Cache-Instance-States-PubSub-Cache-Instance-States"><a href="#PubSub-Cache-Instance-States-PubSub-Cache-Instance-States" class="headerlink" title="PubSub+ Cache Instance States PubSub+ Cache Instance States"></a>PubSub+ Cache Instance States PubSub+ Cache Instance States</h5><h5 id="Clearing-PubSub-Cache-Instance-Statistics"><a href="#Clearing-PubSub-Cache-Instance-Statistics" class="headerlink" title="Clearing PubSub+ Cache Instance Statistics"></a>Clearing PubSub+ Cache Instance Statistics</h5><h4 id="Output-PubSub-Cache-Instance-Debug-Information"><a href="#Output-PubSub-Cache-Instance-Debug-Information" class="headerlink" title="Output PubSub+ Cache Instance Debug Information"></a>Output PubSub+ Cache Instance Debug Information</h4><h4 id="Using-SEMP-to-Monitor-PubSub-Cache"><a href="#Using-SEMP-to-Monitor-PubSub-Cache" class="headerlink" title="Using SEMP to Monitor PubSub+ Cache"></a>Using SEMP to Monitor PubSub+ Cache</h4><h5 id="SEMP-Polling-Frequency-Guidelines"><a href="#SEMP-Polling-Frequency-Guidelines" class="headerlink" title="SEMP Polling Frequency Guidelines"></a>SEMP Polling Frequency Guidelines</h5>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Installing-PubSub-Cache&quot;&gt;&lt;a href=&quot;#Installing-PubSub-Cache&quot; class=&quot;headerlink&quot; title=&quot;Installing PubSub+ Cache&quot;&gt;&lt;/a&gt;Installing PubSu
    
    </summary>
    
      <category term="Solace" scheme="http://yoursite.com/categories/Solace/"/>
    
      <category term="Solace Cache" scheme="http://yoursite.com/categories/Solace/Solace-Cache/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux - systemctl Command Usage</title>
    <link href="http://yoursite.com/2020/07/22/%5BLinux%5DLinux-systemctl/"/>
    <id>http://yoursite.com/2020/07/22/[Linux]Linux-systemctl/</id>
    <published>2020-07-21T16:00:00.000Z</published>
    <updated>2020-07-22T08:37:31.655Z</updated>
    
    <content type="html"><![CDATA[<h2 id="systemctl-command-Usage"><a href="#systemctl-command-Usage" class="headerlink" title="systemctl command Usage"></a>systemctl command Usage</h2><h3 id="透過systemctl管理單一服務-service-unit-的啟動-開機啟動與觀察狀態"><a href="#透過systemctl管理單一服務-service-unit-的啟動-開機啟動與觀察狀態" class="headerlink" title="透過systemctl管理單一服務(service unit)的啟動/開機啟動與觀察狀態"></a>透過systemctl管理單一服務(service unit)的啟動/開機啟動與觀察狀態</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">systemctl [command] [service]</div><div class="line">command 主要有：</div><div class="line">start     ：立刻啟動後面接的service</div><div class="line">stop      ：立刻關閉後面接的service</div><div class="line">restart   ：立刻關閉後啟動後面接的service，亦即執行 stop 再 start 的意思</div><div class="line">reload    ：不關閉後面接的service的情況下，重新載入設定檔，讓設定生效</div><div class="line">enable    ：設定下次開機時，後面接的service 會被啟動</div><div class="line">disable   ：設定下次開機時，後面接的service 不會被啟動</div><div class="line">status    ：目前後面接的這個service 的狀態，會列出有沒有正在執行、開機預設執行否、登錄等資訊等！</div><div class="line">is-active ：目前有沒有正在運作中</div><div class="line">is-enabled：開機時有沒有預設要啟用這個service</div><div class="line"></div><div class="line">Example:</div><div class="line">systemctl enable docker</div><div class="line">systemctl start docker</div><div class="line">systemctl status docker</div><div class="line">systemctl restart docker</div><div class="line">systemctl stop docker</div><div class="line">systemctl disable docker</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="透過-systemctl-觀察系統上所有的服務"><a href="#透過-systemctl-觀察系統上所有的服務" class="headerlink" title="透過 systemctl 觀察系統上所有的服務"></a>透過 systemctl 觀察系統上所有的服務</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">systemctl [command] [--type=TYPE] [--all]</div><div class="line">command:</div><div class="line">    list-units      ：依據 unit 列出目前有啟動的 unit。若加上 --all 才會列出沒啟動的。</div><div class="line">    list-unit-files ：依據 /usr/lib/systemd/system/ 內的檔案，將所有檔案列表說明。</div><div class="line">--type=TYPE：unit type(service, socket, target...)</div><div class="line"></div><div class="line">Example:</div><div class="line">Example:</div><div class="line">systemctl list-units --type=service --all</div><div class="line">systemctl list-units --type=service --all | grep cpu</div><div class="line">systemctl list-unit-files</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="透過-systemctl-管理不同的操作環境-target-unit"><a href="#透過-systemctl-管理不同的操作環境-target-unit" class="headerlink" title="透過 systemctl 管理不同的操作環境 (target unit)"></a>透過 systemctl 管理不同的操作環境 (target unit)</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">systemctl get-default</div><div class="line"></div><div class="line">systemctl poweroff	##關機</div><div class="line">systemctl reboot	##重開機</div><div class="line">systemctl suspend	##暫停</div><div class="line">systemctl hibernate	##休眠</div><div class="line">systemctl rescue	##</div><div class="line">systemctl emergency	##</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="透過-systemctl-分析各服務之間的相依性"><a href="#透過-systemctl-分析各服務之間的相依性" class="headerlink" title="透過 systemctl 分析各服務之間的相依性"></a>透過 systemctl 分析各服務之間的相依性</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">systemctl list-dependencies</div><div class="line">systemctl list-dependencies --reverse</div><div class="line">systemctl list-dependencies graphical.target</div></pre></td></tr></table></figure>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;systemctl-command-Usage&quot;&gt;&lt;a href=&quot;#systemctl-command-Usage&quot; class=&quot;headerlink&quot; title=&quot;systemctl command Usage&quot;&gt;&lt;/a&gt;systemctl command
    
    </summary>
    
      <category term="OS" scheme="http://yoursite.com/categories/OS/"/>
    
      <category term="Linux" scheme="http://yoursite.com/categories/OS/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Solace Install for docker</title>
    <link href="http://yoursite.com/2020/07/22/%5Bsolace%5DSolaceforDocker_Install/"/>
    <id>http://yoursite.com/2020/07/22/[solace]SolaceforDocker_Install/</id>
    <published>2020-07-21T16:00:00.000Z</published>
    <updated>2020-07-22T07:21:06.921Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Solace-for-Docker-環境安裝"><a href="#Solace-for-Docker-環境安裝" class="headerlink" title="Solace for Docker 環境安裝"></a>Solace for Docker 環境安裝</h2><h3 id="File-list-amp-Work-preparation"><a href="#File-list-amp-Work-preparation" class="headerlink" title="File list &amp; Work preparation"></a>File list &amp; Work preparation</h3><h3 id="VM-Create-amp-Package-Install"><a href="#VM-Create-amp-Package-Install" class="headerlink" title="VM Create &amp; Package Install"></a>VM Create &amp; Package Install</h3><h4 id="OS-Install-for-CentOS8-2"><a href="#OS-Install-for-CentOS8-2" class="headerlink" title="OS Install(for CentOS8.2)"></a>OS Install(for CentOS8.2)</h4><h4 id="OS-Network-Setting"><a href="#OS-Network-Setting" class="headerlink" title="OS Network Setting"></a>OS Network Setting</h4><h4 id="OS-start-amp-Package-Install"><a href="#OS-start-amp-Package-Install" class="headerlink" title="OS start &amp; Package Install"></a>OS start &amp; Package Install</h4><h5 id="cifs-package-install"><a href="#cifs-package-install" class="headerlink" title="cifs package install"></a>cifs package install</h5><h4 id="Docker-Install-amp-Setting"><a href="#Docker-Install-amp-Setting" class="headerlink" title="Docker Install&amp;Setting"></a>Docker Install&amp;Setting</h4><h5 id="Docker-Package-Install"><a href="#Docker-Package-Install" class="headerlink" title="Docker Package Install"></a>Docker Package Install</h5><h5 id="Docker-Enable-Start"><a href="#Docker-Enable-Start" class="headerlink" title="Docker Enable/Start"></a>Docker Enable/Start</h5><h5 id="Docker-macvlan-Network-Setting"><a href="#Docker-macvlan-Network-Setting" class="headerlink" title="Docker macvlan Network Setting"></a>Docker macvlan Network Setting</h5><h4 id="VM-Export"><a href="#VM-Export" class="headerlink" title="VM Export"></a>VM Export</h4><h3 id="Get-a-PubSub-Software-Event-Broker-Image"><a href="#Get-a-PubSub-Software-Event-Broker-Image" class="headerlink" title="Get a PubSub+ Software Event Broker Image"></a>Get a PubSub+ Software Event Broker Image</h3><h4 id="start-Docker"><a href="#start-Docker" class="headerlink" title="start Docker"></a>start Docker</h4><h4 id="Pull-the-image"><a href="#Pull-the-image" class="headerlink" title="Pull the image"></a>Pull the image</h4><h3 id="Create-the-PubSub-Software-Event-Broker-Container"><a href="#Create-the-PubSub-Software-Event-Broker-Container" class="headerlink" title="Create the PubSub+ Software Event Broker Container"></a>Create the PubSub+ Software Event Broker Container</h3><h3 id="Manage-the-PubSub-Software-Event-Broker"><a href="#Manage-the-PubSub-Software-Event-Broker" class="headerlink" title="Manage the PubSub+ Software Event Broker"></a>Manage the PubSub+ Software Event Broker</h3><h4 id="Solace-PubSub-Manager-management-access"><a href="#Solace-PubSub-Manager-management-access" class="headerlink" title="Solace PubSub+ Manager management access"></a>Solace PubSub+ Manager management access</h4><h4 id="Solace-CLI-management-access"><a href="#Solace-CLI-management-access" class="headerlink" title="Solace CLI management access"></a>Solace CLI management access</h4><h2 id="使用-Solace-CLI-進行基本設定操作"><a href="#使用-Solace-CLI-進行基本設定操作" class="headerlink" title="使用 Solace CLI 進行基本設定操作"></a>使用 Solace CLI 進行基本設定操作</h2><h3 id="System-User-Create"><a href="#System-User-Create" class="headerlink" title="System User Create"></a>System User Create</h3><h4 id="cli-user"><a href="#cli-user" class="headerlink" title="cli user"></a>cli user</h4><h4 id="file-transfer"><a href="#file-transfer" class="headerlink" title="file-transfer"></a>file-transfer</h4><h3 id="Client-User-Create"><a href="#Client-User-Create" class="headerlink" title="Client User Create"></a>Client User Create</h3><h3 id="Connect-Max-Setting"><a href="#Connect-Max-Setting" class="headerlink" title="Connect Max. Setting"></a>Connect Max. Setting</h3><h2 id="使用-Try-Me-進行Publish-Subscribe功能測試"><a href="#使用-Try-Me-進行Publish-Subscribe功能測試" class="headerlink" title="使用 Try Me! 進行Publish/Subscribe功能測試"></a>使用 Try Me! 進行Publish/Subscribe功能測試</h2>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Solace-for-Docker-環境安裝&quot;&gt;&lt;a href=&quot;#Solace-for-Docker-環境安裝&quot; class=&quot;headerlink&quot; title=&quot;Solace for Docker 環境安裝&quot;&gt;&lt;/a&gt;Solace for Docker 環境
    
    </summary>
    
      <category term="Solace" scheme="http://yoursite.com/categories/Solace/"/>
    
      <category term="Solace Install" scheme="http://yoursite.com/categories/Solace/Solace-Install/"/>
    
    
  </entry>
  
  <entry>
    <title>Solace Install for VitualBox</title>
    <link href="http://yoursite.com/2020/07/22/%5BSolace%5DSolaceforVirtualbox/"/>
    <id>http://yoursite.com/2020/07/22/[Solace]SolaceforVirtualbox/</id>
    <published>2020-07-21T16:00:00.000Z</published>
    <updated>2020-07-22T07:10:28.737Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Get-a-Software-Event-Broker"><a href="#Get-a-Software-Event-Broker" class="headerlink" title="Get a Software Event Broker"></a>Get a Software Event Broker</h2><h3 id="Solace-PubSub-Standard-or-Solace-PubSub-Enterprise-Evaluation-Edition"><a href="#Solace-PubSub-Standard-or-Solace-PubSub-Enterprise-Evaluation-Edition" class="headerlink" title="Solace PubSub+ Standard or Solace PubSub+ Enterprise Evaluation Edition"></a>Solace PubSub+ Standard or Solace PubSub+ Enterprise Evaluation Edition</h3><h3 id="Solace-PubSub-Enterprise"><a href="#Solace-PubSub-Enterprise" class="headerlink" title="Solace PubSub+ Enterprise"></a>Solace PubSub+ Enterprise</h3><h2 id="Import-the-Software-Event-Broker"><a href="#Import-the-Software-Event-Broker" class="headerlink" title="Import the Software Event Broker"></a>Import the Software Event Broker</h2><h3 id="In-Oracle-VM-VirtualBox-Manager-select-File-gt-Import-Appliance"><a href="#In-Oracle-VM-VirtualBox-Manager-select-File-gt-Import-Appliance" class="headerlink" title="In Oracle VM VirtualBox Manager, select File &gt; Import Appliance."></a>In Oracle VM VirtualBox Manager, select File &gt; Import Appliance.</h3><h3 id="select-the-OVA-file-downloaded-in-the-previous-step"><a href="#select-the-OVA-file-downloaded-in-the-previous-step" class="headerlink" title="select the OVA file downloaded in the previous step"></a>select the OVA file downloaded in the previous step</h3><h3 id="default-appliance-settings-are-sufficient-to-begin-the-import"><a href="#default-appliance-settings-are-sufficient-to-begin-the-import" class="headerlink" title="default appliance settings are sufficient to begin the import"></a>default appliance settings are sufficient to begin the import</h3><h3 id="Oracle-VM-VirtualBox-Manager-will-import-the-event-broker-OVA"><a href="#Oracle-VM-VirtualBox-Manager-will-import-the-event-broker-OVA" class="headerlink" title="Oracle VM VirtualBox Manager will import the event broker OVA."></a>Oracle VM VirtualBox Manager will import the event broker OVA.</h3><h3 id="VM-VirtualBox-Manager-Settings"><a href="#VM-VirtualBox-Manager-Settings" class="headerlink" title="VM VirtualBox Manager,Settings"></a>VM VirtualBox Manager,Settings</h3><h3 id="start-the-VM"><a href="#start-the-VM" class="headerlink" title="start the VM"></a>start the VM</h3><h3 id="Login-to-the-event-broker-must-“sysadmin”"><a href="#Login-to-the-event-broker-must-“sysadmin”" class="headerlink" title="Login to the event broker,must “sysadmin”"></a>Login to the event broker,must “sysadmin”</h3><h2 id="Configure-the-Host-Name"><a href="#Configure-the-Host-Name" class="headerlink" title="Configure the Host Name"></a>Configure the Host Name</h2><h3 id="Shut-down-Solace-PubSub"><a href="#Shut-down-Solace-PubSub" class="headerlink" title="Shut down Solace PubSub+"></a>Shut down Solace PubSub+</h3><h3 id="Configure-the-hostname"><a href="#Configure-the-hostname" class="headerlink" title="Configure the hostname"></a>Configure the hostname</h3><h3 id="Restart-the-Solace-PubSub"><a href="#Restart-the-Solace-PubSub" class="headerlink" title="Restart the Solace PubSub+"></a>Restart the Solace PubSub+</h3><h3 id="Verify-that-the-hostname-has-been-set-to-the-new-one"><a href="#Verify-that-the-hostname-has-been-set-to-the-new-one" class="headerlink" title="Verify that the hostname has been set to the new one"></a>Verify that the hostname has been set to the new one</h3><h2 id="Access-the-Solace-CLI"><a href="#Access-the-Solace-CLI" class="headerlink" title="Access the Solace CLI"></a>Access the Solace CLI</h2><h3 id="To-enter-the-Solace-CLI-from-the-console-in-the-Linux-host-environment"><a href="#To-enter-the-Solace-CLI-from-the-console-in-the-Linux-host-environment" class="headerlink" title="To enter the Solace CLI from the console in the Linux host environment"></a>To enter the Solace CLI from the console in the Linux host environment</h3><h3 id="create-an-admin-user-named"><a href="#create-an-admin-user-named" class="headerlink" title="create an admin user named"></a>create an admin user named</h3><h3 id="To-determine-the-IP-address-assigned-to-the-event-broker"><a href="#To-determine-the-IP-address-assigned-to-the-event-broker" class="headerlink" title="To determine the IP address assigned to the event broker"></a>To determine the IP address assigned to the event broker</h3><h3 id="To-remotely-access-the-Solace-CLI-for-the-event-broker-you-can-now-ssh-to-port-2222"><a href="#To-remotely-access-the-Solace-CLI-for-the-event-broker-you-can-now-ssh-to-port-2222" class="headerlink" title="To remotely access the Solace CLI for the event broker, you can now ssh to port 2222"></a>To remotely access the Solace CLI for the event broker, you can now ssh to port 2222</h3><h2 id="Review-Configuration-Defaults"><a href="#Review-Configuration-Defaults" class="headerlink" title="Review Configuration Defaults"></a>Review Configuration Defaults</h2>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Get-a-Software-Event-Broker&quot;&gt;&lt;a href=&quot;#Get-a-Software-Event-Broker&quot; class=&quot;headerlink&quot; title=&quot;Get a Software Event Broker&quot;&gt;&lt;/a&gt;Get a
    
    </summary>
    
      <category term="Solace" scheme="http://yoursite.com/categories/Solace/"/>
    
      <category term="Solace Install" scheme="http://yoursite.com/categories/Solace/Solace-Install/"/>
    
    
  </entry>
  
  <entry>
    <title>Hadoop3.0.0 Document List</title>
    <link href="http://yoursite.com/2018/01/09/%5Bhadoop%5DHadoop3.0.0_doc/"/>
    <id>http://yoursite.com/2018/01/09/[hadoop]Hadoop3.0.0_doc/</id>
    <published>2018-01-08T16:00:00.000Z</published>
    <updated>2020-07-22T08:28:51.203Z</updated>
    
    <content type="html"><![CDATA[<h5 id="General"><a href="#General" class="headerlink" title="General"></a>General</h5><hr>
<p>Overview<br>Hadoop: Setting up a Single Node Cluster.<br>Hadoop Cluster Setup<br>Hadoop Commands Guide<br>FileSystem Shell<br>Apache Hadoop Compatibility<br>Apache Hadoop Downstream Developer’s Guide<br>Hadoop Interface Taxonomy: Audience and Stability Classification<br>The Hadoop FileSystem API Definition</p>
<h5 id="Common"><a href="#Common" class="headerlink" title="Common"></a>Common</h5><hr>
<p>Hadoop: CLI MiniCluster<br>Native Libraries Guide<br>Proxy user - Superusers Acting On Behalf Of Other Users<br>Rack Awareness<br>Hadoop in Secure Mode<br>Service Level Authorization Guide<br>Authentication for Hadoop HTTP web-consoles<br>CredentialProvider API Guide<br>Hadoop Key Management Server (KMS) - Documentation Sets<br>Enabling Dapper-like Tracing in Hadoop<br>Unix Shell Guide</p>
<h5 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h5><hr>
<p>HDFS Architecture<br>HDFS Users Guide<br>HDFS Commands Guide<br>HDFS High Availability Using the Quorum Journal Manager<br>HDFS High Availability<br>HDFS Federation<br>ViewFs Guide<br>HDFS Snapshots<br>Offline Edits Viewer Guide<br>Offline Image Viewer Guide<br>HDFS Permissions Guide<br>HDFS Quotas Guide<br>C API libhdfs<br>WebHDFS REST API<br>Hadoop HDFS over HTTP - Documentation Sets<br>HDFS Short-Circuit Local Reads<br>Centralized Cache Management in HDFS<br>HDFS NFS Gateway<br>HDFS Rolling Upgrade<br>Extended Attributes in HDFS<br>Transparent Encryption in HDFS<br>HDFS Support for Multihomed Networks<br>Archival Storage, SSD &amp; Memory<br>Memory Storage Support in HDFS<br>Synthetic Load Generator Guide<br>HDFS Erasure Coding<br>HDFS Disk Balancer<br>HDFS Upgrade Domain<br>HDFS DataNode Admin Guide<br>Unix Shell Guide<br>HDFS Router-based Federation</p>
<h5 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h5><hr>
<p>Tutorial<br>MapReduce Commands Guide<br>Apache Hadoop MapReduce - Migrating from Apache Hadoop 1.x to Apache Hadoop 2.x<br>Hadoop: Encrypted Shuffle<br>Hadoop: Pluggable Shuffle and Pluggable Sort<br>Hadoop: Distributed Cache Deploy<br>MR Support for YARN Shared Cache</p>
<h5 id="MapReduce-REST-APIs"><a href="#MapReduce-REST-APIs" class="headerlink" title="MapReduce REST APIs"></a>MapReduce REST APIs</h5><hr>
<p>MapReduce Application Master REST API’s.<br>MapReduce History Server REST API’s.</p>
<h5 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h5><hr>
<p>Apache Hadoop YARN<br>YARN Commands<br>Hadoop: Capacity Scheduler<br>Hadoop: Fair Scheduler<br>ResourceManager Restart<br>ResourceManager High Availability<br>Hadoop: YARN Resource Configuration<br>YARN Node Labels<br>Web Application Proxy<br>The YARN Timeline Server<br>The YARN Timeline Service v.2<br>Hadoop: Writing YARN Applications<br>YARN Application Security<br>NodeManager<br>Launching Applications Using Docker Containers<br>Using CGroups with YARN<br>YARN Secure Containers<br>YARN Service Registry<br>Reservation System<br>Graceful Decommission of YARN Nodes<br>Opportunistic Containers<br>Hadoop: YARN Federation<br>YARN Shared Cache</p>
<h5 id="YARN-REST-APIs"><a href="#YARN-REST-APIs" class="headerlink" title="YARN REST APIs"></a>YARN REST APIs</h5><hr>
<p>Hadoop YARN - Introduction to the web services REST API’s<br>ResourceManager REST API’s.<br>NodeManager REST API’s<br>The YARN Timeline Server<br>The YARN Timeline Service v.2</p>
<h5 id="Hadoop-Compatible-File-Systems"><a href="#Hadoop-Compatible-File-Systems" class="headerlink" title="Hadoop Compatible File Systems"></a>Hadoop Compatible File Systems</h5><hr>
<p>Hadoop-Aliyun module: Integration with Aliyun Web Services<br>Hadoop-AWS module: Integration with Amazon Web Services<br>Hadoop Azure Support: Azure Blob Storage<br>Hadoop Azure Data Lake Support<br>Hadoop OpenStack Support: Swift Object Store</p>
<h5 id="Auth"><a href="#Auth" class="headerlink" title="Auth"></a>Auth</h5><hr>
<p>Hadoop Auth, Java HTTP SPNEGO<br>Hadoop Auth, Java HTTP SPNEGO - Examples<br>Hadoop Auth, Java HTTP SPNEGO - Server Side Configuration<br>Hadoop Auth, Java HTTP SPNEGO - Building It</p>
<h5 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h5><hr>
<p>Hadoop Streaming<br>Hadoop Archives Guide<br>Hadoop Archive Logs Guide<br>DistCp Guide<br>Gridmix<br>Rumen<br>Resource Estimator Service<br>YARN Scheduler Load Simulator (SLS)<br>Hadoop Benchmarking</p>
<h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><hr>
<p>Changelog and Release Notes<br>Java API docs<br>Unix Shell API<br>Metrics</p>
<h5 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h5><hr>
<p>core-default.xml<br>hdfs-default.xml<br>mapred-default.xml<br>yarn-default.xml<br>Deprecated Properties</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;General&quot;&gt;&lt;a href=&quot;#General&quot; class=&quot;headerlink&quot; title=&quot;General&quot;&gt;&lt;/a&gt;General&lt;/h5&gt;&lt;hr&gt;
&lt;p&gt;Overview&lt;br&gt;Hadoop: Setting up a Single Node 
    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/BigData/Hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>PySpark-RDD Memo_1</title>
    <link href="http://yoursite.com/2018/01/09/%5Bspark%5Dspark2.2.1_RDD_Memo_1/"/>
    <id>http://yoursite.com/2018/01/09/[spark]spark2.2.1_RDD_Memo_1/</id>
    <published>2018-01-08T16:00:00.000Z</published>
    <updated>2020-07-22T08:36:44.362Z</updated>
    
    <content type="html"><![CDATA[<h5 id="start-pyspark"><a href="#start-pyspark" class="headerlink" title="start pyspark"></a>start pyspark</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">hadoop@hadoop-master:~$ pyspark</div><div class="line"></div><div class="line">###看到以下spark歡迎畫面表示pyspark啟動成功,pyspark預設會把SparkSession的Instance建構好</div><div class="line">Python 2.7.13 (default, Jan 19 2017, 14:48:08)</div><div class="line">[GCC 6.3.0 20170118] on linux2</div><div class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</div><div class="line">Setting default log level to &quot;WARN&quot;.</div><div class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</div><div class="line">2018-01-09 15:32:11,663 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">2018-01-09 15:32:21,550 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</div><div class="line">Welcome to</div><div class="line">      ____              __</div><div class="line">     / __/__  ___ _____/ /__</div><div class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</div><div class="line">   /__ / .__/\_,_/_/ /_/\_\   version 2.2.1</div><div class="line">      /_/</div><div class="line"></div><div class="line">Using Python version 2.7.13 (default, Jan 19 2017 14:48:08)</div><div class="line">SparkSession available as &apos;spark&apos;.</div><div class="line">&gt;&gt;&gt;</div><div class="line"></div><div class="line">##開始可以操作pyspark相關語法</div><div class="line">## sc    是 SparkContext物件</div><div class="line">## spark 是 SparkSession物件</div><div class="line">&gt;&gt;&gt; sc</div><div class="line">&lt;SparkContext master=local[*] appName=PySparkShell&gt;</div><div class="line"></div><div class="line">&gt;&gt;&gt; spark</div><div class="line">&lt;pyspark.sql.session.SparkSession object at 0x7fa3a1ab4610&gt;</div><div class="line"></div><div class="line">&gt;&gt;&gt; from pyspark.conf import SparkConf</div><div class="line">&gt;&gt;&gt; conf = SparkConf()</div><div class="line">&gt;&gt;&gt; print (&quot;環境變數\n&quot;)</div><div class="line">環境變數</div><div class="line"></div><div class="line">&gt;&gt;&gt; print (conf.toDebugString())</div><div class="line">spark.app.name=PySparkShell</div><div class="line">spark.master=local[*]</div><div class="line">spark.submit.deployMode=client</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="pyspark-RDD-Operate"><a href="#pyspark-RDD-Operate" class="headerlink" title="pyspark: RDD Operate"></a>pyspark: RDD Operate</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line">## Apache Spark RDD - aggregate函數</div><div class="line">&gt;&gt;&gt; seqOp = (lambda x, y: (x[0] + y, x[1] + 1))</div><div class="line">&gt;&gt;&gt; combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))</div><div class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).aggregate((0, 0), seqOp, combOp)</div><div class="line">(15, 5)</div><div class="line">&gt;&gt;&gt; sc.parallelize([]).aggregate((0, 0), seqOp, combOp)</div><div class="line">(0, 0)</div><div class="line"></div><div class="line">## Apache Spark RDD - cartesian</div><div class="line">&gt;&gt;&gt; rdd = sc.parallelize([1, 2])</div><div class="line">&gt;&gt;&gt; sorted(rdd.cartesian(rdd).collect())</div><div class="line">[(1, 1), (1, 2), (2, 1), (2, 2)]</div><div class="line"></div><div class="line">## Apache Spark RDD - glom,coalesce</div><div class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()</div><div class="line">[[1], [2, 3], [4, 5]]</div><div class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()</div><div class="line">[[1, 2, 3, 4, 5]]</div><div class="line"></div><div class="line">## Apache Spark RDD - collectAsMap</div><div class="line">&gt;&gt;&gt; m = sc.parallelize([(&quot;ted&quot;, 2), (&quot;kevin&quot;, 4)]).collectAsMap()</div><div class="line">&gt;&gt;&gt; m[&quot;ted&quot;]</div><div class="line">2</div><div class="line">&gt;&gt;&gt; m[&quot;kevin&quot;]</div><div class="line">4</div><div class="line"></div><div class="line">## Apache Spark RDD - combineByKey</div><div class="line">&gt;&gt;&gt; x = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 2)])</div><div class="line">&gt;&gt;&gt; def to_list(a):</div><div class="line">...     return [a]</div><div class="line">...</div><div class="line">&gt;&gt;&gt; def append(a,b):</div><div class="line">...     a.append(b)</div><div class="line">...     return a</div><div class="line">...</div><div class="line">&gt;&gt;&gt; def extend(a,b):</div><div class="line">...     a.extend(b)</div><div class="line">...     return a</div><div class="line">...</div><div class="line">&gt;&gt;&gt; sorted(x.combineByKey(to_list, append, extend).collect())</div><div class="line">[(&apos;a&apos;, [1, 2]), (&apos;b&apos;, [1])]</div><div class="line"></div><div class="line">## Apache Spark RDD - count</div><div class="line">&gt;&gt;&gt; sc.parallelize([2, 3, 4]).count()</div><div class="line">3</div><div class="line"></div><div class="line">## Apache Spark RDD - countApproxDistinct</div><div class="line">&gt;&gt;&gt; n = sc.parallelize(range(1000)).map(str).countApproxDistinct()</div><div class="line">&gt;&gt;&gt; 900 &lt; n &lt; 1100</div><div class="line">True</div><div class="line">&gt;&gt;&gt; n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()</div><div class="line">&gt;&gt;&gt; 16 &lt; n &lt; 24</div><div class="line">True</div><div class="line"></div><div class="line">## Apache Spark RDD - countApprox</div><div class="line">&gt;&gt;&gt; rdd = sc.parallelize(range(1000), 10)</div><div class="line">&gt;&gt;&gt; rdd.countApprox(1000, 1.0)</div><div class="line">[Stage 20:===================================================&gt;     (9 + 1) / 10]1000</div><div class="line"></div><div class="line">## Apache Spark RDD - countByKey</div><div class="line">&gt;&gt;&gt; rdd = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1)])</div><div class="line">&gt;&gt;&gt; sorted(rdd.countByKey().items())</div><div class="line">[(&apos;a&apos;, 2), (&apos;b&apos;, 1)]</div><div class="line"></div><div class="line">## Apache Spark RDD - countByValue</div><div class="line">&gt;&gt;&gt; sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())</div><div class="line">[(1, 2), (2, 3)]</div><div class="line"></div><div class="line">## Apache Spark RDD - distinct</div><div class="line">&gt;&gt;&gt; sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())</div><div class="line">[1, 2, 3]</div><div class="line"></div><div class="line">## Apache Spark RDD - filter</div><div class="line">&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4, 5])</div><div class="line">&gt;&gt;&gt; rdd.filter(lambda x: x % 2 == 0).collect()</div><div class="line">[2,4]</div></pre></td></tr></table></figure>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;start-pyspark&quot;&gt;&lt;a href=&quot;#start-pyspark&quot; class=&quot;headerlink&quot; title=&quot;start pyspark&quot;&gt;&lt;/a&gt;start pyspark&lt;/h5&gt;&lt;blockquote&gt;
&lt;figure class=&quot;h
    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Spark" scheme="http://yoursite.com/categories/BigData/Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Hadoop 3.0.0 Overview</title>
    <link href="http://yoursite.com/2018/01/09/%5Bhadoop%5DHadoop3.0.0_overview/"/>
    <id>http://yoursite.com/2018/01/09/[hadoop]Hadoop3.0.0_overview/</id>
    <published>2018-01-08T16:00:00.000Z</published>
    <updated>2020-07-22T08:30:09.831Z</updated>
    
    <content type="html"><![CDATA[<ol>
<li>Minimum required Java version increased from Java 7 to Java 8</li>
<li>Support for erasure coding in HDFS</li>
<li>YARN Timeline Service v.2</li>
<li>Shell script rewrite</li>
<li>Shaded client jars</li>
<li>Support for Opportunistic Containers and Distributed Scheduling.</li>
<li>MapReduce task-level native optimization</li>
<li>Support for more than 2 NameNodes</li>
<li>Default ports of multiple services have been changed.</li>
<li>Support for Microsoft Azure Data Lake and Aliyun Object Storage System filesystem connectors</li>
<li>Intra-datanode balancer</li>
<li>Reworked daemon and task heap management</li>
<li>S3Guard: Consistency and Metadata Caching for the S3A filesystem client</li>
<li>HDFS Router-Based Federation</li>
<li>API-based configuration of Capacity Scheduler queue configuration</li>
<li>YARN Resource Types</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;ol&gt;
&lt;li&gt;Minimum required Java version increased from Java 7 to Java 8&lt;/li&gt;
&lt;li&gt;Support for erasure coding in HDFS&lt;/li&gt;
&lt;li&gt;YARN Timeline Se
    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/BigData/Hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>Hadoop3.0.0 Quota</title>
    <link href="http://yoursite.com/2018/01/09/%5Bhadoop%5Dhadoop3.0.0_quota_cmd/"/>
    <id>http://yoursite.com/2018/01/09/[hadoop]hadoop3.0.0_quota_cmd/</id>
    <published>2018-01-08T16:00:00.000Z</published>
    <updated>2020-07-22T08:30:28.759Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Quota相關指令"><a href="#Quota相關指令" class="headerlink" title="Quota相關指令:"></a>Quota相關指令:</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">Administrative Commands:</div><div class="line">	hdfs dfsadmin</div><div class="line">		[-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">		[-clrQuota &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">		[-setSpaceQuota &lt;quota&gt; [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">		[-clrSpaceQuota [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">	</div><div class="line">Reporting Commands:</div><div class="line">	hadoop fs -count -q -v &lt;dirname&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="Name-Quotas"><a href="#Name-Quotas" class="headerlink" title="Name Quotas"></a>Name Quotas</h5><blockquote>
<p>透過限制目錄或文件數量,當達到限制上限時,系統會發出錯誤訊息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir -p /user/hadoop/example/data2</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -setQuota 3 /user/hadoop/example/data2</div><div class="line"></div><div class="line">## QUOTA     --&gt;Name Quota(-setQuota設定數量)</div><div class="line">## REM_QUOTA(剩餘Quota) --&gt;REM_QUOTA = Quota-(DIR_COUNT+FILE_COUNT)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -q -v /user/hadoop/example/data2</div><div class="line">       QUOTA       REM_QUOTA     SPACE_QUOTA REM_SPACE_QUOTA    DIR_COUNT   FILE_COUNT       CONTENT_SIZE PATHNAME</div><div class="line">           3               2            none             inf            1            0                  0 /user/hadoop/example/data2</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -put - /user/hadoop/example/data2/data1.txt</div><div class="line">Test1!!!!</div><div class="line">hadoop@hadoop-master:~$  hadoop fs -count -q -v /user/hadoop/example/data2</div><div class="line">       QUOTA       REM_QUOTA     SPACE_QUOTA REM_SPACE_QUOTA    DIR_COUNT   FILE_COUNT       CONTENT_SIZE PATHNAME</div><div class="line">           3               1            none             inf            1            1                 10 /user/hadoop/example/data2</div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -put - /user/hadoop/example/data2/data2.txt</div><div class="line">Test2!!!!</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -q -v /user/hadoop/example/data2</div><div class="line">       QUOTA       REM_QUOTA     SPACE_QUOTA REM_SPACE_QUOTA    DIR_COUNT   FILE_COUNT       CONTENT_SIZE PATHNAME</div><div class="line">           3               0            none             inf            1            2                 20 /user/hadoop/example/data2</div><div class="line">		   </div><div class="line">##無法再加入檔案,因為REM_QUOTA已經=0了		   </div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -put - /user/hadoop/example/data2/data3.txt</div><div class="line">put: The NameSpace quota (directories and files) of directory /user/hadoop/example/data2 is exceeded: quota=3 file count=4</div><div class="line"></div><div class="line">##清除Quota限制</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -clrQuota /user/hadoop/example/data2</div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -put - /user/hadoop/example/data2/data3.txt</div><div class="line">TEST3!!!!!!</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -q -v /user/hadoop/example/data2</div><div class="line">       QUOTA       REM_QUOTA     SPACE_QUOTA REM_SPACE_QUOTA    DIR_COUNT   FILE_COUNT       CONTENT_SIZE PATHNAME</div><div class="line">        none             inf            none             inf            1            3                 32 /user/hadoop/example/data2</div></pre></td></tr></table></figure></p>
</blockquote>
<h5 id="Space-Quotas"><a href="#Space-Quotas" class="headerlink" title="Space Quotas"></a>Space Quotas</h5><blockquote>
<p>透過限制目錄或檔案size大小,當達到限制上限時,系統會發出錯誤訊息(檔案大小和block size有關係)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir -p /user/hadoop/example/data3</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -setSpaceQuota 1M /user/hadoop/example/data3</div><div class="line"></div><div class="line">##Space Quota要考慮到Block size的問題,所以基本上會以block size的倍數來做為空間配額大小(block size*repl number)</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -setSpaceQuota 64M /user/hadoop/example/data3</div><div class="line">hadoop@hadoop-master:~$  hadoop fs -put - /user/hadoop/example/data3/data1.txt</div><div class="line">Test11111!!!</div><div class="line">hadoop@hadoop-master:~$  hadoop fs -count -q -v /user/hadoop/example/data3</div><div class="line">       QUOTA       REM_QUOTA     SPACE_QUOTA REM_SPACE_QUOTA    DIR_COUNT   FILE_COUNT       CONTENT_SIZE PATHNAME</div><div class="line">        none             inf        67108864        67108851            1            1                 13 /user/hadoop/example/data3</div><div class="line">hadoop@hadoop-master:~$  hadoop fs -put - /user/hadoop/example/data3/data2.txt</div><div class="line">TEST2!!!!</div><div class="line">put: The DiskSpace quota of /user/hadoop/example/data3 is exceeded: quota = 67108864 B = 64 MB but diskspace consumed = 67108877 B = 64.00 MB</div><div class="line"></div><div class="line">## 清除SpaceQuota的限制</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -clrSpaceQuota /user/hadoop/example/data3</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R /user/hadoop/example/data3</div><div class="line">-rw-r--r--   1 hadoop supergroup         13 2018-01-09 12:06 /user/hadoop/example/data3/data1.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -q -v /user/hadoop/example/data3</div><div class="line">       QUOTA       REM_QUOTA     SPACE_QUOTA REM_SPACE_QUOTA    DIR_COUNT   FILE_COUNT       CONTENT_SIZE PATHNAME</div><div class="line">        none             inf            none             inf            1            1                 13 /user/hadoop/example/data3</div></pre></td></tr></table></figure></p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;Quota相關指令&quot;&gt;&lt;a href=&quot;#Quota相關指令&quot; class=&quot;headerlink&quot; title=&quot;Quota相關指令:&quot;&gt;&lt;/a&gt;Quota相關指令:&lt;/h5&gt;&lt;blockquote&gt;
&lt;figure class=&quot;highlight plain
    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/BigData/Hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hadoop3.0.0 daemon command</title>
    <link href="http://yoursite.com/2018/01/08/%5Bhadoop%5Dhadoop%203.0.0_daemon_cmd/"/>
    <id>http://yoursite.com/2018/01/08/[hadoop]hadoop 3.0.0_daemon_cmd/</id>
    <published>2018-01-07T16:00:00.000Z</published>
    <updated>2020-07-22T08:25:48.726Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">hdfs --daemon start balancer             run a cluster balancing utility</div><div class="line">hdfs --daemon start datanode             run a DFS datanode</div><div class="line">hdfs --daemon start dfsrouter            run the DFS router</div><div class="line">hdfs --daemon start diskbalancer         Distributes data evenly among disks on a given node</div><div class="line">hdfs --daemon start journalnode          run the DFS journalnode</div><div class="line">hdfs --daemon start mover                run a utility to move block replicas across storage types</div><div class="line">hdfs --daemon start namenode             run the DFS namenode</div><div class="line">hdfs --daemon start nfs3                 run an NFS version 3 gateway</div><div class="line">hdfs --daemon start portmap              run a portmap service</div><div class="line">hdfs --daemon start secondarynamenode    run the DFS secondary namenode</div><div class="line">hdfs --daemon start zkfc                 run the ZK Failover Controller daemon</div><div class="line"></div><div class="line">yarn --daemon start nodemanager          run a nodemanager on each worker </div><div class="line">yarn --daemon start proxyserver          run the web app proxy server</div><div class="line">yarn --daemon start resourcemanager      run the ResourceManager</div><div class="line">yarn --daemon start router               run the Router daemon</div><div class="line">yarn --daemon start sharedcachemanager   run the SharedCacheManager daemon</div><div class="line">yarn --daemon start timelineserver       run the timeline server</div><div class="line"></div><div class="line">mapred --daemon start historyserver      run job history servers as a standalone daemon</div></pre></td></tr></table></figure></blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class
    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/BigData/Hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>Hadoop3.0.0 NFS Gateway</title>
    <link href="http://yoursite.com/2018/01/08/%5Bhadoop%5Dhadoop3.0.0_nfs_gateway/"/>
    <id>http://yoursite.com/2018/01/08/[hadoop]hadoop3.0.0_nfs_gateway/</id>
    <published>2018-01-07T16:00:00.000Z</published>
    <updated>2020-07-22T08:29:54.781Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Hadoop3-0-0-NFS-Gateway-setting-amp-start"><a href="#Hadoop3-0-0-NFS-Gateway-setting-amp-start" class="headerlink" title="Hadoop3.0.0 NFS Gateway setting &amp; start"></a>Hadoop3.0.0 NFS Gateway setting &amp; start</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ service nfs-kernel-server stop</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ start-dfs.sh</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ start-yarn.sh</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs --daemon start nfs3</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ rpcinfo -p</div><div class="line">   program vers proto   port  service</div><div class="line">    100000    4   tcp    111  portmapper</div><div class="line">    100000    3   tcp    111  portmapper</div><div class="line">    100000    2   tcp    111  portmapper</div><div class="line">    100000    4   udp    111  portmapper</div><div class="line">    100000    3   udp    111  portmapper</div><div class="line">    100000    2   udp    111  portmapper</div><div class="line">    100005    1   udp  46340  mountd</div><div class="line">    100005    1   tcp  33399  mountd</div><div class="line">    100005    2   udp  56322  mountd</div><div class="line">    100005    2   tcp  43065  mountd</div><div class="line">    100005    3   udp  52402  mountd</div><div class="line">    100005    3   tcp  49301  mountd</div><div class="line">    100003    2   tcp   2049  nfs</div><div class="line">    100003    3   tcp   2049  nfs</div><div class="line">    100003    4   tcp   2049  nfs</div><div class="line">    100227    2   tcp   2049</div><div class="line">    100227    3   tcp   2049</div><div class="line">    100003    2   udp   2049  nfs</div><div class="line">    100003    3   udp   2049  nfs</div><div class="line">    100003    4   udp   2049  nfs</div><div class="line">    100227    2   udp   2049</div><div class="line">    100227    3   udp   2049</div><div class="line">    100021    1   udp  50043  nlockmgr</div><div class="line">    100021    3   udp  50043  nlockmgr</div><div class="line">    100021    4   udp  50043  nlockmgr</div><div class="line">    100021    1   tcp  33247  nlockmgr</div><div class="line">    100021    3   tcp  33247  nlockmgr</div><div class="line">    100021    4   tcp  33247  nlockmgr</div><div class="line">	</div><div class="line">hadoop@hadoop-master:~$ showmount -e</div><div class="line">Export list for hadoop-master:</div><div class="line">/ *</div><div class="line">	</div><div class="line">hadoop@hadoop-master:~$ netstat -tnl</div><div class="line">Active Internet connections (only servers)</div><div class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State</div><div class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8088       0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8030       0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:50079           0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8031       0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8032       0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:2049            0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8033       0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:50090      0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:5355            0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:9870            0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:4242            0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8020       0.0.0.0:*               LISTEN</div><div class="line">tcp6       0      0 :::22                   :::*                    LISTEN</div><div class="line">tcp6       0      0 :::5355                 :::*                    LISTEN</div><div class="line">tcp6       0      0 :::111                  :::*                    LISTEN</div><div class="line"></div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ jps</div><div class="line">4116 Nfs3</div><div class="line">2392 ResourceManager</div><div class="line">2108 SecondaryNameNode</div><div class="line">4157 Jps</div><div class="line">1837 NameNode</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ mkdir -p /opt/hdfs</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ sudo mount -t nfs -o vers=3,proto=tcp,nolock,sync 192.168.51.4:/  /opt/hdfs</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ ls -la /opt/hdfs/user</div><div class="line">total 4</div><div class="line">drwxr-xr-x  6 hadoop 2584148964  192 Jan  4 16:35 .</div><div class="line">drwxr-xr-x  4 hadoop 2584148964  128 Dec 26 15:25 ..</div><div class="line">drwxr-xr-x 41 hadoop 2584148964 1312 Jan  8 17:00 hadoop</div><div class="line">drwxr-xr-x  3 hadoop 2584148964   96 Dec 26 18:44 hive</div><div class="line">drwxr-xr-x  2 hadoop 2584148964   64 Jan  3 17:30 snapshot</div><div class="line">drwxr-xr-x  2 hadoop 2584148964   64 Jan  4 16:35 vagrant</div><div class="line"></div><div class="line">hadoop@hadoop-master:/home$ sudo umount /opt/hdfs</div><div class="line">hadoop@hadoop-master:/home$ ls -la /opt/hdfs</div><div class="line">total 8</div><div class="line">drwxrwxr-x 2 hadoop hadoop 4096 Jan  8 17:32 .</div><div class="line">drwxr-xr-x 4 hadoop hadoop 4096 Jan  8 17:32 ..</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="Hadoop3-0-0-NFS-Gateway-modify-mount-point"><a href="#Hadoop3-0-0-NFS-Gateway-modify-mount-point" class="headerlink" title="Hadoop3.0.0 NFS Gateway modify mount point"></a>Hadoop3.0.0 NFS Gateway modify mount point</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">vi /bgdt/hadoop-3.0.0/etc/hadoop/hdfs-site.xml</div><div class="line"></div><div class="line">##在設定檔中加入以下參數(nfs.export.point)</div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;nfs.export.point&lt;/name&gt;</div><div class="line">  &lt;value&gt;/user&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line"></div><div class="line">##重新啟動HDFS</div><div class="line">hadoop@hadoop-master:~$start-dfs.sh</div><div class="line"></div><div class="line">##重新啟動NFS3</div><div class="line">hadoop@hadoop-master:~$hdfs --daemon start nfs3</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$showmount -e</div><div class="line">Export list for hadoop-master:</div><div class="line">/user *</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ sudo mount -t nfs -o vers=3,proto=tcp,nolock,sync 192.168.51.4:/user  /opt/hdfs</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ ls -la /opt/hdfs</div><div class="line">total 8</div><div class="line">drwxr-xr-x  6 hadoop 2584148964  192 Jan  4 16:35 .</div><div class="line">drwxr-xr-x  4 hadoop hadoop     4096 Jan  8 17:32 ..</div><div class="line">drwxr-xr-x 41 hadoop 2584148964 1312 Jan  8 17:00 hadoop</div><div class="line">drwxr-xr-x  3 hadoop 2584148964   96 Dec 26 18:44 hive</div><div class="line">drwxr-xr-x  2 hadoop 2584148964   64 Jan  3 17:30 snapshot</div><div class="line">drwxr-xr-x  2 hadoop 2584148964   64 Jan  4 16:35 vagrant</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ sudo umount /opt/hdfs</div></pre></td></tr></table></figure>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;Hadoop3-0-0-NFS-Gateway-setting-amp-start&quot;&gt;&lt;a href=&quot;#Hadoop3-0-0-NFS-Gateway-setting-amp-start&quot; class=&quot;headerlink&quot; title=&quot;Hadoop3.0.
    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/BigData/Hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hadoop3.0.0 相關服務 Port Number 設定參數</title>
    <link href="http://yoursite.com/2018/01/08/%5Bhadoop%5Dhadoop3_port_number_list/"/>
    <id>http://yoursite.com/2018/01/08/[hadoop]hadoop3_port_number_list/</id>
    <published>2018-01-07T16:00:00.000Z</published>
    <updated>2020-07-22T08:30:51.651Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">dfs.balancer.address					0.0.0.0:0</div><div class="line">dfs.mover.address					0.0.0.0:0</div><div class="line"></div><div class="line">dfs.federation.router.http-address			0.0.0.0:50071</div><div class="line">dfs.federation.router.https-address			0.0.0.0:50072</div><div class="line">dfs.federation.router.admin-address			0.0.0.0:8111</div><div class="line">dfs.federation.router.rpc-address			0.0.0.0:8888</div><div class="line"></div><div class="line">dfs.namenode.backup.address				0.0.0.0:50100</div><div class="line">dfs.namenode.backup.http-address			0.0.0.0:50105</div><div class="line"></div><div class="line">dfs.journalnode.http-address				0.0.0.0:8480</div><div class="line">dfs.journalnode.https-address				0.0.0.0:8481</div><div class="line">dfs.journalnode.rpc-address				0.0.0.0:8485</div><div class="line"></div><div class="line">dfs.datanode.http.address				0.0.0.0:9864</div><div class="line">dfs.datanode.https.address				0.0.0.0:9865</div><div class="line">dfs.datanode.address					0.0.0.0:9866</div><div class="line">dfs.datanode.ipc.address				0.0.0.0:9867</div><div class="line"></div><div class="line">dfs.namenode.secondary.http-address			0.0.0.0:9868</div><div class="line">dfs.namenode.secondary.https-address			0.0.0.0:9869</div><div class="line">dfs.namenode.http-address				0.0.0.0:9870</div><div class="line">dfs.namenode.https-address				0.0.0.0:9871</div><div class="line"></div><div class="line">yarn.nodemanager.address				$&#123;yarn.nodemanager.hostname&#125;:0</div><div class="line">yarn.nodemanager.localizer.address			$&#123;yarn.nodemanager.hostname&#125;:8040</div><div class="line">yarn.nodemanager.webapp.address				$&#123;yarn.nodemanager.hostname&#125;:8042</div><div class="line">yarn.nodemanager.collector-service.address		$&#123;yarn.nodemanager.hostname&#125;:8048</div><div class="line"></div><div class="line">yarn.resourcemanager.scheduler.address			$&#123;yarn.resourcemanager.hostname&#125;:8030</div><div class="line">yarn.resourcemanager.resource-tracker.address		$&#123;yarn.resourcemanager.hostname&#125;:8031</div><div class="line">yarn.resourcemanager.address				$&#123;yarn.resourcemanager.hostname&#125;:8032</div><div class="line">yarn.resourcemanager.admin.address			$&#123;yarn.resourcemanager.hostname&#125;:8033</div><div class="line">yarn.resourcemanager.webapp.address			$&#123;yarn.resourcemanager.hostname&#125;:8088</div><div class="line">yarn.resourcemanager.webapp.https.address		$&#123;yarn.resourcemanager.hostname&#125;:8090</div><div class="line"></div><div class="line">yarn.timeline-service.address				$&#123;yarn.timeline-service.hostname&#125;:10200</div><div class="line">yarn.timeline-service.webapp.address			$&#123;yarn.timeline-service.hostname&#125;:8188</div><div class="line">yarn.timeline-service.webapp.https.address		$&#123;yarn.timeline-service.hostname&#125;:8190</div><div class="line"></div><div class="line">mapreduce.jobhistory.address				0.0.0.0:10020	</div><div class="line">mapreduce.jobhistory.webapp.address			0.0.0.0:19888	</div><div class="line">mapreduce.jobhistory.webapp.https.address		0.0.0.0:19890</div><div class="line">mapreduce.jobhistory.admin.address			0.0.0.0:10033</div></pre></td></tr></table></figure></blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class
    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/BigData/Hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hadoop archive command Memo</title>
    <link href="http://yoursite.com/2018/01/04/%5Bhadoop%5Dhadoop_archive_cmd/"/>
    <id>http://yoursite.com/2018/01/04/[hadoop]hadoop_archive_cmd/</id>
    <published>2018-01-03T16:00:00.000Z</published>
    <updated>2020-07-22T08:26:27.142Z</updated>
    
    <content type="html"><![CDATA[<h5 id="hadoop壓縮機制"><a href="#hadoop壓縮機制" class="headerlink" title="hadoop壓縮機制"></a>hadoop壓縮機制</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">usage: archive &lt;-archiveName &lt;NAME&gt;.har&gt; &lt;-p &lt;parent path&gt;&gt; [-r &lt;replication factor&gt;] &lt;src&gt;* &lt;dest&gt;</div><div class="line"> -archiveName &lt;arg&gt;   Name of the Archive. This is mandatory option</div><div class="line"> -help                Show the usage</div><div class="line"> -p &lt;arg&gt;             Parent path of sources. This is mandatory option</div><div class="line"> -r &lt;arg&gt;             Replication factor archive files</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hadoop-archive操作方式"><a href="#hadoop-archive操作方式" class="headerlink" title="hadoop archive操作方式"></a>hadoop archive操作方式</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/test1.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/test2.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/test3.txt</div><div class="line"></div><div class="line">##壓縮/user/hadoop/example目錄下所有檔案和目錄</div><div class="line">hadoop@hadoop-master:~$ hadoop archive -archiveName example1.har -p /user/hadoop/example -r 3 /user/hadoop</div><div class="line"></div><div class="line">##壓縮/user/hadoop/example目錄下所有txt檔</div><div class="line">hadoop@hadoop-master:~$ hadoop archive -archiveName example2.har -p /user/hadoop/example/ -r 3 *.txt /user/hadoop</div><div class="line"></div><div class="line">## 查詢壓縮檔的內容</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R har:///user/hadoop/example1.har</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-04 18:12 har:///user/hadoop/example1.har/lab_2</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-04 18:13 har:///user/hadoop/example1.har/lab_2/example1.har</div><div class="line">-rw-r--r--   3 hadoop supergroup        373 2018-01-03 15:41 har:///user/hadoop/example1.har/lab_2/test9.tar.gz</div><div class="line">-rw-r--r--   3 hadoop supergroup         23 2018-01-04 18:09 har:///user/hadoop/example1.har/test1.txt</div><div class="line">-rw-r--r--   3 hadoop supergroup         30 2018-01-04 18:10 har:///user/hadoop/example1.har/test2.txt</div><div class="line">-rw-r--r--   3 hadoop supergroup         15 2018-01-04 18:10 har:///user/hadoop/example1.har/test3.txt</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R har:///user/hadoop/example2.har</div><div class="line">-rw-r--r--   3 hadoop supergroup         23 2018-01-04 18:09 har:///user/hadoop/example2.har/test1.txt</div><div class="line">-rw-r--r--   3 hadoop supergroup         30 2018-01-04 18:10 har:///user/hadoop/example2.har/test2.txt</div><div class="line">-rw-r--r--   3 hadoop supergroup         15 2018-01-04 18:10 har:///user/hadoop/example2.har/test3.txt</div><div class="line"></div><div class="line">##解壓縮使用cp命令</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir -p /user/hadoop/example/lab_3</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cp har:///user/hadoop/example2.har/* hdfs:/user/hadoop/example/lab_3</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R /user/hadoop/example/lab_3</div><div class="line">-rw-r--r--   1 hadoop supergroup         23 2018-01-04 18:34 /user/hadoop/example/lab_3/test1.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup         30 2018-01-04 18:34 /user/hadoop/example/lab_3/test2.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup         15 2018-01-04 18:34 /user/hadoop/example/lab_3/test3.txt</div></pre></td></tr></table></figure>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;hadoop壓縮機制&quot;&gt;&lt;a href=&quot;#hadoop壓縮機制&quot; class=&quot;headerlink&quot; title=&quot;hadoop壓縮機制&quot;&gt;&lt;/a&gt;hadoop壓縮機制&lt;/h5&gt;&lt;blockquote&gt;
&lt;figure class=&quot;highlight pla
    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/BigData/Hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hdfs snapshot command Memo</title>
    <link href="http://yoursite.com/2018/01/04/%5Bhadoop%5Dhadoop_snapshot_cmd/"/>
    <id>http://yoursite.com/2018/01/04/[hadoop]hadoop_snapshot_cmd/</id>
    <published>2018-01-03T16:00:00.000Z</published>
    <updated>2020-07-22T08:28:34.749Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Hadoop-Snapshot機制用到的相關語法"><a href="#Hadoop-Snapshot機制用到的相關語法" class="headerlink" title="Hadoop Snapshot機制用到的相關語法"></a>Hadoop Snapshot機制用到的相關語法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">hdfs lsSnapshottableDir</div><div class="line">hdfs snapshotDiff</div><div class="line">hdfs dfsadmin -allowSnapshot</div><div class="line">hdfs dfsadmin -disallowSnapshot</div><div class="line">hdfs dfs -createSnapshot</div><div class="line">hdfs dfs -deleteSnapshot</div><div class="line">hdfs dfs -renameSnapshot</div><div class="line">hdfs fsck -includeSnapshots</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="Snapshot機制操作說明"><a href="#Snapshot機制操作說明" class="headerlink" title="Snapshot機制操作說明"></a>Snapshot機制操作說明</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir /user/hadoop/snapshot</div><div class="line"></div><div class="line">##選定一個目錄做為Snapshot的起點</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -allowSnapshot /user/hadoop/snapshot</div><div class="line">Allowing snaphot on /user/hadoop/snapshot succeeded</div><div class="line"></div><div class="line">##列出所有要做Snapshot目錄</div><div class="line">hadoop@hadoop-master:~$ hdfs lsSnapshottableDir</div><div class="line">drwxr-xr-x 0 hadoop supergroup 0 2018-01-03 16:50 1 65536 /user/hadoop/snapshot</div><div class="line"></div><div class="line">##建立Snapshot</div><div class="line">hadoop@hadoop-master:~$  hadoop fs -createSnapshot /user/hadoop/snapshot snapshot</div><div class="line">Created snapshot /user/hadoop/snapshot/.snapshot/snapshot</div><div class="line"></div><div class="line">##在Snapshot目錄中新增一個檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/snapshot/test1.txt</div><div class="line">11111</div><div class="line">2222</div><div class="line">33333</div><div class="line">44444</div><div class="line">55555</div><div class="line"></div><div class="line">##再做一次Snapshot</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -createSnapshot /user/hadoop/snapshot snapshot_201801031654</div><div class="line">Created snapshot /user/hadoop/snapshot/.snapshot/snapshot_201801031654</div><div class="line"></div><div class="line">##比較兩個Snapshot之間的差異</div><div class="line">hadoop@hadoop-master:~$ hdfs snapshotDiff /user/hadoop/snapshot snapshot snapshot_201801031654</div><div class="line">Difference between snapshot snapshot and snapshot snapshot_201801031654 under directory /user/hadoop/snapshot:</div><div class="line">M       .</div><div class="line">+       ./test1.txt</div><div class="line"></div><div class="line">##以下為復原Snapshot方式</div><div class="line">hadoop@hadoop-master:~$hadoop fs -rm -r -skipTrash /user/hadoop/snapshot/*</div><div class="line">hadoop@hadoop-master:~$hadoop fs -cp /user/hadoop/snapshot/.snapshot/snapshot/* /user/hadoop/snapshot</div><div class="line"></div><div class="line"></div><div class="line">##刪除Snapshot</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir /tmp/important-data</div><div class="line">hadoop@hadoop-master:~$ echo &quot;important data&quot; | hdfs dfs -put - /tmp/important-dir/important-file.txt</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -allowSnapshot  /tmp/important-dir</div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -createSnapshot /tmp/important-dir first-snapshot</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs lsSnapshottableDir</div><div class="line">drwxr-xr-x 0 hadoop supergroup 0 2018-01-04 11:12 1 65536 /tmp/important-dir</div><div class="line">drwxr-xr-x 0 hadoop supergroup 0 2018-01-04 11:59 3 65536 /user/hadoop/snapshot</div><div class="line"></div><div class="line">##此目錄下尚有其他Snapshot存在無法操作disallowSnapshot</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -disallowSnapshot /tmp/important-dir</div><div class="line">disallowSnapshot: The directory /tmp/important-dir has snapshot(s). Please redo the operation after removing all the snapshots.</div><div class="line"></div><div class="line">##須將所有Snapshot刪除後,才能操作disallowSnapshot</div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -deleteSnapshot /tmp/important-dir first-snapshot</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -disallowSnapshot /tmp/important-dir</div><div class="line">Disallowing snaphot on /tmp/important-dir succeeded</div><div class="line">hadoop@hadoop-master:~$ hdfs lsSnapshottableDir</div><div class="line">drwxr-xr-x 0 hadoop supergroup 0 2018-01-04 11:59 3 65536 /user/hadoop/snapshot</div><div class="line"></div><div class="line">##更改Snapshot名稱,並顯示snapshottable下所有的Snapshot</div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -renameSnapshot /user/hadoop/snapshot snapshot snapshot_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/snapshot/.snapshot</div><div class="line">Found 3 items</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-03 17:01 /user/hadoop/snapshot/.snapshot/snapshot_1</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-03 17:01 /user/hadoop/snapshot/.snapshot/snapshot_201801031654</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-03 18:01 /user/hadoop/snapshot/.snapshot/snapshot_201801031737</div></pre></td></tr></table></figure>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;Hadoop-Snapshot機制用到的相關語法&quot;&gt;&lt;a href=&quot;#Hadoop-Snapshot機制用到的相關語法&quot; class=&quot;headerlink&quot; title=&quot;Hadoop Snapshot機制用到的相關語法&quot;&gt;&lt;/a&gt;Hadoop Snapsho
    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/BigData/Hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hdfs fsck command Memo</title>
    <link href="http://yoursite.com/2018/01/03/%5Bhadoop%5Dhadoop_fsck_cmd/"/>
    <id>http://yoursite.com/2018/01/03/[hadoop]hadoop_fsck_cmd/</id>
    <published>2018-01-02T16:00:00.000Z</published>
    <updated>2020-07-22T08:27:53.217Z</updated>
    
    <content type="html"><![CDATA[<h4 id="hadoop-fsck使用方式"><a href="#hadoop-fsck使用方式" class="headerlink" title="hadoop fsck使用方式"></a>hadoop fsck使用方式</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:15:13 CST 2018</div><div class="line"></div><div class="line">Status: HEALTHY</div><div class="line"> Number of data-nodes:  2</div><div class="line"> Number of racks:               1</div><div class="line"> Total dirs:                    0</div><div class="line"> Total symlinks:                0</div><div class="line"></div><div class="line">Replicated Blocks:</div><div class="line"> Total size:    133 B</div><div class="line"> Total files:   1</div><div class="line"> Total blocks (validated):      1 (avg. block size 133 B)</div><div class="line"> Minimally replicated blocks:   1 (100.0 %)</div><div class="line"> Over-replicated blocks:        0 (0.0 %)</div><div class="line"> Under-replicated blocks:       0 (0.0 %)</div><div class="line"> Mis-replicated blocks:         0 (0.0 %)</div><div class="line"> Default replication factor:    2</div><div class="line"> Average block replication:     2.0</div><div class="line"> Missing blocks:                0</div><div class="line"> Corrupt blocks:                0</div><div class="line"> Missing replicas:              0 (0.0 %)</div><div class="line"></div><div class="line">Erasure Coded Block Groups:</div><div class="line"> Total size:    0 B</div><div class="line"> Total files:   0</div><div class="line"> Total block groups (validated):        0</div><div class="line"> Minimally erasure-coded block groups:  0</div><div class="line"> Over-erasure-coded block groups:       0</div><div class="line"> Under-erasure-coded block groups:      0</div><div class="line"> Unsatisfactory placement block groups: 0</div><div class="line"> Average block group size:      0.0</div><div class="line"> Missing block groups:          0</div><div class="line"> Corrupt block groups:          0</div><div class="line"> Missing internal blocks:       0</div><div class="line">FSCK ended at Wed Jan 03 13:15:13 CST 2018 in 1 milliseconds</div><div class="line"></div><div class="line"></div><div class="line">The filesystem under path &apos;/user/hadoop/test1.txt&apos; is HEALTHY</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="hdfs-fsck-相關參數操作說明"><a href="#hdfs-fsck-相關參數操作說明" class="headerlink" title="hdfs fsck 相關參數操作說明"></a>hdfs fsck 相關參數操作說明</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line">##檢查並列出所有文件的狀態</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:17:02 CST 2018</div><div class="line">/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s):  OK</div><div class="line"></div><div class="line">##列出所有Blocks資訊</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files -blocks</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:18:46 CST 2018</div><div class="line">/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s):  OK</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_1073743138_2323 len=133 Live_repl=2</div><div class="line"></div><div class="line">##列出所有Blocks的位置訊息</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files -blocks -locations</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;locations=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:19:53 CST 2018</div><div class="line">/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s):  OK</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_1073743138_2323 len=133 Live_repl=2  </div><div class="line">[DatanodeInfoWithStorage[192.168.51.6:9866,DS-482194d9-aa70-4ab8-8253-907739d5b1a1,DISK], </div><div class="line"> DatanodeInfoWithStorage[192.168.51.5:9866,DS-f59711c6-801d-4ebc-b55c-228f225117b8,DISK]]</div><div class="line"></div><div class="line">##列出檔案所有的訊息包含Rack位置</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files -blocks -locations -racks</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;locations=1&amp;racks=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Thu Jan 04 18:43:12 CST 2018</div><div class="line">/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s):  OK</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_1073743138_2323 len=133 Live_repl=2  [/default-rack/192.168.51.5:9866, /default-rack/192.168.51.6:9866]</div><div class="line"></div><div class="line"></div><div class="line">##列出所有完整的replication的位置訊息</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files -blocks -replicaDetails</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;replicadetails=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:21:56 CST 2018</div><div class="line">/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s):  OK</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_1073743138_2323 len=133 Live_repl=2  </div><div class="line">[DatanodeInfoWithStorage[192.168.51.6:9866,DS-482194d9-aa70-4ab8-8253-907739d5b1a1,DISK](LIVE), </div><div class="line"> DatanodeInfoWithStorage[192.168.51.5:9866,DS-f59711c6-801d-4ebc-b55c-228f225117b8,DISK](LIVE)]</div><div class="line"></div><div class="line">##查看文件中損壞Blocks的狀況</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -list-corruptfileblocks</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;listcorruptfileblocks=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">The filesystem under path &apos;/user/hadoop/test1.txt&apos; has 0 CORRUPT files</div><div class="line"></div><div class="line">##列出指定的block的詳細資訊</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -blockId blk_1073743138</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;blockId=blk_1073743138+&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 at Wed Jan 03 13:34:40 CST 2018</div><div class="line"></div><div class="line">Block Id: blk_1073743138</div><div class="line">Block belongs to: /user/hadoop/test1.txt</div><div class="line">No. of Expected Replica: 2</div><div class="line">No. of live Replica: 2</div><div class="line">No. of excess Replica: 0</div><div class="line">No. of stale Replica: 0</div><div class="line">No. of decommissioned Replica: 0</div><div class="line">No. of decommissioning Replica: 0</div><div class="line">No. of corrupted Replica: 0</div><div class="line">Block replica on datanode/rack: hadoop-slave1/default-rack is HEALTHY</div><div class="line">Block replica on datanode/rack: hadoop-slave2/default-rack is HEALTHY</div></pre></td></tr></table></figure></blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;hadoop-fsck使用方式&quot;&gt;&lt;a href=&quot;#hadoop-fsck使用方式&quot; class=&quot;headerlink&quot; title=&quot;hadoop fsck使用方式&quot;&gt;&lt;/a&gt;hadoop fsck使用方式&lt;/h4&gt;&lt;blockquote&gt;
&lt;figure 
    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/BigData/Hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hdfs dfsadmin command Memo</title>
    <link href="http://yoursite.com/2018/01/03/%5Bhadoop%5Dhdfs_dfsadmin_cmd/"/>
    <id>http://yoursite.com/2018/01/03/[hadoop]hdfs_dfsadmin_cmd/</id>
    <published>2018-01-02T16:00:00.000Z</published>
    <updated>2020-07-22T08:31:39.885Z</updated>
    
    <content type="html"><![CDATA[<h5 id="hdfs-dfsadmin相關語法"><a href="#hdfs-dfsadmin相關語法" class="headerlink" title="hdfs dfsadmin相關語法"></a>hdfs dfsadmin相關語法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">Note: Administrative commands can only be run as the HDFS superuser.</div><div class="line">        [-report [-live] [-dead] [-decommissioning] [-enteringmaintenance] [-inmaintenance]]</div><div class="line">        [-safemode &lt;enter | leave | get | wait&gt;]</div><div class="line">        [-saveNamespace [-beforeShutdown]]</div><div class="line">        [-rollEdits]</div><div class="line">        [-restoreFailedStorage true|false|check]</div><div class="line">        [-refreshNodes]</div><div class="line">        [-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">        [-clrQuota &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">        [-setSpaceQuota &lt;quota&gt; [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">        [-clrSpaceQuota [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">        [-finalizeUpgrade]</div><div class="line">        [-rollingUpgrade [&lt;query|prepare|finalize&gt;]]</div><div class="line">        [-refreshServiceAcl]</div><div class="line">        [-refreshUserToGroupsMappings]</div><div class="line">        [-refreshSuperUserGroupsConfiguration]</div><div class="line">        [-refreshCallQueue]</div><div class="line">        [-refresh &lt;host:ipc_port&gt; &lt;key&gt; [arg1..argn]</div><div class="line">        [-reconfig &lt;namenode|datanode&gt; &lt;host:ipc_port&gt; &lt;start|status|properties&gt;]</div><div class="line">        [-printTopology]</div><div class="line">        [-refreshNamenodes datanode_host:ipc_port]</div><div class="line">        [-getVolumeReport datanode_host:ipc_port]</div><div class="line">        [-deleteBlockPool datanode_host:ipc_port blockpoolId [force]]</div><div class="line">        [-setBalancerBandwidth &lt;bandwidth in bytes per second&gt;]</div><div class="line">        [-getBalancerBandwidth &lt;datanode_host:ipc_port&gt;]</div><div class="line">        [-fetchImage &lt;local directory&gt;]</div><div class="line">        [-allowSnapshot &lt;snapshotDir&gt;]</div><div class="line">        [-disallowSnapshot &lt;snapshotDir&gt;]</div><div class="line">        [-shutdownDatanode &lt;datanode_host:ipc_port&gt; [upgrade]]</div><div class="line">        [-evictWriters &lt;datanode_host:ipc_port&gt;]</div><div class="line">        [-getDatanodeInfo &lt;datanode_host:ipc_port&gt;]</div><div class="line">        [-metasave filename]</div><div class="line">        [-triggerBlockReport [-incremental] &lt;datanode_host:ipc_port&gt;]</div><div class="line">        [-listOpenFiles]</div><div class="line">        [-help [cmd]]</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-report"><a href="#hdfs-dfsadmin-report" class="headerlink" title="hdfs dfsadmin -report"></a>hdfs dfsadmin -report</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -report</div><div class="line">Configured Capacity: 42002972672 (39.12 GB)</div><div class="line">Present Capacity: 26379444224 (24.57 GB)</div><div class="line">DFS Remaining: 26331783168 (24.52 GB)</div><div class="line">DFS Used: 47661056 (45.45 MB)</div><div class="line">DFS Used%: 0.18%</div><div class="line">Replicated Blocks:</div><div class="line">        Under replicated blocks: 967</div><div class="line">        Blocks with corrupt replicas: 0</div><div class="line">        Missing blocks: 0</div><div class="line">        Missing blocks (with replication factor 1): 0</div><div class="line">        Pending deletion blocks: 0</div><div class="line">Erasure Coded Block Groups:</div><div class="line">        Low redundancy block groups: 0</div><div class="line">        Block groups with corrupt internal blocks: 0</div><div class="line">        Missing block groups: 0</div><div class="line">        Pending deletion blocks: 0</div><div class="line"></div><div class="line">-------------------------------------------------</div><div class="line">Live datanodes (2):</div><div class="line"></div><div class="line">Name: 192.168.51.5:9866 (hadoop-slave1)</div><div class="line">Hostname: hadoop-slave1</div><div class="line">Decommission Status : Normal</div><div class="line">Configured Capacity: 21001486336 (19.56 GB)</div><div class="line">DFS Used: 23842816 (22.74 MB)</div><div class="line">Non DFS Used: 6787604480 (6.32 GB)</div><div class="line">DFS Remaining: 13099626496 (12.20 GB)</div><div class="line">DFS Used%: 0.11%</div><div class="line">DFS Remaining%: 62.37%</div><div class="line">Configured Cache Capacity: 0 (0 B)</div><div class="line">Cache Used: 0 (0 B)</div><div class="line">Cache Remaining: 0 (0 B)</div><div class="line">Cache Used%: 100.00%</div><div class="line">Cache Remaining%: 0.00%</div><div class="line">Xceivers: 1</div><div class="line">Last contact: Wed Jan 03 17:52:32 CST 2018</div><div class="line">Last Block Report: Wed Jan 03 15:42:49 CST 2018</div><div class="line"></div><div class="line"></div><div class="line">Name: 192.168.51.6:9866 (hadoop-slave2)</div><div class="line">Hostname: hadoop-slave2</div><div class="line">Decommission Status : Normal</div><div class="line">Configured Capacity: 21001486336 (19.56 GB)</div><div class="line">DFS Used: 23818240 (22.71 MB)</div><div class="line">Non DFS Used: 6655098880 (6.20 GB)</div><div class="line">DFS Remaining: 13232156672 (12.32 GB)</div><div class="line">DFS Used%: 0.11%</div><div class="line">DFS Remaining%: 63.01%</div><div class="line">Configured Cache Capacity: 0 (0 B)</div><div class="line">Cache Used: 0 (0 B)</div><div class="line">Cache Remaining: 0 (0 B)</div><div class="line">Cache Used%: 100.00%</div><div class="line">Cache Remaining%: 0.00%</div><div class="line">Xceivers: 1</div><div class="line">Last contact: Wed Jan 03 17:52:33 CST 2018</div><div class="line">Last Block Report: Wed Jan 03 16:33:35 CST 2018</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-printTopology"><a href="#hdfs-dfsadmin-printTopology" class="headerlink" title="hdfs dfsadmin -printTopology"></a>hdfs dfsadmin -printTopology</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -printTopology</div><div class="line">Rack: /default-rack</div><div class="line">   192.168.51.5:9866 (hadoop-slave1)</div><div class="line">   192.168.51.6:9866 (hadoop-slave2)</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-getDatanodeInfo"><a href="#hdfs-dfsadmin-getDatanodeInfo" class="headerlink" title="hdfs dfsadmin -getDatanodeInfo"></a>hdfs dfsadmin -getDatanodeInfo</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getDatanodeInfo 192.168.51.6:9867</div><div class="line">Uptime: 5100, Software version: 3.0.0, Config version: core-3.0.0,hdfs-1</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getDatanodeInfo 192.168.51.5:9867</div><div class="line">Uptime: 10709, Software version: 3.0.0, Config version: core-3.0.0,hdfs-1</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-safemode"><a href="#hdfs-dfsadmin-safemode" class="headerlink" title="hdfs dfsadmin -safemode"></a>hdfs dfsadmin -safemode</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -safemode</div><div class="line">Usage: hdfs dfsadmin [-safemode enter | leave | get | wait | forceExit]</div><div class="line"></div><div class="line">##取得safemode模式</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -safemode get</div><div class="line">Safe mode is OFF</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/snapshot/test3.txt</div><div class="line">3333</div><div class="line">4444</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/snapshot</div><div class="line">Found 3 items</div><div class="line">-rw-r--r--   1 hadoop supergroup         29 2018-01-03 16:53 /user/hadoop/snapshot/test1.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup          7 2018-01-03 17:36 /user/hadoop/snapshot/test2.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup         10 2018-01-03 18:04 /user/hadoop/snapshot/test3.txt</div><div class="line"></div><div class="line">##進入safemode模式</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -safemode enter</div><div class="line">Safe mode is ON</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -safemode get</div><div class="line">Safe mode is ON</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/snapshot/test4.txt</div><div class="line">put: Cannot create file/user/hadoop/snapshot/test4.txt._COPYING_. Name node is in safe mode.</div><div class="line"></div><div class="line">##離開safemode模式</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -safemode leave</div><div class="line">Safe mode is OFF</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/snapshot/test4.txt</div><div class="line">55555</div><div class="line">66666</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/snapshot</div><div class="line">Found 4 items</div><div class="line">-rw-r--r--   1 hadoop supergroup         29 2018-01-03 16:53 /user/hadoop/snapshot/test1.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup          7 2018-01-03 17:36 /user/hadoop/snapshot/test2.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup         10 2018-01-03 18:04 /user/hadoop/snapshot/test3.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup         12 2018-01-03 18:05 /user/hadoop/snapshot/test4.txt</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-shutdownDatanode"><a href="#hdfs-dfsadmin-shutdownDatanode" class="headerlink" title="hdfs dfsadmin -shutdownDatanode"></a>hdfs dfsadmin -shutdownDatanode</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">##停掉 Datanode</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -shutdownDatanode hadoop-slave2:9867</div><div class="line">Submitted a shutdown request to datanode hadoop-slave2:9867</div><div class="line"></div><div class="line">##觀察 Datanode Status</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -report -dead</div><div class="line">Safe mode is ON</div><div class="line">Configured Capacity: 21001486336 (19.56 GB)</div><div class="line">Present Capacity: 13123469312 (12.22 GB)</div><div class="line">DFS Remaining: 13099626496 (12.20 GB)</div><div class="line">DFS Used: 23842816 (22.74 MB)</div><div class="line">DFS Used%: 0.18%</div><div class="line">Replicated Blocks:</div><div class="line">        Under replicated blocks: 971</div><div class="line">        Blocks with corrupt replicas: 0</div><div class="line">        Missing blocks: 2</div><div class="line">        Missing blocks (with replication factor 1): 2</div><div class="line">        Pending deletion blocks: 0</div><div class="line">Erasure Coded Block Groups:</div><div class="line">        Low redundancy block groups: 1</div><div class="line">        Block groups with corrupt internal blocks: 0</div><div class="line">        Missing block groups: 0</div><div class="line">        Pending deletion blocks: 0</div><div class="line"></div><div class="line">-------------------------------------------------</div><div class="line">Dead datanodes (1):</div><div class="line"></div><div class="line">Name: 192.168.51.6:9866 (hadoop-slave2)</div><div class="line">Hostname: hadoop-slave2</div><div class="line">Decommission Status : Normal</div><div class="line">Configured Capacity: 21001486336 (19.56 GB)</div><div class="line">DFS Used: 23834624 (22.73 MB)</div><div class="line">Non DFS Used: 6655102976 (6.20 GB)</div><div class="line">DFS Remaining: 13232136192 (12.32 GB)</div><div class="line">DFS Used%: 0.11%</div><div class="line">DFS Remaining%: 63.01%</div><div class="line">Configured Cache Capacity: 0 (0 B)</div><div class="line">Cache Used: 0 (0 B)</div><div class="line">Cache Remaining: 0 (0 B)</div><div class="line">Cache Used%: 100.00%</div><div class="line">Cache Remaining%: 0.00%</div><div class="line">Xceivers: 0</div><div class="line">Last contact: Wed Jan 03 18:16:00 CST 2018</div><div class="line">Last Block Report: Wed Jan 03 16:33:35 CST 2018</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-metasave"><a href="#hdfs-dfsadmin-metasave" class="headerlink" title="hdfs dfsadmin -metasave"></a>hdfs dfsadmin -metasave</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">## 產生所有metadata資訊,產生後會存放於hdfs系統的Log資料夾內</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -metasave metasave.txt</div><div class="line">Created metasave file metasave.txt in the log directory of namenode hdfs://hadoop-master:8020</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-listOpenFiles"><a href="#hdfs-dfsadmin-listOpenFiles" class="headerlink" title="hdfs dfsadmin -listOpenFiles"></a>hdfs dfsadmin -listOpenFiles</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -listOpenFiles</div><div class="line">Client Host             Client Name             Open File Path</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-triggerBlockReport"><a href="#hdfs-dfsadmin-triggerBlockReport" class="headerlink" title="hdfs dfsadmin -triggerBlockReport"></a>hdfs dfsadmin -triggerBlockReport</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -triggerBlockReport 192.168.51.5:9867</div><div class="line">Triggering a full block report on 192.168.51.5:9867.</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-BalancerBandwidth"><a href="#hdfs-dfsadmin-BalancerBandwidth" class="headerlink" title="hdfs dfsadmin BalancerBandwidth"></a>hdfs dfsadmin BalancerBandwidth</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">##取得Namenode與各Datanode之間的IPC頻寬(目前設定為10M bytes/s)</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getBalancerBandwidth hadoop-slave1:9867</div><div class="line">Balancer bandwidth is 10485760 bytes per second.</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getBalancerBandwidth hadoop-slave2:9867</div><div class="line">Balancer bandwidth is 10485760 bytes per second.</div><div class="line"></div><div class="line">##設定Namenode與Datanode之間的頻寬大小</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -setBalancerBandwidth 15M</div><div class="line">Balancer bandwidth is set to 15728640</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getBalancerBandwidth hadoop-slave2:9867</div><div class="line">Balancer bandwidth is 15728640 bytes per second.</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getBalancerBandwidth hadoop-slave1:9867</div><div class="line">Balancer bandwidth is 15728640 bytes per second.</div></pre></td></tr></table></figure>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;hdfs-dfsadmin相關語法&quot;&gt;&lt;a href=&quot;#hdfs-dfsadmin相關語法&quot; class=&quot;headerlink&quot; title=&quot;hdfs dfsadmin相關語法&quot;&gt;&lt;/a&gt;hdfs dfsadmin相關語法&lt;/h5&gt;&lt;blockquote&gt;

    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/BigData/Hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hdfs ec command Memo</title>
    <link href="http://yoursite.com/2018/01/03/%5Bhadoop%5Dhdfs_ec_cmd/"/>
    <id>http://yoursite.com/2018/01/03/[hadoop]hdfs_ec_cmd/</id>
    <published>2018-01-02T16:00:00.000Z</published>
    <updated>2020-07-22T08:32:10.444Z</updated>
    
    <content type="html"><![CDATA[<h4 id="hdfs-ec-使用方式-以XOR-2-1-1024k-Policy做為測試"><a href="#hdfs-ec-使用方式-以XOR-2-1-1024k-Policy做為測試" class="headerlink" title="hdfs ec 使用方式(以XOR-2-1-1024k Policy做為測試)"></a>hdfs ec 使用方式(以XOR-2-1-1024k Policy做為測試)</h4><blockquote>
<p>EC相關架構的介紹可以參考網路相關的文章<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div></pre></td><td class="code"><pre><div class="line">Usage: bin/hdfs ec [COMMAND]</div><div class="line">          [-listPolicies]</div><div class="line">          [-addPolicies -policyFile &lt;file&gt;]</div><div class="line">          [-getPolicy -path &lt;path&gt;]</div><div class="line">          [-removePolicy -policy &lt;policy&gt;]</div><div class="line">          [-setPolicy -path &lt;path&gt; [-policy &lt;policy&gt;] [-replicate]]</div><div class="line">          [-unsetPolicy -path &lt;path&gt;]</div><div class="line">          [-listCodecs]</div><div class="line">          [-enablePolicy -policy &lt;policy&gt;]</div><div class="line">          [-disablePolicy -policy &lt;policy&gt;]</div><div class="line">          [-help &lt;command-name&gt;]</div><div class="line"></div><div class="line">##列出所有Erasure Coding可用的相關Policies</div><div class="line">hadoop@hadoop-master:~$ hdfs ec -listPolicies</div><div class="line">Erasure Coding Policies:</div><div class="line">ErasureCodingPolicy=[Name=RS-10-4-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=10, numParityUnits=4]], CellSize=1048576, Id=5], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-3-2-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=3, numParityUnits=2]], CellSize=1048576, Id=2], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-LEGACY-6-3-1024k, Schema=[ECSchema=[Codec=rs-legacy, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=3], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=XOR-2-1-1024k, Schema=[ECSchema=[Codec=xor, numDataUnits=2, numParityUnits=1]], CellSize=1048576, Id=4], State=DISABLED</div><div class="line"></div><div class="line">##列出所有Erasure Coding Codec列表</div><div class="line">hadoop@hadoop-master:~$ hdfs ec -listCodecs</div><div class="line">Erasure Coding Codecs: Codec [Coder List]</div><div class="line">        RS [RS_NATIVE, RS_JAVA]</div><div class="line">        RS-LEGACY [RS-LEGACY_JAVA]</div><div class="line">        XOR [XOR_NATIVE, XOR_JAVA]</div><div class="line"></div><div class="line">##Enable XOR-2-1-1024k的EC Policy		</div><div class="line">hadoop@hadoop-master:~$ hdfs ec -enablePolicy -policy XOR-2-1-1024k</div><div class="line">Erasure coding policy XOR-2-1-1024k is enabled</div><div class="line">hadoop@hadoop-master:~$ hdfs ec -listPolicies</div><div class="line">Erasure Coding Policies:</div><div class="line">ErasureCodingPolicy=[Name=RS-10-4-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=10, numParityUnits=4]], CellSize=1048576, Id=5], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-3-2-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=3, numParityUnits=2]], CellSize=1048576, Id=2], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-LEGACY-6-3-1024k, Schema=[ECSchema=[Codec=rs-legacy, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=3], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=XOR-2-1-1024k, Schema=[ECSchema=[Codec=xor, numDataUnits=2, numParityUnits=1]], CellSize=1048576, Id=4], State=ENABLED</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir -p /user/hadoop/example/lab_2</div><div class="line"></div><div class="line">##將目錄設定為XOR-2-1-1024k的EC Policy</div><div class="line">hadoop@hadoop-master:~$ hdfs ec -setPolicy -path /user/hadoop/example/lab_2 -policy XOR-2-1-1024k</div><div class="line">Set erasure coding policy XOR-2-1-1024k on /user/hadoop/example/lab_2</div><div class="line"></div><div class="line">##上傳檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put ~/test9.tar.gz /user/hadoop/example/lab_2</div><div class="line">2018-01-03 15:41:12,678 WARN erasurecode.ErasureCodeNative: ISA-L support is not available in your platform... using builtin-java codec where applicable</div><div class="line">2018-01-03 15:41:12,741 WARN hdfs.DFSOutputStream: Cannot allocate parity block(index=2, policy=XOR-2-1-1024k). Not enough datanodes? Exclude nodes=[]</div><div class="line">2018-01-03 15:41:12,953 WARN hdfs.DFSOutputStream: Block group &lt;1&gt; has 1 corrupt blocks. It&apos;s at high risk of losing data.</div><div class="line"></div><div class="line">##觀察上傳檔案的複本數</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_2</div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   1 hadoop supergroup        373 2018-01-03 15:41 /user/hadoop/example/lab_2/test9.tar.gz</div><div class="line"></div><div class="line">##以下為使用fsck指令觀察該檔案Block相關訊息</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/example/lab_2/test9.tar.gz -files -blocks -locations</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;locations=1&amp;path=%2Fuser%2Fhadoop%2Fexample%2Flab_2%2Ftest9.tar.gz</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/example/lab_2/test9.tar.gz at Wed Jan 03 15:46:48 CST 2018</div><div class="line">/user/hadoop/example/lab_2/test9.tar.gz 373 bytes, erasure-coded: policy=XOR-2-1-1024k, 1 block(s):  OK</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_-9223372036854775776_2346 len=373 Live_repl=2  </div><div class="line">[blk_-9223372036854775776:DatanodeInfoWithStorage[192.168.51.5:9866,DS-f59711c6-801d-4ebc-b55c-228f225117b8,DISK], </div><div class="line"> blk_-9223372036854775774:DatanodeInfoWithStorage[192.168.51.6:9866,DS-482194d9-aa70-4ab8-8253-907739d5b1a1,DISK]]</div><div class="line"></div><div class="line"></div><div class="line">Status: HEALTHY</div><div class="line"> Number of data-nodes:  2</div><div class="line"> Number of racks:               1</div><div class="line"> Total dirs:                    0</div><div class="line"> Total symlinks:                0</div><div class="line"></div><div class="line">Replicated Blocks:</div><div class="line"> Total size:    0 B</div><div class="line"> Total files:   0</div><div class="line"> Total blocks (validated):      0</div><div class="line"> Minimally replicated blocks:   0</div><div class="line"> Over-replicated blocks:        0</div><div class="line"> Under-replicated blocks:       0</div><div class="line"> Mis-replicated blocks:         0</div><div class="line"> Default replication factor:    1</div><div class="line"> Average block replication:     0.0</div><div class="line"> Missing blocks:                0</div><div class="line"> Corrupt blocks:                0</div><div class="line"> Missing replicas:              0</div><div class="line"></div><div class="line">Erasure Coded Block Groups:</div><div class="line"> Total size:    373 B</div><div class="line"> Total files:   1</div><div class="line"> Total block groups (validated):        1 (avg. block group size 373 B)</div><div class="line"> Minimally erasure-coded block groups:  1 (100.0 %)</div><div class="line"> Over-erasure-coded block groups:       0 (0.0 %)</div><div class="line"> Under-erasure-coded block groups:      0 (0.0 %)</div><div class="line"> Unsatisfactory placement block groups: 0 (0.0 %)</div><div class="line"> Average block group size:      2.0</div><div class="line"> Missing block groups:          0</div><div class="line"> Corrupt block groups:          0</div><div class="line"> Missing internal blocks:       0 (0.0 %)</div><div class="line">FSCK ended at Wed Jan 03 15:46:48 CST 2018 in 2 milliseconds</div><div class="line"></div><div class="line"></div><div class="line">The filesystem under path &apos;/user/hadoop/example/lab_2/test9.tar.gz&apos; is HEALTHY</div><div class="line"></div><div class="line">##以blockId觀察複本狀況</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck -blockId blk_-9223372036854775776</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;blockId=blk_-9223372036854775776+&amp;path=%2F</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 at Wed Jan 03 15:49:12 CST 2018</div><div class="line"></div><div class="line">Block Id: blk_-9223372036854775776</div><div class="line">Block belongs to: /user/hadoop/example/lab_2/test9.tar.gz</div><div class="line">No. of Expected Replica: 2</div><div class="line">No. of live Replica: 2</div><div class="line">No. of excess Replica: 0</div><div class="line">No. of stale Replica: 0</div><div class="line">No. of decommissioned Replica: 0</div><div class="line">No. of decommissioning Replica: 0</div><div class="line">No. of corrupted Replica: 0</div><div class="line">null</div><div class="line"></div><div class="line"></div><div class="line">Fsck on blockId &apos;blk_-9223372036854775776</div><div class="line"></div><div class="line">##以下為模擬shutdown 一台Datanode,資料是否還會存在??</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -shutdownDatanode hadoop-slave2:9867</div><div class="line">Submitted a shutdown request to datanode hadoop-slave2:9867</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/example/lab_2/test9.tar.gz -files -blocks -locations</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;locations=1&amp;path=%2Fuser%2Fhadoop%2Fexample%2Flab_2%2Ftest9.tar.gz</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/example/lab_2/test9.tar.gz at Wed Jan 03 16:11:01 CST 2018</div><div class="line">/user/hadoop/example/lab_2/test9.tar.gz 373 bytes, erasure-coded: policy=XOR-2-1-1024k, 1 block(s):  </div><div class="line">Under replicated BP-1139418417-192.168.51.4-1514272982687:blk_-9223372036854775776_2346. </div><div class="line">Target Replicas is 2 but found 1 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_-9223372036854775776_2346 len=373 Live_repl=1  </div><div class="line">[blk_-9223372036854775776:DatanodeInfoWithStorage[192.168.51.5:9866,DS-f59711c6-801d-4ebc-b55c-228f225117b8,DISK]]</div><div class="line"></div><div class="line"></div><div class="line">Status: HEALTHY</div><div class="line"> Number of data-nodes:  1</div><div class="line"> Number of racks:               1</div><div class="line"> Total dirs:                    0</div><div class="line"> Total symlinks:                0</div><div class="line"></div><div class="line">Replicated Blocks:</div><div class="line"> Total size:    0 B</div><div class="line"> Total files:   0</div><div class="line"> Total blocks (validated):      0</div><div class="line"> Minimally replicated blocks:   0</div><div class="line"> Over-replicated blocks:        0</div><div class="line"> Under-replicated blocks:       0</div><div class="line"> Mis-replicated blocks:         0</div><div class="line"> Default replication factor:    1</div><div class="line"> Average block replication:     0.0</div><div class="line"> Missing blocks:                0</div><div class="line"> Corrupt blocks:                0</div><div class="line"> Missing replicas:              0</div><div class="line"></div><div class="line">Erasure Coded Block Groups:</div><div class="line"> Total size:    373 B</div><div class="line"> Total files:   1</div><div class="line"> Total block groups (validated):        1 (avg. block group size 373 B)</div><div class="line"> Minimally erasure-coded block groups:  1 (100.0 %)</div><div class="line"> Over-erasure-coded block groups:       0 (0.0 %)</div><div class="line"> Under-erasure-coded block groups:      1 (100.0 %)</div><div class="line"> Unsatisfactory placement block groups: 0 (0.0 %)</div><div class="line"> Average block group size:      1.0</div><div class="line"> Missing block groups:          0</div><div class="line"> Corrupt block groups:          0</div><div class="line"> Missing internal blocks:       1 (50.0 %)</div><div class="line">FSCK ended at Wed Jan 03 16:11:01 CST 2018 in 3 milliseconds</div><div class="line"></div><div class="line"></div><div class="line">The filesystem under path &apos;/user/hadoop/example/lab_2/test9.tar.gz&apos; is HEALTHY</div><div class="line"></div><div class="line">雖然複本數是設定為1,但因為使用的是EC策略,資料仍然是可下載而且資料內容也是正確的</div></pre></td></tr></table></figure></p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;hdfs-ec-使用方式-以XOR-2-1-1024k-Policy做為測試&quot;&gt;&lt;a href=&quot;#hdfs-ec-使用方式-以XOR-2-1-1024k-Policy做為測試&quot; class=&quot;headerlink&quot; title=&quot;hdfs ec 使用方式(以XO
    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/BigData/Hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hdfs client command Memo</title>
    <link href="http://yoursite.com/2018/01/03/%5Bhadoop%5Dhdfs_other_cmd/"/>
    <id>http://yoursite.com/2018/01/03/[hadoop]hdfs_other_cmd/</id>
    <published>2018-01-02T16:00:00.000Z</published>
    <updated>2020-07-22T08:32:27.248Z</updated>
    
    <content type="html"><![CDATA[<h5 id="hdfs-getconf相關用法"><a href="#hdfs-getconf相關用法" class="headerlink" title="hdfs getconf相關用法"></a>hdfs getconf相關用法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">Usage:</div><div class="line">hadoop getconf</div><div class="line">        [-namenodes]            gets list of namenodes in the cluster.</div><div class="line">        [-secondaryNameNodes]   gets list of secondary namenodes in the cluster.</div><div class="line">        [-backupNodes]          gets list of backup nodes in the cluster.</div><div class="line">        [-includeFile]          gets the include file path that defines the datanodes that can join the cluster.</div><div class="line">        [-excludeFile]          gets the exclude file path that defines the datanodes that need to decommissioned.</div><div class="line">        [-nnRpcAddresses]       gets the namenode rpc addresses</div><div class="line">        [-confKey [key]]		gets a specific key from the configuration</div><div class="line">---</div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -namenodes</div><div class="line">hadoop-master</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -secondaryNameNodes</div><div class="line">hadoop-master</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -backupNodes</div><div class="line">0.0.0.0</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -nnRpcAddresses</div><div class="line">hadoop-master:8020</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -includeFile</div><div class="line">Configuration dfs.hosts is missing.</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -excludeFile</div><div class="line">Configuration dfs.hosts.exclude is missing.</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -confKey &quot;fs.trash.interval&quot;</div><div class="line">1440</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hdfs-envvars相關用法"><a href="#hdfs-envvars相關用法" class="headerlink" title="hdfs envvars相關用法"></a>hdfs envvars相關用法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs envvars</div><div class="line">JAVA_HOME=&apos;/bgdt/java/jdk1.8.0_101&apos;</div><div class="line">HADOOP_HDFS_HOME=&apos;/bgdt/hadoop-3.0.0&apos;</div><div class="line">HDFS_DIR=&apos;share/hadoop/hdfs&apos;</div><div class="line">HDFS_LIB_JARS_DIR=&apos;share/hadoop/hdfs/lib&apos;</div><div class="line">HADOOP_CONF_DIR=&apos;/bgdt/hadoop-3.0.0/etc/hadoop&apos;</div><div class="line">HADOOP_TOOLS_HOME=&apos;/bgdt/hadoop-3.0.0&apos;</div><div class="line">HADOOP_TOOLS_DIR=&apos;share/hadoop/tools&apos;</div><div class="line">HADOOP_TOOLS_LIB_JARS_DIR=&apos;share/hadoop/tools/lib&apos;</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hdfs-classpath相關用法"><a href="#hdfs-classpath相關用法" class="headerlink" title="hdfs classpath相關用法"></a>hdfs classpath相關用法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs classpath</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop:/bgdt/hadoop-3.0.0/share/hadoop/common/lib/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/common/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/hdfs:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/hdfs/lib/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/hdfs/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/mapreduce/lib/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/mapreduce/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/yarn:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/yarn/lib/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/yarn/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hdfs-version相關用法"><a href="#hdfs-version相關用法" class="headerlink" title="hdfs version相關用法"></a>hdfs version相關用法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs version</div><div class="line">Hadoop 3.0.0</div><div class="line">Source code repository https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533</div><div class="line">Compiled by andrew on 2017-12-08T19:16Z</div><div class="line">Compiled with protoc 2.5.0</div><div class="line">From source with checksum 397832cb5529187dc8cd74ad54ff22</div><div class="line">This command was run using /bgdt/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hdfs-groups相關用法"><a href="#hdfs-groups相關用法" class="headerlink" title="hdfs groups相關用法"></a>hdfs groups相關用法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs groups</div><div class="line">hadoop : hadoop</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hadoop-conftest"><a href="#hadoop-conftest" class="headerlink" title="hadoop conftest"></a>hadoop conftest</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop conftest</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/hadoop-policy.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/yarn-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/hdfs-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/core-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/mapred-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/capacity-scheduler.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/kms-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/httpfs-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/kms-acls.xml: valid</div><div class="line">OK</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hadoop-jnipath"><a href="#hadoop-jnipath" class="headerlink" title="hadoop jnipath"></a>hadoop jnipath</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop jnipath</div><div class="line">/bgdt/hadoop-3.0.0/lib/native</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hadoop-checknative"><a href="#hadoop-checknative" class="headerlink" title="hadoop checknative"></a>hadoop checknative</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop checknative</div><div class="line">2018-01-04 17:18:04,659 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native</div><div class="line">2018-01-04 17:18:04,667 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</div><div class="line">2018-01-04 17:18:04,682 ERROR snappy.SnappyCompressor: failed to load SnappyCompressor</div><div class="line">java.lang.UnsatisfiedLinkError: Cannot load libsnappy.so.1 (libsnappy.so.1: cannot open shared object file: No such file or directory)!</div><div class="line">        at org.apache.hadoop.io.compress.snappy.SnappyCompressor.initIDs(Native Method)</div><div class="line">        at org.apache.hadoop.io.compress.snappy.SnappyCompressor.&lt;clinit&gt;(SnappyCompressor.java:57)</div><div class="line">        at org.apache.hadoop.io.compress.SnappyCodec.isNativeCodeLoaded(SnappyCodec.java:82)</div><div class="line">        at org.apache.hadoop.util.NativeLibraryChecker.main(NativeLibraryChecker.java:100)</div><div class="line">2018-01-04 17:18:04,691 WARN erasurecode.ErasureCodeNative: ISA-L support is not available in your platform... using builtin-java codec where applicable</div><div class="line">Native library checking:</div><div class="line">hadoop:  true /bgdt/hadoop-3.0.0/lib/native/libhadoop.so.1.0.0</div><div class="line">zlib:    true /lib/x86_64-linux-gnu/libz.so.1</div><div class="line">zstd  :  false</div><div class="line">snappy:  false</div><div class="line">lz4:     true revision:10301</div><div class="line">bzip2:   true /lib/x86_64-linux-gnu/libbz2.so.1</div><div class="line">openssl: false Cannot load libcrypto.so (libcrypto.so: cannot open shared object file: No such file or directory)!</div><div class="line">ISA-L:   false libhadoop was built without ISA-L support</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hadoop-daemonlog"><a href="#hadoop-daemonlog" class="headerlink" title="hadoop daemonlog"></a>hadoop daemonlog</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">##取得Class Log Level級別</div><div class="line">hadoop@hadoop-master:~$ hadoop daemonlog -getlevel hadoop-master:9870 org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Connecting to http://hadoop-master:9870/logLevel?log=org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Submitted Class Name: org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Log Class: org.apache.commons.logging.impl.Log4JLogger</div><div class="line">Effective Level: INFO</div><div class="line"></div><div class="line">##設定Class Log Level級別</div><div class="line">hadoop@hadoop-master:~$ hadoop daemonlog -setlevel  hadoop-master:9870 org.apache.hadoop.yarn.server.nodemanager.NodeManager DEBUG</div><div class="line">Connecting to http://hadoop-master:9870/logLevel?log=org.apache.hadoop.yarn.server.nodemanager.NodeManager&amp;level=DEBUG</div><div class="line">Submitted Class Name: org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Log Class: org.apache.commons.logging.impl.Log4JLogger</div><div class="line">Submitted Level: DEBUG</div><div class="line">Setting Level to DEBUG ...</div><div class="line">Effective Level: DEBUG</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop daemonlog -getlevel hadoop-master:9870 org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Connecting to http://hadoop-master:9870/logLevel?log=org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Submitted Class Name: org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Log Class: org.apache.commons.logging.impl.Log4JLogger</div><div class="line">Effective Level: DEBUG</div></pre></td></tr></table></figure></blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;hdfs-getconf相關用法&quot;&gt;&lt;a href=&quot;#hdfs-getconf相關用法&quot; class=&quot;headerlink&quot; title=&quot;hdfs getconf相關用法&quot;&gt;&lt;/a&gt;hdfs getconf相關用法&lt;/h5&gt;&lt;blockquote&gt;
&lt;fig
    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/BigData/Hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hadoop fs command Memo</title>
    <link href="http://yoursite.com/2018/01/02/%5Bhadoop%5Dhadoop_fs_cmd/"/>
    <id>http://yoursite.com/2018/01/02/[hadoop]hadoop_fs_cmd/</id>
    <published>2018-01-01T16:00:00.000Z</published>
    <updated>2020-07-22T08:27:28.729Z</updated>
    
    <content type="html"><![CDATA[<h4 id="建立與刪除目錄"><a href="#建立與刪除目錄" class="headerlink" title="建立與刪除目錄"></a>建立與刪除目錄</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">##建立目錄但不會將上層目錄一併建立完成</div><div class="line">hadoop@hadoop-master:~$hadoop fs -mkdir /user/hadoop/example/lab_1</div><div class="line">mkdir: `/user/hadoop/example/lab_1&apos;: No such file or directory</div><div class="line"></div><div class="line">##建立目錄,會連同上層目錄一併建立</div><div class="line">hadoop@hadoop-master:~$hadoop fs -mkdir -p /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$hadoop fs ls -d /user/hadoop/example</div><div class="line"></div><div class="line">##刪除一個空目錄</div><div class="line">hadoop@hadoop-master:~$hadoop fs -rmdir /user/hadoop/example/lab_1</div><div class="line"></div><div class="line">##透過std in 建立一個test1.txt檔案</div><div class="line">hadoop@hadoop-master:~$hadoop fs -put -f - /user/hadoop/example/test1.txt</div><div class="line"></div><div class="line">##無法刪除,目錄下有檔案</div><div class="line">hadoop@hadoop-master:~$hadoop fs -rmdir  /user/hadoop/example</div><div class="line">&quot;rmdir: `/user/hadoop/example&apos;: Directory is not empty&quot;</div><div class="line"></div><div class="line">##將所有檔案刪除完成後,刪除目錄</div><div class="line">hadoop@hadoop-master:~$hadoop fs -rm -r -f  /user/hadoop/example			</div><div class="line"></div><div class="line">##建立目錄</div><div class="line">hadoop@hadoop-master:~$hadoop fs -mkdir -p /user/hadoop/example/lab_1</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="檔案建立-複製-刪除-搬移"><a href="#檔案建立-複製-刪除-搬移" class="headerlink" title="檔案建立/複製/刪除/搬移"></a>檔案建立/複製/刪除/搬移</h4><h5 id="put-get操作檔案"><a href="#put-get操作檔案" class="headerlink" title="put/get操作檔案"></a>put/get操作檔案</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$rm -rf  ~/get_hdfs_test2.txt</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$dd if=/dev/urandom of=test2.txt bs=1K count=1</div><div class="line">1+0 records in</div><div class="line">1+0 records out</div><div class="line">1024 bytes (1.0 kB, 1.0 KiB) copied, 0.00234966 s, 436 kB/s</div><div class="line"></div><div class="line">##使用PUT上傳檔案到HDFS</div><div class="line">hadoop@hadoop-master:~$hadoop fs -put ~/test2.txt /user/hadoop/example/</div><div class="line">hadoop@hadoop-master:~$hadoop fs -ls /user/hadoop/example</div><div class="line">Found 2 items</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 15:40 /user/hadoop/example/lab_1</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/test2.txt</div><div class="line"></div><div class="line">##使用GET下載檔案到Local</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -get /user/hadoop/example/test2.txt ~/get_hdfs_test2.txt</div><div class="line">hadoop@hadoop-master:~$ ls -la ~/get*</div><div class="line">-rw-r--r-- 1 hadoop hadoop 1024 Jan  2 15:44 /home/hadoop/get_hdfs_test2.txt</div><div class="line"></div><div class="line"></div><div class="line">##使用getmerge結合數個檔案內容,並下載至Local</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/lab_1/test1.txt</div><div class="line">1,&quot;test_001&quot;,10000</div><div class="line">2,&quot;test_002&quot;,20000</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/lab_1/test2.txt</div><div class="line">3,&quot;test_003&quot;,30000</div><div class="line">4,&quot;test_004&quot;,40000</div><div class="line">hadoop@hadoop-master:~$ rm -rf ~/merge_file.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -getmerge -nl /user/hadoop/example/lab_1/* ~/merge_file.txt</div><div class="line">hadoop@hadoop-master:~$ ls -la ~/merge_file.txt</div><div class="line">-rw-r--r-- 1 hadoop hadoop 78 Jan  2 15:53 /home/hadoop/merge_file.txt</div><div class="line">hadoop@hadoop-master:~$ cat ~/merge_file.txt</div><div class="line">1,&quot;test_001&quot;,10000</div><div class="line">2,&quot;test_002&quot;,20000</div><div class="line"></div><div class="line">3,&quot;test_003&quot;,30000</div><div class="line">4,&quot;test_004&quot;,40000</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="copy-move操作檔案"><a href="#copy-move操作檔案" class="headerlink" title="copy/move操作檔案"></a>copy/move操作檔案</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div></pre></td><td class="code"><pre><div class="line">## copyfromlocal/copytolocal操作</div><div class="line">hadoop@hadoop-master:~$ dd if=/dev/urandom of=test3.txt bs=1K count=1</div><div class="line">1+0 records in</div><div class="line">1+0 records out</div><div class="line">1024 bytes (1.0 kB, 1.0 KiB) copied, 0.00109903 s, 932 kB/s</div><div class="line">hadoop@hadoop-master:~$ rm -rf ~/cp_hdfs_tolocal_test3.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -copyFromLocal ~/test3.txt /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 3 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -copyToLocal  /user/hadoop/example/lab_1/test3.txt ~/cp_hdfs_tolocal_test3.txt</div><div class="line">hadoop@hadoop-master:~$ ls -la ~/cp_hdfs_tolocal_test3.txt</div><div class="line">-rw-r--r-- 1 hadoop hadoop 1024 Jan  2 16:01 /home/hadoop/cp_hdfs_tolocal_test3.txt</div><div class="line">hadoop@hadoop-master:~$</div><div class="line"></div><div class="line">## cp操作(同一個HDFS中資料複製)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 3 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cp /user/hadoop/example/test2.txt  /user/hadoop/example/lab_1/test4.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 4 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example</div><div class="line">Found 2 items</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 16:14 /user/hadoop/example/lab_1</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/test2.txt</div><div class="line"></div><div class="line">## cp操作(不同HDFS中資料複製)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cp hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test5.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 5 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test5.txt</div><div class="line">id,end_date,start_date,location</div><div class="line">1,2015-10-14 00:00:00,2015-09-14 00:00:00,CA-SF</div><div class="line">2,2015-10-15 01:00:20,2015-08-14 00:00:00,CA-SD</div><div class="line">3,2015-10-16 02:30:00,2015-01-14 00:00:00,NY-NY</div><div class="line">4,2015-10-17 03:00:20,2015-02-14 00:00:00,NY-NY</div><div class="line">5,2015-10-18 04:30:00,2014-04-14 00:00:00,CA-SD</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls hdfs://172.20.22.95:8020/user/hadoop/tmp</div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2017-11-22 17:09 hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv</div><div class="line"></div><div class="line">## moveFromLocal/moveToLocal</div><div class="line">hadoop@hadoop-master:~$ dd if=/dev/urandom of=test6.txt bs=1K count=1</div><div class="line">1+0 records in</div><div class="line">1+0 records out</div><div class="line">1024 bytes (1.0 kB, 1.0 KiB) copied, 0.00116886 s, 876 kB/s</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -moveFromLocal ~/test6.txt /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 6 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -moveToLocal /user/hadoop/example/test6.txt ~/mv_hdfs_tolocal_test6.txt</div><div class="line">moveToLocal: Option &apos;-moveToLocal&apos; is not implemented yet.</div><div class="line"></div><div class="line">## mv操作(同一個HDFS中資料複製),被搬移之後檔案會不見</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 6 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mv /user/hadoop/example/test2.txt /user/hadoop/example/lab_1/test7.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 7 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example</div><div class="line">Found 1 items</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 16:21 /user/hadoop/example/lab_1</div><div class="line"></div><div class="line">## mv操作(不同一個HDFS中資料複製),如果hadoop版本不一致無法使用mv搬移檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls hdfs://172.20.22.95:8020/user/hadoop/tmp</div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2017-11-22 17:09 hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls hdfs://192.168.51.4:8020/user/hadoop/example/lab_1</div><div class="line">Found 7 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test7.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mv hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test8.txt</div><div class="line">mv: `hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv&apos;: Does not match target filesystem</div><div class="line">hadoop@hadoop-master:~$  hadoop fs -cp hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv  hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test8.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls hdfs://192.168.51.4:8020/user/hadoop/example/lab_1</div><div class="line">Found 8 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test8.txt</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="查看檔案內容"><a href="#查看檔案內容" class="headerlink" title="查看檔案內容"></a>查看檔案內容</h4><h5 id="檢視壓縮檔案內容-tar-gz"><a href="#檢視壓縮檔案內容-tar-gz" class="headerlink" title="檢視壓縮檔案內容(.tar.gz)"></a>檢視壓縮檔案內容(.tar.gz)</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ tar -zcvf ~/test9.tar.gz &quot;test1.csv&quot; &quot;test2.csv&quot; &quot;test3.csv&quot;</div><div class="line">test1.csv</div><div class="line">test2.csv</div><div class="line">test3.csv</div><div class="line">hadoop@hadoop-master:~$ tar -ztvf ~/test9.tar.gz</div><div class="line">-rw-rw-r-- hadoop/hadoop   272 2017-11-08 15:49 test1.csv</div><div class="line">-rw-rw-r-- hadoop/hadoop   300 2017-11-06 19:14 test2.csv</div><div class="line">-rw-rw-r-- hadoop/hadoop   422 2017-08-21 16:30 test3.csv</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put ~/test9.tar.gz /user/hadoop/example/lab_1/</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 9 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">##可以使用hadoop fs -text來查看壓縮檔案內容</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -text /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line">test1.csv0000664000175100017510000000042013200533434012136 0ustar  hadoophadoop100,1,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.1&quot;</div><div class="line">200,2,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.2&quot;</div><div class="line">150,3,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.3&quot;</div><div class="line">300,4,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.4&quot;</div><div class="line">..........</div><div class="line">5,&quot;http://www.google.com&quot;;&quot;192.168.1.5&quot;;&quot;201&quot;,&quot;005&quot;;&quot;test1_005&quot;;&quot;500&quot;</div><div class="line">6,&quot;http://www.google.com&quot;;&quot;192.168.1.6&quot;;&quot;403&quot;,&quot;006&quot;;&quot;test1_006&quot;;&quot;1600&quot;</div><div class="line"></div><div class="line">##hadoop fs -cat查看壓縮檔時,會出現亂碼,需要配合zcat指令來查看</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test9.tar.gz | zcat</div><div class="line">test1.csv0000664000175100017510000000042013200533434012136 0ustar  hadoophadoop100,1,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.1&quot;</div><div class="line">200,2,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.2&quot;</div><div class="line">150,3,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.3&quot;</div><div class="line">300,4,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.4&quot;</div><div class="line">..........</div><div class="line">5,&quot;http://www.google.com&quot;;&quot;192.168.1.5&quot;;&quot;201&quot;,&quot;005&quot;;&quot;test1_005&quot;;&quot;500&quot;</div><div class="line">6,&quot;http://www.google.com&quot;;&quot;192.168.1.6&quot;;&quot;403&quot;,&quot;006&quot;;&quot;test1_006&quot;;&quot;1600&quot;</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="檢視檔案內容-且資料append到檔案時-會馬上呈現"><a href="#檢視檔案內容-且資料append到檔案時-會馬上呈現" class="headerlink" title="檢視檔案內容,且資料append到檔案時,會馬上呈現"></a>檢視檔案內容,且資料append到檔案時,會馬上呈現</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">##hadoop fs -tail指令的功能(與cat/text相同可以觀察檔案內容)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -tail /user/hadoop/example/lab_1/test1.txt</div><div class="line">1,&quot;test_001&quot;,10000</div><div class="line">2,&quot;test_002&quot;,20000</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="以下使用圖片說明"><a href="#以下使用圖片說明" class="headerlink" title="以下使用圖片說明"></a>以下使用圖片說明</h6><blockquote>
<p>hadoop fs -tail時,可以查看檔案完整內容<br><img src="/images/hadoop_fs_cmd/tail_1.jpg" alt=""><br>hadoop fs -tail -f 時,持續呈現檔案Append的相關內容<br><img src="/images/hadoop_fs_cmd/tail_2_1.jpg" alt=""><br><img src="/images/hadoop_fs_cmd/tail_2_2.jpg" alt=""></p>
</blockquote>
<h4 id="建立空檔案vs-清空檔案內容"><a href="#建立空檔案vs-清空檔案內容" class="headerlink" title="建立空檔案vs.清空檔案內容"></a>建立空檔案vs.清空檔案內容</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line">##使用 -touchz建立一個空的檔案,常用於建立時戳或者flag檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 9 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         76 2018-01-02 17:28 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -touchz /user/hadoop/example/lab_1/test10.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 10 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         76 2018-01-02 17:28 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test1.txt</div><div class="line">1,&quot;test_001&quot;,10000</div><div class="line">2,&quot;test_002&quot;,20000</div><div class="line">3,&quot;test_003&quot;,30000</div><div class="line">4,&quot;test_004&quot;,40000</div><div class="line"></div><div class="line">## -truncate -w 10,只留10 bytes資料</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -truncate -w 10 /user/hadoop/example/lab_1/test1.txt</div><div class="line">Waiting for /user/hadoop/example/lab_1/test1.txt ...</div><div class="line">Truncated /user/hadoop/example/lab_1/test1.txt to length: 10</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test1.txt</div><div class="line">1,&quot;test_00</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         10 2018-01-02 17:42 /user/hadoop/example/lab_1/test1.txt</div><div class="line"></div><div class="line">## -truncate -w 0,資料全部清空</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -truncate -w 0 /user/hadoop/example/lab_1/test1.txt</div><div class="line">Truncated /user/hadoop/example/lab_1/test1.txt to length: 0</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test1.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-find找檔案"><a href="#使用hadoop-fs-find找檔案" class="headerlink" title="使用hadoop fs -find找檔案"></a>使用hadoop fs -find找檔案</h4><blockquote>
<p>此部分可以參考Linux find指令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">說明:</div><div class="line">-iname pattern 所要查找的文檔名，不區分大小寫</div><div class="line">-name pattern  所要查找的文檔名，區分大小寫</div><div class="line">-print 換行列印</div><div class="line">-print0 連續列印</div><div class="line"></div><div class="line">##-name後,可接欲找尋的檔案名稱,&quot;*&quot;代表任何值</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -find / -name &quot;test*.txt&quot; -print</div><div class="line">/user/hadoop/example/lab_1/test1.txt</div><div class="line">/user/hadoop/example/lab_1/test10.txt</div><div class="line">/user/hadoop/example/lab_1/test2.txt</div><div class="line">/user/hadoop/example/lab_1/test3.txt</div><div class="line">/user/hadoop/example/lab_1/test4.txt</div><div class="line">/user/hadoop/example/lab_1/test5.txt</div><div class="line">/user/hadoop/example/lab_1/test6.txt</div><div class="line">/user/hadoop/example/lab_1/test7.txt</div><div class="line">/user/hadoop/example/lab_1/test8.txt</div><div class="line">/user/hadoop/test1.txt</div><div class="line">##-name後,可接正規式的相關參數</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -find / -name &quot;[A-Z,a-z]*.db&quot; -print</div><div class="line">/user/hive/warehouse/test1.db</div><div class="line">/user/hive/warehouse/test2.db</div><div class="line">/user/hive/warehouse/test3.db</div><div class="line">/user/hive/warehouse/test4.db</div><div class="line">/user/hive/warehouse/test5.db</div><div class="line">/user/hive/warehouse/test6.db</div></pre></td></tr></table></figure></p>
</blockquote>
<h4 id="使用hadoop-fs-df-du查看檔案目錄的大小"><a href="#使用hadoop-fs-df-du查看檔案目錄的大小" class="headerlink" title="使用hadoop fs -df/-du查看檔案目錄的大小"></a>使用hadoop fs -df/-du查看檔案目錄的大小</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">##後面如果都沒接任何參數,看各個檔案的大小(以byte為單位)及檔名</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -du /user/hadoop/example/lab_1</div><div class="line">0     0     /user/hadoop/example/lab_1/test1.txt</div><div class="line">0     0     /user/hadoop/example/lab_1/test10.txt</div><div class="line">38    76    /user/hadoop/example/lab_1/test2.txt</div><div class="line">1024  2048  /user/hadoop/example/lab_1/test3.txt</div><div class="line">1024  2048  /user/hadoop/example/lab_1/test4.txt</div><div class="line">272   544   /user/hadoop/example/lab_1/test5.txt</div><div class="line">1024  2048  /user/hadoop/example/lab_1/test6.txt</div><div class="line">1024  2048  /user/hadoop/example/lab_1/test7.txt</div><div class="line">272   544   /user/hadoop/example/lab_1/test8.txt</div><div class="line">373   746   /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">##加入&quot;-h&quot;,在顯示檔案大小時,會將單位標示出來</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -du -h /user/hadoop/example/lab_1</div><div class="line">0    0    /user/hadoop/example/lab_1/test1.txt</div><div class="line">0    0    /user/hadoop/example/lab_1/test10.txt</div><div class="line">38   76   /user/hadoop/example/lab_1/test2.txt</div><div class="line">1 K  2 K  /user/hadoop/example/lab_1/test3.txt</div><div class="line">1 K  2 K  /user/hadoop/example/lab_1/test4.txt</div><div class="line">272  544  /user/hadoop/example/lab_1/test5.txt</div><div class="line">1 K  2 K  /user/hadoop/example/lab_1/test6.txt</div><div class="line">1 K  2 K  /user/hadoop/example/lab_1/test7.txt</div><div class="line">272  544  /user/hadoop/example/lab_1/test8.txt</div><div class="line">373  746  /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">##加入&quot;-s&quot;,最主要是用來顯示該目錄下所有檔案的summary後的大小</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -du -h -s /user/hadoop/example/lab_1</div><div class="line">4.9 K  9.9 K  /user/hadoop/example/lab_1</div><div class="line"></div><div class="line">##hadoop fs -du 加入&quot;-v&quot;,可以看到每個欄位的標題</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -du -h -v  /user/hadoop/example/lab_1</div><div class="line">SIZE  DISK_SPACE_CONSUMED_WITH_ALL_REPLICAS  FULL_PATH_NAME</div><div class="line">0     0                                      /user/hadoop/example/lab_1/test1.txt</div><div class="line">0     0                                      /user/hadoop/example/lab_1/test10.txt</div><div class="line">38    76                                     /user/hadoop/example/lab_1/test2.txt</div><div class="line">1 K   2 K                                    /user/hadoop/example/lab_1/test3.txt</div><div class="line">1 K   2 K                                    /user/hadoop/example/lab_1/test4.txt</div><div class="line">272   544                                    /user/hadoop/example/lab_1/test5.txt</div><div class="line">1 K   2 K                                    /user/hadoop/example/lab_1/test6.txt</div><div class="line">1 K   2 K                                    /user/hadoop/example/lab_1/test7.txt</div><div class="line">272   544                                    /user/hadoop/example/lab_1/test8.txt</div><div class="line">373   746                                    /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">## hadoop fs -df 顯示HDFS系統(多個)剩餘空間</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -df hdfs://172.20.22.95:8020 hdfs://192.168.51.4:8020</div><div class="line">Filesystem                         Size         Used      Available  Use%</div><div class="line">hdfs://172.20.22.95:8020  2232839094272  29245636608  1911548899328    1%</div><div class="line">hdfs://192.168.51.4:8020    42002972672     47734784    26333745152    0%</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-ls查看檔案目錄相關資訊"><a href="#使用hadoop-fs-ls查看檔案目錄相關資訊" class="headerlink" title="使用hadoop fs -ls查看檔案目錄相關資訊"></a>使用hadoop fs -ls查看檔案目錄相關資訊</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">##</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example</div><div class="line">Found 2 items</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1</div><div class="line">-rw-r--r--   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">##-r  Reverse the order of the sort</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -r /user/hadoop/example</div><div class="line">Found 2 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1</div><div class="line"></div><div class="line">## 檢視&quot;-R&quot;遞回目錄下所有檔案及目錄</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R /user/hadoop/example</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line">-rw-r--r--   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">## &quot;-d&quot;只顯示目前只定的檔案或目錄</div><div class="line">hadoop fs -ls -d /user/hadoop/example</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 18:29 /user/hadoop/example</div><div class="line"></div><div class="line">## &quot;-e&quot; Display the erasure coding policy of files and directories.</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -e /user/hadoop/example/lab_1</div><div class="line">Found 10 items</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-count查看檔案目錄相關資訊"><a href="#使用hadoop-fs-count查看檔案目錄相關資訊" class="headerlink" title="使用hadoop fs -count查看檔案目錄相關資訊"></a>使用hadoop fs -count查看檔案目錄相關資訊</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">##計算指定的目錄或檔案的數量大小</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count &quot;/user/hadoop/example/*&quot;</div><div class="line">           1           10               5051 /user/hadoop/example/lab_1</div><div class="line">           0            1                 36 /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">## &quot;-q&quot;選項會多出以下欄位</div><div class="line">## QUOTA,REM_QUOTA,SPACE_QUOTA,REM_SPACE_QUOTA,DIR_COUNT,FILE_COUNT,CONTENT_SIZE,PATHNAME	   </div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -q &quot;/user/hadoop/example/*&quot;</div><div class="line">        none             inf            none             inf            1           10               5051 /user/hadoop/example/lab_1</div><div class="line">        none             inf            none             inf            0            1                 36 /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">## &quot;-u&quot; option shows the quota and the usage against the quota without the detailed content summary</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -u &quot;/user/hadoop/example/*&quot;</div><div class="line">        none             inf            none             inf /user/hadoop/example/lab_1</div><div class="line">        none             inf            none             inf /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">## &quot;-e&quot; option shows the erasure coding policy.</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -e &quot;/user/hadoop/example/*&quot;</div><div class="line">           1           10               5051 EC: /user/hadoop/example/lab_1</div><div class="line">           0            1                 36 Replicated /user/hadoop/example/test.txt</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-stat查看檔案目錄相關資訊"><a href="#使用hadoop-fs-stat查看檔案目錄相關資訊" class="headerlink" title="使用hadoop fs -stat查看檔案目錄相關資訊"></a>使用hadoop fs -stat查看檔案目錄相關資訊</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">##相關-stat Format說明</div><div class="line">%a: octal</div><div class="line">%A: symbolic</div><div class="line">%b: filesize in bytes</div><div class="line">%F: type</div><div class="line">%g: group name of owner</div><div class="line">%n: name</div><div class="line">%o: block size</div><div class="line">%r: replication</div><div class="line">%u: user name of owner</div><div class="line">%x,%X :access date</div><div class="line">%y,%Y :modification date</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -stat /user/*</div><div class="line">2018-01-02 09:05:22</div><div class="line">2017-12-26 10:44:41</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -stat &quot;%A,perm:%a,type:%F,%u:%g,size:%b,block size:%o,%r,mtime:%y atime:%x name:%n&quot; &quot;/user/hadoop/example/lab_1/*&quot;</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:0,block size:67108864,2,mtime:2018-01-02 09:45:22 atime:2018-01-02 09:18:30 name:test1.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:0,block size:67108864,2,mtime:2018-01-02 09:38:21 atime:2018-01-02 09:38:21 name:test10.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:38,block size:67108864,2,mtime:2018-01-02 07:51:41 atime:2018-01-02 07:51:16 name:test2.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:1024,block size:67108864,2,mtime:2018-01-02 07:59:28 atime:2018-01-02 07:59:28 name:test3.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:1024,block size:67108864,2,mtime:2018-01-02 08:07:12 atime:2018-01-02 08:07:12 name:test4.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:272,block size:67108864,2,mtime:2018-01-02 08:11:21 atime:2018-01-02 08:11:21 name:test5.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:1024,block size:67108864,2,mtime:2018-01-02 08:14:24 atime:2018-01-02 08:14:23 name:test6.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:1024,block size:67108864,2,mtime:2018-01-02 07:40:46 atime:2018-01-02 07:40:46 name:test7.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:272,block size:67108864,2,mtime:2018-01-02 08:31:28 atime:2018-01-02 08:31:28 name:test8.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:373,block size:67108864,2,mtime:2018-01-02 09:05:38 atime:2018-01-02 09:05:37 name:test9.tar.gz</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-test檔案測試"><a href="#使用hadoop-fs-test檔案測試" class="headerlink" title="使用hadoop fs -test檔案測試"></a>使用hadoop fs -test檔案測試</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line">##相關參數說明:</div><div class="line">-d  return 0 if &lt;path&gt; is a directory.</div><div class="line">-e  return 0 if &lt;path&gt; exists.</div><div class="line">-f  return 0 if &lt;path&gt; is a file.</div><div class="line">-s  return 0 if file &lt;path&gt; is greater than zero bytes in size.</div><div class="line">-w  return 0 if file &lt;path&gt; exists and write permission is granted.</div><div class="line">-r  return 0 if file &lt;path&gt; exists and read permission is granted.</div><div class="line">-z  return 0 if file &lt;path&gt; is zero bytes in size, else return 1.</div><div class="line">---</div><div class="line"></div><div class="line">## &quot;-e&quot; 判斷指定的路徑是否存在(0:是,1:否)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -e /user/hadoop</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -e /user/vagrant</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line"></div><div class="line">## &quot;-d&quot; 判斷是否為目錄(0:是,1:否)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -d /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -d /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line"></div><div class="line"></div><div class="line">## &quot;-f&quot; 判斷是否為目錄(0:是,1:否)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -f /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -f /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line"></div><div class="line">## &quot;-w&quot;判斷檔案是否有寫的權限</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/test.txt</div><div class="line">-rwxr-xr-x   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -w  /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line">hadoop@hadoop-master:~$ su - popal</div><div class="line">Password:</div><div class="line">popal@hadoop-master:~$ /bgdt/hadoop-3.0.0/bin/hadoop fs -test -w  /user/hadoop/example/test.txt</div><div class="line">popal@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -chmod 766 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/test.txt</div><div class="line">-rwxrw-rw-   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ su - popal</div><div class="line">Password:</div><div class="line">popal@hadoop-master:~$ /bgdt/hadoop-3.0.0/bin/hadoop fs -test -w /user/hadoop/example/test.txt</div><div class="line">popal@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line"></div><div class="line">## &quot;-r&quot;判斷檔案是否有讀的權限</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/test.txt</div><div class="line">-rwxrw-rw-   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ su - popal</div><div class="line">Password:</div><div class="line">popal@hadoop-master:~$ /bgdt/hadoop-3.0.0/bin/hadoop fs -test -r /user/hadoop/example/test.txt</div><div class="line">popal@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line">popal@hadoop-master:~$ exit</div><div class="line">logout</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -chmod 700 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/test.txt</div><div class="line">-rwx------   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ su - popal</div><div class="line">Password:</div><div class="line">popal@hadoop-master:~$ /bgdt/hadoop-3.0.0/bin/hadoop fs -test -r /user/hadoop/example/test.txt</div><div class="line">popal@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line"></div><div class="line">## &quot;-z&quot;判斷檔案是否為0 bytes</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 10 items</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -z /user/hadoop/example/lab_1/test1.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -z /user/hadoop/example/lab_1/test2.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line"></div><div class="line">## &quot;-s&quot;判斷檔案是否不為0 bytes</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -s /user/hadoop/example/lab_1/test1.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -s /user/hadoop/example/lab_1/test2.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-setrep-設定檔案複本數"><a href="#使用hadoop-fs-setrep-設定檔案複本數" class="headerlink" title="使用hadoop fs -setrep 設定檔案複本數"></a>使用hadoop fs -setrep 設定檔案複本數</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 10 items</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -setrep -w 1 /user/hadoop/example/lab_1/test1.txt</div><div class="line">Replication 1 set: /user/hadoop/example/lab_1/test1.txt</div><div class="line">Waiting for /user/hadoop/example/lab_1/test1.txt ... done</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 10 items</div><div class="line">-rw-r--r--   1 hadoop supergroup          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-rm刪除檔案"><a href="#使用hadoop-fs-rm刪除檔案" class="headerlink" title="使用hadoop fs -rm刪除檔案"></a>使用hadoop fs -rm刪除檔案</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line">## hadoop fs -rm指令,只能刪檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rm  /user/hadoop/example/lab_1/*</div><div class="line">Deleted /user/hadoop/example/lab_1/test1.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test10.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test2.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test3.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test4.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test5.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test6.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test7.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test8.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">## &quot;-rm&quot;無法刪目錄,如要刪除目錄需要用&quot;-rmdir or -rm -r&quot;</div><div class="line">hadoop fs -rm  /user/hadoop/example</div><div class="line">rm: `/user/hadoop/example&apos;: Is a directory</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rmdir  /user/hadoop/example</div><div class="line">rmdir: `/user/hadoop/example&apos;: Directory is not empty</div><div class="line"></div><div class="line">## &quot;-rmdir&quot;只能刪除空目錄</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rmdir /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">ls: `/user/hadoop/example/lab_1&apos;: No such file or directory</div><div class="line"></div><div class="line">## &quot;-rm -r&quot;,可以刪除所有檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -touchz /user/hadoop/example/lab_1/test1.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rmdir /user/hadoop/example/lab_1</div><div class="line">rmdir: `/user/hadoop/example/lab_1&apos;: Directory is not empty</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rm /user/hadoop/example/lab_1</div><div class="line">rm: `/user/hadoop/example/lab_1&apos;: Is a directory</div><div class="line"></div><div class="line">## &quot;-rm -r&quot;,會將刪除的檔案或目錄先丟到垃圾桶</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rm -r /user/hadoop/example/lab_1</div><div class="line">2018-01-03 11:38:37,066 INFO fs.TrashPolicyDefault: </div><div class="line">Moved: &apos;hdfs://hadoop-master:8020/user/hadoop/example/lab_1&apos; to trash </div><div class="line">at: hdfs://hadoop-master:8020/user/hadoop/.Trash/Current/user/hadoop/example/lab_1</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example</div><div class="line">Found 1 items</div><div class="line">-rwx------   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">##檢查垃圾桶</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R /user/hadoop/.Trash</div><div class="line">drwx------   - hadoop supergroup          0 2018-01-03 11:38 /user/hadoop/.Trash/Current</div><div class="line">drwx------   - hadoop supergroup          0 2018-01-03 11:38 /user/hadoop/.Trash/Current/user</div><div class="line">drwx------   - hadoop supergroup          0 2018-01-03 11:38 /user/hadoop/.Trash/Current/user/hadoop</div><div class="line">drwx------   - hadoop supergroup          0 2018-01-03 11:38 /user/hadoop/.Trash/Current/user/hadoop/example</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-03 11:37 /user/hadoop/.Trash/Current/user/hadoop/example/lab_1</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-03 11:37 /user/hadoop/.Trash/Current/user/hadoop/example/lab_1/test1.txt</div><div class="line"></div><div class="line">## &quot;-rm -r -skipTrash&quot;,會直接刪除目錄或檔案,並且不會進垃圾桶</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rm -r -skipTrash /user/hadoop/example/lab_1</div><div class="line">Deleted /user/hadoop/example/lab_1</div></pre></td></tr></table></figure>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;建立與刪除目錄&quot;&gt;&lt;a href=&quot;#建立與刪除目錄&quot; class=&quot;headerlink&quot; title=&quot;建立與刪除目錄&quot;&gt;&lt;/a&gt;建立與刪除目錄&lt;/h4&gt;&lt;blockquote&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;
    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/BigData/Hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>Hadoop權限操作說明</title>
    <link href="http://yoursite.com/2018/01/02/%5Bhadoop%5DHadoop_permission_memo/"/>
    <id>http://yoursite.com/2018/01/02/[hadoop]Hadoop_permission_memo/</id>
    <published>2018-01-01T16:00:00.000Z</published>
    <updated>2020-07-22T08:28:09.538Z</updated>
    
    <content type="html"><![CDATA[<h4 id="使用到的HDFS語法整理"><a href="#使用到的HDFS語法整理" class="headerlink" title="使用到的HDFS語法整理"></a>使用到的HDFS語法整理</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hadoop fs -ls -d /user/hadoop</div><div class="line">hadoop fs -put ~/test1.txt /user/hadoop</div><div class="line">hadoop fs -chmod -R 757 /user/hadoop</div><div class="line">hadoop fs -ls  /user/hadoop</div><div class="line">hadoop fs -chown hadoop /user/hadoop/test1.txt</div><div class="line">hadoop fs -appendToFile - /user/hadoop/test1.txt</div><div class="line">hadoop fs -cat /user/hadoop/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="以下為操作驗證"><a href="#以下為操作驗證" class="headerlink" title="以下為操作驗證"></a>以下為操作驗證</h4><h5 id="使用hadoop語法查看目錄權限"><a href="#使用hadoop語法查看目錄權限" class="headerlink" title="使用hadoop語法查看目錄權限"></a>使用hadoop語法查看目錄權限</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$hadoop fs -ls -d /user/hadoop</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2017-12-27 18:39 /user/hadoop</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="使用putty-以hadoop帳號-登入-hadoop-master主機建立一個新帳號"><a href="#使用putty-以hadoop帳號-登入-hadoop-master主機建立一個新帳號" class="headerlink" title="使用putty(以hadoop帳號)登入,hadoop-master主機建立一個新帳號"></a>使用putty(以hadoop帳號)登入,hadoop-master主機建立一個新帳號</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$sudo useradd -m popal</div><div class="line">hadoop@hadoop-master:~$ls -la /home</div><div class="line">hadoop@hadoop-master:~$sudo passwd popal</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="建立一個文字檔"><a href="#建立一個文字檔" class="headerlink" title="建立一個文字檔"></a>建立一個文字檔</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$su - popal</div><div class="line">popal@hadoop-master:~$vi ~/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="使用hadoop語法-上傳至HDFS時-出現錯誤訊息"><a href="#使用hadoop語法-上傳至HDFS時-出現錯誤訊息" class="headerlink" title="使用hadoop語法,上傳至HDFS時,出現錯誤訊息"></a>使用hadoop語法,上傳至HDFS時,出現錯誤訊息</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -put ~/test1.txt /user/hadoop</div><div class="line">Error:&quot;put: Permission denied: user=popal, access=WRITE, inode=&quot;/user/hadoop&quot;:hadoop:supergroup:drwxr-xr-x&quot;</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="將-user-hadoop權限變更"><a href="#將-user-hadoop權限變更" class="headerlink" title="將/user/hadoop權限變更"></a>將/user/hadoop權限變更</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">popal@hadoop-master:~$su - hadoop</div><div class="line">hadoop@hadoop-master:~$hadoop fs -chmod -R 757 /user/hadoop</div><div class="line">hadoop@hadoop-master:~$hadoop fs -ls -d /user/hadoop</div><div class="line">drwxr-xrwx   - hadoop supergroup          0 2017-12-27 18:39 /user/hadoop</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="使用popal帳號再傳送一次檔案"><a href="#使用popal帳號再傳送一次檔案" class="headerlink" title="使用popal帳號再傳送一次檔案"></a>使用popal帳號再傳送一次檔案</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$su - popal</div><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -put ~/test1.txt /user/hadoop</div><div class="line">(沒有任何錯誤訊息,並傳送成功)</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="觀察檔案是否有上傳至HDFS"><a href="#觀察檔案是否有上傳至HDFS" class="headerlink" title="觀察檔案是否有上傳至HDFS"></a>觀察檔案是否有上傳至HDFS</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -ls  /user/hadoop</div><div class="line">-rw-r--r--   3 popal  supergroup         95 2018-01-02 10:43 /user/hadoop/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="將原本權限復原"><a href="#將原本權限復原" class="headerlink" title="將原本權限復原"></a>將原本權限復原</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">popal@hadoop-master:~$su - hadoop</div><div class="line">hadoop@hadoop-master:~$hadoop fs -chmod -R 755 /user/hadoop</div><div class="line">hadoop@hadoop-master:~$hadoop fs -ls /user/hadoop</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="將test1-txt的擁有者變更成hadoop"><a href="#將test1-txt的擁有者變更成hadoop" class="headerlink" title="將test1.txt的擁有者變更成hadoop"></a>將test1.txt的擁有者變更成hadoop</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$hadoop fs -chown hadoop /user/hadoop/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="使用popal使用者變更test1-txt內容"><a href="#使用popal使用者變更test1-txt內容" class="headerlink" title="使用popal使用者變更test1.txt內容"></a>使用popal使用者變更test1.txt內容</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$su - popal</div><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -appendToFile - /user/hadoop/test1.txt</div><div class="line">Error&quot;appendToFile: Permission denied: user=popal, access=WRITE, inode=&quot;/user/hadoop/test1.txt&quot;:hadoop:supergroup:-rwxr-xr-x&quot;</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="將-user-hadoop-test1-txt的擁有者變更"><a href="#將-user-hadoop-test1-txt的擁有者變更" class="headerlink" title="將/user/hadoop/test1.txt的擁有者變更"></a>將/user/hadoop/test1.txt的擁有者變更</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">popal@hadoop-master:~$su - hadoop</div><div class="line">hadoop@hadoop-master:~$hadoop fs -chown popal /user/hadoop/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="使用popal的帳號再變更-user-hadoop-test1-txt"><a href="#使用popal的帳號再變更-user-hadoop-test1-txt" class="headerlink" title="使用popal的帳號再變更/user/hadoop/test1.txt"></a>使用popal的帳號再變更/user/hadoop/test1.txt</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$su - popal</div><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -appendToFile - /user/hadoop/test1.txt</div><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -cat /user/hadoop/test1.txt</div></pre></td></tr></table></figure></blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;使用到的HDFS語法整理&quot;&gt;&lt;a href=&quot;#使用到的HDFS語法整理&quot; class=&quot;headerlink&quot; title=&quot;使用到的HDFS語法整理&quot;&gt;&lt;/a&gt;使用到的HDFS語法整理&lt;/h4&gt;&lt;blockquote&gt;
&lt;figure class=&quot;highl
    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/BigData/Hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark2.2.1環境安裝說明</title>
    <link href="http://yoursite.com/2017/12/29/%5Bspark%5Dspark2.2.1_Install/"/>
    <id>http://yoursite.com/2017/12/29/[spark]spark2.2.1_Install/</id>
    <published>2017-12-28T16:00:00.000Z</published>
    <updated>2020-07-22T08:36:20.890Z</updated>
    
    <content type="html"><![CDATA[<h3 id="相關環境說明"><a href="#相關環境說明" class="headerlink" title="相關環境說明"></a>相關環境說明</h3><blockquote>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>HostName</th>
<th>角色</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.168.51.4</td>
<td>hadoop-master</td>
<td>NameNode(NN),SecondaryNameNode, HiveServer2</td>
</tr>
<tr>
<td>192.168.51.5</td>
<td>hadoop-slave1</td>
<td>DataNode(DN1)</td>
</tr>
<tr>
<td>192.168.51.6</td>
<td>hadoop-slave2</td>
<td>DataNode(DN2)</td>
</tr>
</tbody>
</table>
</blockquote>
<h3 id="環境準備"><a href="#環境準備" class="headerlink" title="環境準備"></a>環境準備</h3><h4 id="Install-Linux-OS-略"><a href="#Install-Linux-OS-略" class="headerlink" title="Install Linux OS(略)"></a>Install Linux OS(略)</h4><h4 id="Install-Linux-JDK8-略"><a href="#Install-Linux-JDK8-略" class="headerlink" title="Install Linux JDK8(略)"></a>Install Linux JDK8(略)</h4><h4 id="Install-Linux-Hadoop3-0-0-略"><a href="#Install-Linux-Hadoop3-0-0-略" class="headerlink" title="Install Linux Hadoop3.0.0(略)"></a>Install Linux Hadoop3.0.0(略)</h4><h4 id="Install-MariaDB-略"><a href="#Install-MariaDB-略" class="headerlink" title="Install MariaDB(略)"></a>Install MariaDB(略)</h4><h4 id="Install-Hive2-3-略"><a href="#Install-Hive2-3-略" class="headerlink" title="Install Hive2.3(略)"></a>Install Hive2.3(略)</h4><h3 id="Download-Spark2-2-1-tagz-file"><a href="#Download-Spark2-2-1-tagz-file" class="headerlink" title="Download Spark2.2.1 tagz file"></a>Download Spark2.2.1 tagz file</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wget -P /bgdt http://apache.stu.edu.tw/spark/spark-2.2.1/spark-2.2.1-bin-hadoop2.7.tgz</div><div class="line">wget -P /bgdt http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.39/mysql-connector-java-5.1.39.jar</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="解壓縮下載的spark-2-2-1-bin-hadoop2-7-tgz-file-並更改目錄名稱-過長"><a href="#解壓縮下載的spark-2-2-1-bin-hadoop2-7-tgz-file-並更改目錄名稱-過長" class="headerlink" title="解壓縮下載的spark-2.2.1-bin-hadoop2.7.tgz file,並更改目錄名稱(過長)"></a>解壓縮下載的spark-2.2.1-bin-hadoop2.7.tgz file,並更改目錄名稱(過長)</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tar -zxvf /bgdt/spark-2.2.1-bin-hadoop2.7.tgz -C /bgdt</div><div class="line">mv /bgdt/spark-2.2.1-bin-hadoop2.7 /bgdt/spark-2.2.1</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="加入SPARK-HOME相關的環境變數"><a href="#加入SPARK-HOME相關的環境變數" class="headerlink" title="加入SPARK_HOME相關的環境變數"></a>加入SPARK_HOME相關的環境變數</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">vi ~/.bashrc</div><div class="line"></div><div class="line">##Spark Segement</div><div class="line">export SPARK_HOME=/bgdt/spark-2.2.1</div><div class="line">export PATH=$PATH:$SPARK_HOME/sbin:$SPARK_HOME/bin</div><div class="line"></div><div class="line">source ~/.bashrc</div><div class="line"></div><div class="line">cp /bgdt/mysql-connector-java-5.1.39.jar /bgdt/spark-2.2.1/jars</div><div class="line">ln -s /bgdt/hive-2.3.2/conf/hive-site.xml /bgdt/spark-2.2.1/conf/hive-site.xml</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="執行-spark-shell-for-python"><a href="#執行-spark-shell-for-python" class="headerlink" title="執行 spark shell(for python)"></a>執行 spark shell(for python)</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ pyspark</div><div class="line">Python 2.7.13 (default, Jan 19 2017, 14:48:08)</div><div class="line">[GCC 6.3.0 20170118] on linux2</div><div class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</div><div class="line">Setting default log level to &quot;WARN&quot;.</div><div class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</div><div class="line">2017-12-29 11:34:53,515 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">2017-12-29 11:35:06,762 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</div><div class="line">Welcome to</div><div class="line">      ____              __</div><div class="line">     / __/__  ___ _____/ /__</div><div class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</div><div class="line">   /__ / .__/\_,_/_/ /_/\_\   version 2.2.1</div><div class="line">      /_/</div><div class="line"></div><div class="line">Using Python version 2.7.13 (default, Jan 19 2017 14:48:08)</div><div class="line">SparkSession available as &apos;spark&apos;.</div><div class="line">&gt;&gt;&gt; spark</div><div class="line">&lt;pyspark.sql.session.SparkSession object at 0x7f62b8e37610&gt;</div><div class="line">&gt;&gt;&gt; sc</div><div class="line">&lt;SparkContext master=local[*] appName=PySparkShell&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="執行-spark-shell-for-scala"><a href="#執行-spark-shell-for-scala" class="headerlink" title="執行 spark shell(for scala)"></a>執行 spark shell(for scala)</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ spark-shell</div><div class="line">Setting default log level to &quot;WARN&quot;.</div><div class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</div><div class="line">2017-12-29 11:44:58,352 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">Spark context Web UI available at http://192.168.51.4:4040</div><div class="line">Spark context available as &apos;sc&apos; (master = local[*], app id = local-1514519099800).</div><div class="line">Spark session available as &apos;spark&apos;.</div><div class="line">Welcome to</div><div class="line">      ____              __</div><div class="line">     / __/__  ___ _____/ /__</div><div class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</div><div class="line">   /___/ .__/\_,_/_/ /_/\_\   version 2.2.1</div><div class="line">      /_/</div><div class="line"></div><div class="line">Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_101)</div><div class="line">Type in expressions to have them evaluated.</div><div class="line">Type :help for more information.</div><div class="line"></div><div class="line">scala&gt; spark</div><div class="line">res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7afb9c93</div><div class="line"></div><div class="line">scala&gt; sc</div><div class="line">res1: org.apache.spark.SparkContext = org.apache.spark.SparkContext@14292d71</div><div class="line"></div><div class="line">scala&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="執行spark-sql-Shell"><a href="#執行spark-sql-Shell" class="headerlink" title="執行spark-sql Shell"></a>執行spark-sql Shell</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ spark-sql</div><div class="line">2017-12-29 18:04:01,165 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">2017-12-29 18:04:01,307 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</div><div class="line">2017-12-29 18:04:01,344 INFO metastore.ObjectStore: ObjectStore, initialize called</div><div class="line">2017-12-29 18:04:01,642 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored</div><div class="line">2017-12-29 18:04:01,642 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored</div><div class="line">2017-12-29 18:04:05,450 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=&quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order&quot;</div><div class="line">2017-12-29 18:04:07,495 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</div><div class="line">2017-12-29 18:04:07,497 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</div><div class="line">......</div><div class="line">......</div><div class="line">......</div><div class="line">2017-12-29 18:04:20,967 INFO metastore.HiveMetaStore: 0: get_database: global_temp</div><div class="line">2017-12-29 18:04:20,967 INFO HiveMetaStore.audit: ugi=hadoop    ip=unknown-ip-addr      cmd=get_database: global_temp</div><div class="line">2017-12-29 18:04:20,970 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</div><div class="line">2017-12-29 18:04:21,293 INFO session.SessionState: Created local directory: /tmp/d8b65bb8-f4f6-43f2-b544-39331b237673_resources</div><div class="line">2017-12-29 18:04:21,298 INFO session.SessionState: Created HDFS directory: /tmp/hive/hadoop/d8b65bb8-f4f6-43f2-b544-39331b237673</div><div class="line">2017-12-29 18:04:21,306 INFO session.SessionState: Created local directory: /tmp/hadoop/d8b65bb8-f4f6-43f2-b544-39331b237673</div><div class="line">2017-12-29 18:04:21,310 INFO session.SessionState: Created HDFS directory: /tmp/hive/hadoop/d8b65bb8-f4f6-43f2-b544-39331b237673/_tmp_space.db</div><div class="line">2017-12-29 18:04:21,311 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse</div><div class="line">2017-12-29 18:04:21,406 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint</div><div class="line"></div><div class="line">spark-sql&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="Spark2-2-1-amp-Hive2-3-2整合測試"><a href="#Spark2-2-1-amp-Hive2-3-2整合測試" class="headerlink" title="Spark2.2.1 &amp; Hive2.3.2整合測試"></a>Spark2.2.1 &amp; Hive2.3.2整合測試</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">###開啟第1個putty連線</div><div class="line">hadoop@hadoop-master:~$spark-sql</div><div class="line">spark-sql&gt;create database test1;</div><div class="line">..... INFO execution.SparkSqlParser: Parsing command: create database test1</div><div class="line">..... INFO metastore.HiveMetaStore: 0: create_database: Database(name:test1, description:, locationUri:hdfs://hadoop-master:8020/user/hive/warehouse/test1.db, parameters:&#123;&#125;)</div><div class="line">..... INFO HiveMetaStore.audit: ugi=hadoop    ip=unknown-ip-addr      cmd=create_database: Database(name:test1, description:, locationUri:hdfs://hadoop-master:8020/user/hive/warehouse/test1.db, parameters:&#123;&#125;)</div><div class="line">..... WARN metastore.ObjectStore: Failed to get database test1, returning NoSuchObjectException</div><div class="line">..... INFO common.FileUtils: Creating directory if it doesn&apos;t exist: hdfs://hadoop-master:8020/user/hive/warehouse/test1.db</div><div class="line">Time taken: 0.326 seconds</div><div class="line">..... INFO CliDriver: Time taken: 0.326 seconds</div><div class="line"></div><div class="line">spark-sql&gt;show databases;</div><div class="line">INFO execution.SparkSqlParser: Parsing command: show databases</div><div class="line">INFO metastore.HiveMetaStore: 0: get_databases: *</div><div class="line">HiveMetaStore.audit: ugi=hadoop    ip=unknown-ip-addr      cmd=get_databases: *</div><div class="line">INFO codegen.CodeGenerator: Code generated in 544.163987 ms</div><div class="line">default</div><div class="line">test1</div><div class="line"></div><div class="line">###開啟第2個putty連線</div><div class="line">hadoop@hadoop-master:~$hive</div><div class="line">hive&gt;show databases;</div><div class="line">OK</div><div class="line">default</div><div class="line">test1</div><div class="line">......</div><div class="line"></div><div class="line">hive&gt;create database test2;</div><div class="line">OK</div><div class="line">Time taken: 0.341 seconds</div><div class="line"></div><div class="line">hive&gt;show databases;</div><div class="line">OK</div><div class="line">default</div><div class="line">test1</div><div class="line">test2</div><div class="line">......</div><div class="line"></div><div class="line">spark-sql&gt;show databases;</div><div class="line">default</div><div class="line">test1</div><div class="line">test2</div><div class="line"></div><div class="line">###以上驗證由兩種不同方式所建立的資料庫是可以同步的</div></pre></td></tr></table></figure></blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;相關環境說明&quot;&gt;&lt;a href=&quot;#相關環境說明&quot; class=&quot;headerlink&quot; title=&quot;相關環境說明&quot;&gt;&lt;/a&gt;相關環境說明&lt;/h3&gt;&lt;blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;IP Address&lt;/th&gt;
&lt;th
    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Spark" scheme="http://yoursite.com/categories/BigData/Spark/"/>
    
    
  </entry>
  
</feed>
