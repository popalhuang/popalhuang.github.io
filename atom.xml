<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Popal&#39;s Blog</title>
  <subtitle>邊做邊學</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-07-22T05:27:15.088Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>popal</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linux - systemctl Command Usage</title>
    <link href="http://yoursite.com/2020/07/22/%5BLinux%5DLinux-systemctl/"/>
    <id>http://yoursite.com/2020/07/22/[Linux]Linux-systemctl/</id>
    <published>2020-07-21T16:00:00.000Z</published>
    <updated>2020-07-22T05:27:15.088Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>systemctl Command Memo</p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;systemctl Command Memo&lt;/p&gt;
&lt;/blockquote&gt;

    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hadoop 3.0.0 Overview</title>
    <link href="http://yoursite.com/2018/01/09/%5Bhadoop%5DHadoop3.0.0_overview/"/>
    <id>http://yoursite.com/2018/01/09/[hadoop]Hadoop3.0.0_overview/</id>
    <published>2018-01-08T16:00:00.000Z</published>
    <updated>2018-01-09T05:47:32.641Z</updated>
    
    <content type="html"><![CDATA[<ol>
<li>Minimum required Java version increased from Java 7 to Java 8</li>
<li>Support for erasure coding in HDFS</li>
<li>YARN Timeline Service v.2</li>
<li>Shell script rewrite</li>
<li>Shaded client jars</li>
<li>Support for Opportunistic Containers and Distributed Scheduling.</li>
<li>MapReduce task-level native optimization</li>
<li>Support for more than 2 NameNodes</li>
<li>Default ports of multiple services have been changed.</li>
<li>Support for Microsoft Azure Data Lake and Aliyun Object Storage System filesystem connectors</li>
<li>Intra-datanode balancer</li>
<li>Reworked daemon and task heap management</li>
<li>S3Guard: Consistency and Metadata Caching for the S3A filesystem client</li>
<li>HDFS Router-Based Federation</li>
<li>API-based configuration of Capacity Scheduler queue configuration</li>
<li>YARN Resource Types</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;ol&gt;
&lt;li&gt;Minimum required Java version increased from Java 7 to Java 8&lt;/li&gt;
&lt;li&gt;Support for erasure coding in HDFS&lt;/li&gt;
&lt;li&gt;YARN Timeline Se
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>Hadoop3.0.0 Document List</title>
    <link href="http://yoursite.com/2018/01/09/%5Bhadoop%5DHadoop3.0.0_doc/"/>
    <id>http://yoursite.com/2018/01/09/[hadoop]Hadoop3.0.0_doc/</id>
    <published>2018-01-08T16:00:00.000Z</published>
    <updated>2018-01-09T07:11:00.413Z</updated>
    
    <content type="html"><![CDATA[<h5 id="General"><a href="#General" class="headerlink" title="General"></a>General</h5><hr>
<p>Overview<br>Hadoop: Setting up a Single Node Cluster.<br>Hadoop Cluster Setup<br>Hadoop Commands Guide<br>FileSystem Shell<br>Apache Hadoop Compatibility<br>Apache Hadoop Downstream Developer’s Guide<br>Hadoop Interface Taxonomy: Audience and Stability Classification<br>The Hadoop FileSystem API Definition</p>
<h5 id="Common"><a href="#Common" class="headerlink" title="Common"></a>Common</h5><hr>
<p>Hadoop: CLI MiniCluster<br>Native Libraries Guide<br>Proxy user - Superusers Acting On Behalf Of Other Users<br>Rack Awareness<br>Hadoop in Secure Mode<br>Service Level Authorization Guide<br>Authentication for Hadoop HTTP web-consoles<br>CredentialProvider API Guide<br>Hadoop Key Management Server (KMS) - Documentation Sets<br>Enabling Dapper-like Tracing in Hadoop<br>Unix Shell Guide</p>
<h5 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h5><hr>
<p>HDFS Architecture<br>HDFS Users Guide<br>HDFS Commands Guide<br>HDFS High Availability Using the Quorum Journal Manager<br>HDFS High Availability<br>HDFS Federation<br>ViewFs Guide<br>HDFS Snapshots<br>Offline Edits Viewer Guide<br>Offline Image Viewer Guide<br>HDFS Permissions Guide<br>HDFS Quotas Guide<br>C API libhdfs<br>WebHDFS REST API<br>Hadoop HDFS over HTTP - Documentation Sets<br>HDFS Short-Circuit Local Reads<br>Centralized Cache Management in HDFS<br>HDFS NFS Gateway<br>HDFS Rolling Upgrade<br>Extended Attributes in HDFS<br>Transparent Encryption in HDFS<br>HDFS Support for Multihomed Networks<br>Archival Storage, SSD &amp; Memory<br>Memory Storage Support in HDFS<br>Synthetic Load Generator Guide<br>HDFS Erasure Coding<br>HDFS Disk Balancer<br>HDFS Upgrade Domain<br>HDFS DataNode Admin Guide<br>Unix Shell Guide<br>HDFS Router-based Federation</p>
<h5 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h5><hr>
<p>Tutorial<br>MapReduce Commands Guide<br>Apache Hadoop MapReduce - Migrating from Apache Hadoop 1.x to Apache Hadoop 2.x<br>Hadoop: Encrypted Shuffle<br>Hadoop: Pluggable Shuffle and Pluggable Sort<br>Hadoop: Distributed Cache Deploy<br>MR Support for YARN Shared Cache</p>
<h5 id="MapReduce-REST-APIs"><a href="#MapReduce-REST-APIs" class="headerlink" title="MapReduce REST APIs"></a>MapReduce REST APIs</h5><hr>
<p>MapReduce Application Master REST API’s.<br>MapReduce History Server REST API’s.</p>
<h5 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h5><hr>
<p>Apache Hadoop YARN<br>YARN Commands<br>Hadoop: Capacity Scheduler<br>Hadoop: Fair Scheduler<br>ResourceManager Restart<br>ResourceManager High Availability<br>Hadoop: YARN Resource Configuration<br>YARN Node Labels<br>Web Application Proxy<br>The YARN Timeline Server<br>The YARN Timeline Service v.2<br>Hadoop: Writing YARN Applications<br>YARN Application Security<br>NodeManager<br>Launching Applications Using Docker Containers<br>Using CGroups with YARN<br>YARN Secure Containers<br>YARN Service Registry<br>Reservation System<br>Graceful Decommission of YARN Nodes<br>Opportunistic Containers<br>Hadoop: YARN Federation<br>YARN Shared Cache</p>
<h5 id="YARN-REST-APIs"><a href="#YARN-REST-APIs" class="headerlink" title="YARN REST APIs"></a>YARN REST APIs</h5><hr>
<p>Hadoop YARN - Introduction to the web services REST API’s<br>ResourceManager REST API’s.<br>NodeManager REST API’s<br>The YARN Timeline Server<br>The YARN Timeline Service v.2</p>
<h5 id="Hadoop-Compatible-File-Systems"><a href="#Hadoop-Compatible-File-Systems" class="headerlink" title="Hadoop Compatible File Systems"></a>Hadoop Compatible File Systems</h5><hr>
<p>Hadoop-Aliyun module: Integration with Aliyun Web Services<br>Hadoop-AWS module: Integration with Amazon Web Services<br>Hadoop Azure Support: Azure Blob Storage<br>Hadoop Azure Data Lake Support<br>Hadoop OpenStack Support: Swift Object Store</p>
<h5 id="Auth"><a href="#Auth" class="headerlink" title="Auth"></a>Auth</h5><hr>
<p>Hadoop Auth, Java HTTP SPNEGO<br>Hadoop Auth, Java HTTP SPNEGO - Examples<br>Hadoop Auth, Java HTTP SPNEGO - Server Side Configuration<br>Hadoop Auth, Java HTTP SPNEGO - Building It</p>
<h5 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h5><hr>
<p>Hadoop Streaming<br>Hadoop Archives Guide<br>Hadoop Archive Logs Guide<br>DistCp Guide<br>Gridmix<br>Rumen<br>Resource Estimator Service<br>YARN Scheduler Load Simulator (SLS)<br>Hadoop Benchmarking</p>
<h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><hr>
<p>Changelog and Release Notes<br>Java API docs<br>Unix Shell API<br>Metrics</p>
<h5 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h5><hr>
<p>core-default.xml<br>hdfs-default.xml<br>mapred-default.xml<br>yarn-default.xml<br>Deprecated Properties</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;General&quot;&gt;&lt;a href=&quot;#General&quot; class=&quot;headerlink&quot; title=&quot;General&quot;&gt;&lt;/a&gt;General&lt;/h5&gt;&lt;hr&gt;
&lt;p&gt;Overview&lt;br&gt;Hadoop: Setting up a Single Node 
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>PySpark-RDD Memo_1</title>
    <link href="http://yoursite.com/2018/01/09/%5Bspark%5Dspark2.2.1_RDD_Memo_1/"/>
    <id>http://yoursite.com/2018/01/09/[spark]spark2.2.1_RDD_Memo_1/</id>
    <published>2018-01-08T16:00:00.000Z</published>
    <updated>2018-02-14T06:23:51.249Z</updated>
    
    <content type="html"><![CDATA[<h5 id="start-pyspark"><a href="#start-pyspark" class="headerlink" title="start pyspark"></a>start pyspark</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">hadoop@hadoop-master:~$ pyspark</div><div class="line"></div><div class="line">###看到以下spark歡迎畫面表示pyspark啟動成功,pyspark預設會把SparkSession的Instance建構好</div><div class="line">Python 2.7.13 (default, Jan 19 2017, 14:48:08)</div><div class="line">[GCC 6.3.0 20170118] on linux2</div><div class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</div><div class="line">Setting default log level to &quot;WARN&quot;.</div><div class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</div><div class="line">2018-01-09 15:32:11,663 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">2018-01-09 15:32:21,550 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</div><div class="line">Welcome to</div><div class="line">      ____              __</div><div class="line">     / __/__  ___ _____/ /__</div><div class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</div><div class="line">   /__ / .__/\_,_/_/ /_/\_\   version 2.2.1</div><div class="line">      /_/</div><div class="line"></div><div class="line">Using Python version 2.7.13 (default, Jan 19 2017 14:48:08)</div><div class="line">SparkSession available as &apos;spark&apos;.</div><div class="line">&gt;&gt;&gt;</div><div class="line"></div><div class="line">##開始可以操作pyspark相關語法</div><div class="line">## sc    是 SparkContext物件</div><div class="line">## spark 是 SparkSession物件</div><div class="line">&gt;&gt;&gt; sc</div><div class="line">&lt;SparkContext master=local[*] appName=PySparkShell&gt;</div><div class="line"></div><div class="line">&gt;&gt;&gt; spark</div><div class="line">&lt;pyspark.sql.session.SparkSession object at 0x7fa3a1ab4610&gt;</div><div class="line"></div><div class="line">&gt;&gt;&gt; from pyspark.conf import SparkConf</div><div class="line">&gt;&gt;&gt; conf = SparkConf()</div><div class="line">&gt;&gt;&gt; print (&quot;環境變數\n&quot;)</div><div class="line">環境變數</div><div class="line"></div><div class="line">&gt;&gt;&gt; print (conf.toDebugString())</div><div class="line">spark.app.name=PySparkShell</div><div class="line">spark.master=local[*]</div><div class="line">spark.submit.deployMode=client</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="pyspark-RDD-Operate"><a href="#pyspark-RDD-Operate" class="headerlink" title="pyspark: RDD Operate"></a>pyspark: RDD Operate</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line">## Apache Spark RDD - aggregate函數</div><div class="line">&gt;&gt;&gt; seqOp = (lambda x, y: (x[0] + y, x[1] + 1))</div><div class="line">&gt;&gt;&gt; combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))</div><div class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).aggregate((0, 0), seqOp, combOp)</div><div class="line">(15, 5)</div><div class="line">&gt;&gt;&gt; sc.parallelize([]).aggregate((0, 0), seqOp, combOp)</div><div class="line">(0, 0)</div><div class="line"></div><div class="line">## Apache Spark RDD - cartesian</div><div class="line">&gt;&gt;&gt; rdd = sc.parallelize([1, 2])</div><div class="line">&gt;&gt;&gt; sorted(rdd.cartesian(rdd).collect())</div><div class="line">[(1, 1), (1, 2), (2, 1), (2, 2)]</div><div class="line"></div><div class="line">## Apache Spark RDD - glom,coalesce</div><div class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()</div><div class="line">[[1], [2, 3], [4, 5]]</div><div class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()</div><div class="line">[[1, 2, 3, 4, 5]]</div><div class="line"></div><div class="line">## Apache Spark RDD - collectAsMap</div><div class="line">&gt;&gt;&gt; m = sc.parallelize([(&quot;ted&quot;, 2), (&quot;kevin&quot;, 4)]).collectAsMap()</div><div class="line">&gt;&gt;&gt; m[&quot;ted&quot;]</div><div class="line">2</div><div class="line">&gt;&gt;&gt; m[&quot;kevin&quot;]</div><div class="line">4</div><div class="line"></div><div class="line">## Apache Spark RDD - combineByKey</div><div class="line">&gt;&gt;&gt; x = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 2)])</div><div class="line">&gt;&gt;&gt; def to_list(a):</div><div class="line">...     return [a]</div><div class="line">...</div><div class="line">&gt;&gt;&gt; def append(a,b):</div><div class="line">...     a.append(b)</div><div class="line">...     return a</div><div class="line">...</div><div class="line">&gt;&gt;&gt; def extend(a,b):</div><div class="line">...     a.extend(b)</div><div class="line">...     return a</div><div class="line">...</div><div class="line">&gt;&gt;&gt; sorted(x.combineByKey(to_list, append, extend).collect())</div><div class="line">[(&apos;a&apos;, [1, 2]), (&apos;b&apos;, [1])]</div><div class="line"></div><div class="line">## Apache Spark RDD - count</div><div class="line">&gt;&gt;&gt; sc.parallelize([2, 3, 4]).count()</div><div class="line">3</div><div class="line"></div><div class="line">## Apache Spark RDD - countApproxDistinct</div><div class="line">&gt;&gt;&gt; n = sc.parallelize(range(1000)).map(str).countApproxDistinct()</div><div class="line">&gt;&gt;&gt; 900 &lt; n &lt; 1100</div><div class="line">True</div><div class="line">&gt;&gt;&gt; n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()</div><div class="line">&gt;&gt;&gt; 16 &lt; n &lt; 24</div><div class="line">True</div><div class="line"></div><div class="line">## Apache Spark RDD - countApprox</div><div class="line">&gt;&gt;&gt; rdd = sc.parallelize(range(1000), 10)</div><div class="line">&gt;&gt;&gt; rdd.countApprox(1000, 1.0)</div><div class="line">[Stage 20:===================================================&gt;     (9 + 1) / 10]1000</div><div class="line"></div><div class="line">## Apache Spark RDD - countByKey</div><div class="line">&gt;&gt;&gt; rdd = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1)])</div><div class="line">&gt;&gt;&gt; sorted(rdd.countByKey().items())</div><div class="line">[(&apos;a&apos;, 2), (&apos;b&apos;, 1)]</div><div class="line"></div><div class="line">## Apache Spark RDD - countByValue</div><div class="line">&gt;&gt;&gt; sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())</div><div class="line">[(1, 2), (2, 3)]</div><div class="line"></div><div class="line">## Apache Spark RDD - distinct</div><div class="line">&gt;&gt;&gt; sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())</div><div class="line">[1, 2, 3]</div><div class="line"></div><div class="line">## Apache Spark RDD - filter</div><div class="line">&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4, 5])</div><div class="line">&gt;&gt;&gt; rdd.filter(lambda x: x % 2 == 0).collect()</div><div class="line">[2,4]</div></pre></td></tr></table></figure>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;start-pyspark&quot;&gt;&lt;a href=&quot;#start-pyspark&quot; class=&quot;headerlink&quot; title=&quot;start pyspark&quot;&gt;&lt;/a&gt;start pyspark&lt;/h5&gt;&lt;blockquote&gt;
&lt;figure class=&quot;h
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Hadoop3.0.0 Quota</title>
    <link href="http://yoursite.com/2018/01/09/%5Bhadoop%5Dhadoop3.0.0_quota_cmd/"/>
    <id>http://yoursite.com/2018/01/09/[hadoop]hadoop3.0.0_quota_cmd/</id>
    <published>2018-01-08T16:00:00.000Z</published>
    <updated>2018-01-09T05:28:16.705Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Quota相關指令"><a href="#Quota相關指令" class="headerlink" title="Quota相關指令:"></a>Quota相關指令:</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">Administrative Commands:</div><div class="line">	hdfs dfsadmin</div><div class="line">		[-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">		[-clrQuota &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">		[-setSpaceQuota &lt;quota&gt; [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">		[-clrSpaceQuota [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">	</div><div class="line">Reporting Commands:</div><div class="line">	hadoop fs -count -q -v &lt;dirname&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="Name-Quotas"><a href="#Name-Quotas" class="headerlink" title="Name Quotas"></a>Name Quotas</h5><blockquote>
<p>透過限制目錄或文件數量,當達到限制上限時,系統會發出錯誤訊息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir -p /user/hadoop/example/data2</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -setQuota 3 /user/hadoop/example/data2</div><div class="line"></div><div class="line">## QUOTA     --&gt;Name Quota(-setQuota設定數量)</div><div class="line">## REM_QUOTA(剩餘Quota) --&gt;REM_QUOTA = Quota-(DIR_COUNT+FILE_COUNT)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -q -v /user/hadoop/example/data2</div><div class="line">       QUOTA       REM_QUOTA     SPACE_QUOTA REM_SPACE_QUOTA    DIR_COUNT   FILE_COUNT       CONTENT_SIZE PATHNAME</div><div class="line">           3               2            none             inf            1            0                  0 /user/hadoop/example/data2</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -put - /user/hadoop/example/data2/data1.txt</div><div class="line">Test1!!!!</div><div class="line">hadoop@hadoop-master:~$  hadoop fs -count -q -v /user/hadoop/example/data2</div><div class="line">       QUOTA       REM_QUOTA     SPACE_QUOTA REM_SPACE_QUOTA    DIR_COUNT   FILE_COUNT       CONTENT_SIZE PATHNAME</div><div class="line">           3               1            none             inf            1            1                 10 /user/hadoop/example/data2</div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -put - /user/hadoop/example/data2/data2.txt</div><div class="line">Test2!!!!</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -q -v /user/hadoop/example/data2</div><div class="line">       QUOTA       REM_QUOTA     SPACE_QUOTA REM_SPACE_QUOTA    DIR_COUNT   FILE_COUNT       CONTENT_SIZE PATHNAME</div><div class="line">           3               0            none             inf            1            2                 20 /user/hadoop/example/data2</div><div class="line">		   </div><div class="line">##無法再加入檔案,因為REM_QUOTA已經=0了		   </div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -put - /user/hadoop/example/data2/data3.txt</div><div class="line">put: The NameSpace quota (directories and files) of directory /user/hadoop/example/data2 is exceeded: quota=3 file count=4</div><div class="line"></div><div class="line">##清除Quota限制</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -clrQuota /user/hadoop/example/data2</div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -put - /user/hadoop/example/data2/data3.txt</div><div class="line">TEST3!!!!!!</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -q -v /user/hadoop/example/data2</div><div class="line">       QUOTA       REM_QUOTA     SPACE_QUOTA REM_SPACE_QUOTA    DIR_COUNT   FILE_COUNT       CONTENT_SIZE PATHNAME</div><div class="line">        none             inf            none             inf            1            3                 32 /user/hadoop/example/data2</div></pre></td></tr></table></figure></p>
</blockquote>
<h5 id="Space-Quotas"><a href="#Space-Quotas" class="headerlink" title="Space Quotas"></a>Space Quotas</h5><blockquote>
<p>透過限制目錄或檔案size大小,當達到限制上限時,系統會發出錯誤訊息(檔案大小和block size有關係)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir -p /user/hadoop/example/data3</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -setSpaceQuota 1M /user/hadoop/example/data3</div><div class="line"></div><div class="line">##Space Quota要考慮到Block size的問題,所以基本上會以block size的倍數來做為空間配額大小(block size*repl number)</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -setSpaceQuota 64M /user/hadoop/example/data3</div><div class="line">hadoop@hadoop-master:~$  hadoop fs -put - /user/hadoop/example/data3/data1.txt</div><div class="line">Test11111!!!</div><div class="line">hadoop@hadoop-master:~$  hadoop fs -count -q -v /user/hadoop/example/data3</div><div class="line">       QUOTA       REM_QUOTA     SPACE_QUOTA REM_SPACE_QUOTA    DIR_COUNT   FILE_COUNT       CONTENT_SIZE PATHNAME</div><div class="line">        none             inf        67108864        67108851            1            1                 13 /user/hadoop/example/data3</div><div class="line">hadoop@hadoop-master:~$  hadoop fs -put - /user/hadoop/example/data3/data2.txt</div><div class="line">TEST2!!!!</div><div class="line">put: The DiskSpace quota of /user/hadoop/example/data3 is exceeded: quota = 67108864 B = 64 MB but diskspace consumed = 67108877 B = 64.00 MB</div><div class="line"></div><div class="line">## 清除SpaceQuota的限制</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -clrSpaceQuota /user/hadoop/example/data3</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R /user/hadoop/example/data3</div><div class="line">-rw-r--r--   1 hadoop supergroup         13 2018-01-09 12:06 /user/hadoop/example/data3/data1.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -q -v /user/hadoop/example/data3</div><div class="line">       QUOTA       REM_QUOTA     SPACE_QUOTA REM_SPACE_QUOTA    DIR_COUNT   FILE_COUNT       CONTENT_SIZE PATHNAME</div><div class="line">        none             inf            none             inf            1            1                 13 /user/hadoop/example/data3</div></pre></td></tr></table></figure></p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;Quota相關指令&quot;&gt;&lt;a href=&quot;#Quota相關指令&quot; class=&quot;headerlink&quot; title=&quot;Quota相關指令:&quot;&gt;&lt;/a&gt;Quota相關指令:&lt;/h5&gt;&lt;blockquote&gt;
&lt;figure class=&quot;highlight plain
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hadoop3.0.0 相關服務 Port Number 設定參數</title>
    <link href="http://yoursite.com/2018/01/08/%5Bhadoop%5Dhadoop3_port_number_list/"/>
    <id>http://yoursite.com/2018/01/08/[hadoop]hadoop3_port_number_list/</id>
    <published>2018-01-07T16:00:00.000Z</published>
    <updated>2018-01-08T05:30:13.644Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">dfs.balancer.address					0.0.0.0:0</div><div class="line">dfs.mover.address					0.0.0.0:0</div><div class="line"></div><div class="line">dfs.federation.router.http-address			0.0.0.0:50071</div><div class="line">dfs.federation.router.https-address			0.0.0.0:50072</div><div class="line">dfs.federation.router.admin-address			0.0.0.0:8111</div><div class="line">dfs.federation.router.rpc-address			0.0.0.0:8888</div><div class="line"></div><div class="line">dfs.namenode.backup.address				0.0.0.0:50100</div><div class="line">dfs.namenode.backup.http-address			0.0.0.0:50105</div><div class="line"></div><div class="line">dfs.journalnode.http-address				0.0.0.0:8480</div><div class="line">dfs.journalnode.https-address				0.0.0.0:8481</div><div class="line">dfs.journalnode.rpc-address				0.0.0.0:8485</div><div class="line"></div><div class="line">dfs.datanode.http.address				0.0.0.0:9864</div><div class="line">dfs.datanode.https.address				0.0.0.0:9865</div><div class="line">dfs.datanode.address					0.0.0.0:9866</div><div class="line">dfs.datanode.ipc.address				0.0.0.0:9867</div><div class="line"></div><div class="line">dfs.namenode.secondary.http-address			0.0.0.0:9868</div><div class="line">dfs.namenode.secondary.https-address			0.0.0.0:9869</div><div class="line">dfs.namenode.http-address				0.0.0.0:9870</div><div class="line">dfs.namenode.https-address				0.0.0.0:9871</div><div class="line"></div><div class="line">yarn.nodemanager.address				$&#123;yarn.nodemanager.hostname&#125;:0</div><div class="line">yarn.nodemanager.localizer.address			$&#123;yarn.nodemanager.hostname&#125;:8040</div><div class="line">yarn.nodemanager.webapp.address				$&#123;yarn.nodemanager.hostname&#125;:8042</div><div class="line">yarn.nodemanager.collector-service.address		$&#123;yarn.nodemanager.hostname&#125;:8048</div><div class="line"></div><div class="line">yarn.resourcemanager.scheduler.address			$&#123;yarn.resourcemanager.hostname&#125;:8030</div><div class="line">yarn.resourcemanager.resource-tracker.address		$&#123;yarn.resourcemanager.hostname&#125;:8031</div><div class="line">yarn.resourcemanager.address				$&#123;yarn.resourcemanager.hostname&#125;:8032</div><div class="line">yarn.resourcemanager.admin.address			$&#123;yarn.resourcemanager.hostname&#125;:8033</div><div class="line">yarn.resourcemanager.webapp.address			$&#123;yarn.resourcemanager.hostname&#125;:8088</div><div class="line">yarn.resourcemanager.webapp.https.address		$&#123;yarn.resourcemanager.hostname&#125;:8090</div><div class="line"></div><div class="line">yarn.timeline-service.address				$&#123;yarn.timeline-service.hostname&#125;:10200</div><div class="line">yarn.timeline-service.webapp.address			$&#123;yarn.timeline-service.hostname&#125;:8188</div><div class="line">yarn.timeline-service.webapp.https.address		$&#123;yarn.timeline-service.hostname&#125;:8190</div><div class="line"></div><div class="line">mapreduce.jobhistory.address				0.0.0.0:10020	</div><div class="line">mapreduce.jobhistory.webapp.address			0.0.0.0:19888	</div><div class="line">mapreduce.jobhistory.webapp.https.address		0.0.0.0:19890</div><div class="line">mapreduce.jobhistory.admin.address			0.0.0.0:10033</div></pre></td></tr></table></figure></blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hadoop3.0.0 daemon command</title>
    <link href="http://yoursite.com/2018/01/08/%5Bhadoop%5Dhadoop%203.0.0_daemon_cmd/"/>
    <id>http://yoursite.com/2018/01/08/[hadoop]hadoop 3.0.0_daemon_cmd/</id>
    <published>2018-01-07T16:00:00.000Z</published>
    <updated>2018-01-08T05:35:49.241Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">hdfs --daemon start balancer             run a cluster balancing utility</div><div class="line">hdfs --daemon start datanode             run a DFS datanode</div><div class="line">hdfs --daemon start dfsrouter            run the DFS router</div><div class="line">hdfs --daemon start diskbalancer         Distributes data evenly among disks on a given node</div><div class="line">hdfs --daemon start journalnode          run the DFS journalnode</div><div class="line">hdfs --daemon start mover                run a utility to move block replicas across storage types</div><div class="line">hdfs --daemon start namenode             run the DFS namenode</div><div class="line">hdfs --daemon start nfs3                 run an NFS version 3 gateway</div><div class="line">hdfs --daemon start portmap              run a portmap service</div><div class="line">hdfs --daemon start secondarynamenode    run the DFS secondary namenode</div><div class="line">hdfs --daemon start zkfc                 run the ZK Failover Controller daemon</div><div class="line"></div><div class="line">yarn --daemon start nodemanager          run a nodemanager on each worker </div><div class="line">yarn --daemon start proxyserver          run the web app proxy server</div><div class="line">yarn --daemon start resourcemanager      run the ResourceManager</div><div class="line">yarn --daemon start router               run the Router daemon</div><div class="line">yarn --daemon start sharedcachemanager   run the SharedCacheManager daemon</div><div class="line">yarn --daemon start timelineserver       run the timeline server</div><div class="line"></div><div class="line">mapred --daemon start historyserver      run job history servers as a standalone daemon</div></pre></td></tr></table></figure></blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>Hadoop3.0.0 NFS Gateway</title>
    <link href="http://yoursite.com/2018/01/08/%5Bhadoop%5Dhadoop3.0.0_nfs_gateway/"/>
    <id>http://yoursite.com/2018/01/08/[hadoop]hadoop3.0.0_nfs_gateway/</id>
    <published>2018-01-07T16:00:00.000Z</published>
    <updated>2018-01-09T02:36:54.107Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Hadoop3-0-0-NFS-Gateway-setting-amp-start"><a href="#Hadoop3-0-0-NFS-Gateway-setting-amp-start" class="headerlink" title="Hadoop3.0.0 NFS Gateway setting &amp; start"></a>Hadoop3.0.0 NFS Gateway setting &amp; start</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ service nfs-kernel-server stop</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ start-dfs.sh</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ start-yarn.sh</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs --daemon start nfs3</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ rpcinfo -p</div><div class="line">   program vers proto   port  service</div><div class="line">    100000    4   tcp    111  portmapper</div><div class="line">    100000    3   tcp    111  portmapper</div><div class="line">    100000    2   tcp    111  portmapper</div><div class="line">    100000    4   udp    111  portmapper</div><div class="line">    100000    3   udp    111  portmapper</div><div class="line">    100000    2   udp    111  portmapper</div><div class="line">    100005    1   udp  46340  mountd</div><div class="line">    100005    1   tcp  33399  mountd</div><div class="line">    100005    2   udp  56322  mountd</div><div class="line">    100005    2   tcp  43065  mountd</div><div class="line">    100005    3   udp  52402  mountd</div><div class="line">    100005    3   tcp  49301  mountd</div><div class="line">    100003    2   tcp   2049  nfs</div><div class="line">    100003    3   tcp   2049  nfs</div><div class="line">    100003    4   tcp   2049  nfs</div><div class="line">    100227    2   tcp   2049</div><div class="line">    100227    3   tcp   2049</div><div class="line">    100003    2   udp   2049  nfs</div><div class="line">    100003    3   udp   2049  nfs</div><div class="line">    100003    4   udp   2049  nfs</div><div class="line">    100227    2   udp   2049</div><div class="line">    100227    3   udp   2049</div><div class="line">    100021    1   udp  50043  nlockmgr</div><div class="line">    100021    3   udp  50043  nlockmgr</div><div class="line">    100021    4   udp  50043  nlockmgr</div><div class="line">    100021    1   tcp  33247  nlockmgr</div><div class="line">    100021    3   tcp  33247  nlockmgr</div><div class="line">    100021    4   tcp  33247  nlockmgr</div><div class="line">	</div><div class="line">hadoop@hadoop-master:~$ showmount -e</div><div class="line">Export list for hadoop-master:</div><div class="line">/ *</div><div class="line">	</div><div class="line">hadoop@hadoop-master:~$ netstat -tnl</div><div class="line">Active Internet connections (only servers)</div><div class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State</div><div class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8088       0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8030       0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:50079           0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8031       0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8032       0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:2049            0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8033       0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:50090      0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:5355            0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:9870            0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 0.0.0.0:4242            0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 192.168.51.4:8020       0.0.0.0:*               LISTEN</div><div class="line">tcp6       0      0 :::22                   :::*                    LISTEN</div><div class="line">tcp6       0      0 :::5355                 :::*                    LISTEN</div><div class="line">tcp6       0      0 :::111                  :::*                    LISTEN</div><div class="line"></div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ jps</div><div class="line">4116 Nfs3</div><div class="line">2392 ResourceManager</div><div class="line">2108 SecondaryNameNode</div><div class="line">4157 Jps</div><div class="line">1837 NameNode</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ mkdir -p /opt/hdfs</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ sudo mount -t nfs -o vers=3,proto=tcp,nolock,sync 192.168.51.4:/  /opt/hdfs</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ ls -la /opt/hdfs/user</div><div class="line">total 4</div><div class="line">drwxr-xr-x  6 hadoop 2584148964  192 Jan  4 16:35 .</div><div class="line">drwxr-xr-x  4 hadoop 2584148964  128 Dec 26 15:25 ..</div><div class="line">drwxr-xr-x 41 hadoop 2584148964 1312 Jan  8 17:00 hadoop</div><div class="line">drwxr-xr-x  3 hadoop 2584148964   96 Dec 26 18:44 hive</div><div class="line">drwxr-xr-x  2 hadoop 2584148964   64 Jan  3 17:30 snapshot</div><div class="line">drwxr-xr-x  2 hadoop 2584148964   64 Jan  4 16:35 vagrant</div><div class="line"></div><div class="line">hadoop@hadoop-master:/home$ sudo umount /opt/hdfs</div><div class="line">hadoop@hadoop-master:/home$ ls -la /opt/hdfs</div><div class="line">total 8</div><div class="line">drwxrwxr-x 2 hadoop hadoop 4096 Jan  8 17:32 .</div><div class="line">drwxr-xr-x 4 hadoop hadoop 4096 Jan  8 17:32 ..</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="Hadoop3-0-0-NFS-Gateway-modify-mount-point"><a href="#Hadoop3-0-0-NFS-Gateway-modify-mount-point" class="headerlink" title="Hadoop3.0.0 NFS Gateway modify mount point"></a>Hadoop3.0.0 NFS Gateway modify mount point</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">vi /bgdt/hadoop-3.0.0/etc/hadoop/hdfs-site.xml</div><div class="line"></div><div class="line">##在設定檔中加入以下參數(nfs.export.point)</div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;nfs.export.point&lt;/name&gt;</div><div class="line">  &lt;value&gt;/user&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line"></div><div class="line">##重新啟動HDFS</div><div class="line">hadoop@hadoop-master:~$start-dfs.sh</div><div class="line"></div><div class="line">##重新啟動NFS3</div><div class="line">hadoop@hadoop-master:~$hdfs --daemon start nfs3</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$showmount -e</div><div class="line">Export list for hadoop-master:</div><div class="line">/user *</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ sudo mount -t nfs -o vers=3,proto=tcp,nolock,sync 192.168.51.4:/user  /opt/hdfs</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ ls -la /opt/hdfs</div><div class="line">total 8</div><div class="line">drwxr-xr-x  6 hadoop 2584148964  192 Jan  4 16:35 .</div><div class="line">drwxr-xr-x  4 hadoop hadoop     4096 Jan  8 17:32 ..</div><div class="line">drwxr-xr-x 41 hadoop 2584148964 1312 Jan  8 17:00 hadoop</div><div class="line">drwxr-xr-x  3 hadoop 2584148964   96 Dec 26 18:44 hive</div><div class="line">drwxr-xr-x  2 hadoop 2584148964   64 Jan  3 17:30 snapshot</div><div class="line">drwxr-xr-x  2 hadoop 2584148964   64 Jan  4 16:35 vagrant</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ sudo umount /opt/hdfs</div></pre></td></tr></table></figure>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;Hadoop3-0-0-NFS-Gateway-setting-amp-start&quot;&gt;&lt;a href=&quot;#Hadoop3-0-0-NFS-Gateway-setting-amp-start&quot; class=&quot;headerlink&quot; title=&quot;Hadoop3.0.
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hadoop archive command Memo</title>
    <link href="http://yoursite.com/2018/01/04/%5Bhadoop%5Dhadoop_archive_cmd/"/>
    <id>http://yoursite.com/2018/01/04/[hadoop]hadoop_archive_cmd/</id>
    <published>2018-01-03T16:00:00.000Z</published>
    <updated>2018-01-04T10:37:42.385Z</updated>
    
    <content type="html"><![CDATA[<h5 id="hadoop壓縮機制"><a href="#hadoop壓縮機制" class="headerlink" title="hadoop壓縮機制"></a>hadoop壓縮機制</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">usage: archive &lt;-archiveName &lt;NAME&gt;.har&gt; &lt;-p &lt;parent path&gt;&gt; [-r &lt;replication factor&gt;] &lt;src&gt;* &lt;dest&gt;</div><div class="line"> -archiveName &lt;arg&gt;   Name of the Archive. This is mandatory option</div><div class="line"> -help                Show the usage</div><div class="line"> -p &lt;arg&gt;             Parent path of sources. This is mandatory option</div><div class="line"> -r &lt;arg&gt;             Replication factor archive files</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hadoop-archive操作方式"><a href="#hadoop-archive操作方式" class="headerlink" title="hadoop archive操作方式"></a>hadoop archive操作方式</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/test1.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/test2.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/test3.txt</div><div class="line"></div><div class="line">##壓縮/user/hadoop/example目錄下所有檔案和目錄</div><div class="line">hadoop@hadoop-master:~$ hadoop archive -archiveName example1.har -p /user/hadoop/example -r 3 /user/hadoop</div><div class="line"></div><div class="line">##壓縮/user/hadoop/example目錄下所有txt檔</div><div class="line">hadoop@hadoop-master:~$ hadoop archive -archiveName example2.har -p /user/hadoop/example/ -r 3 *.txt /user/hadoop</div><div class="line"></div><div class="line">## 查詢壓縮檔的內容</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R har:///user/hadoop/example1.har</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-04 18:12 har:///user/hadoop/example1.har/lab_2</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-04 18:13 har:///user/hadoop/example1.har/lab_2/example1.har</div><div class="line">-rw-r--r--   3 hadoop supergroup        373 2018-01-03 15:41 har:///user/hadoop/example1.har/lab_2/test9.tar.gz</div><div class="line">-rw-r--r--   3 hadoop supergroup         23 2018-01-04 18:09 har:///user/hadoop/example1.har/test1.txt</div><div class="line">-rw-r--r--   3 hadoop supergroup         30 2018-01-04 18:10 har:///user/hadoop/example1.har/test2.txt</div><div class="line">-rw-r--r--   3 hadoop supergroup         15 2018-01-04 18:10 har:///user/hadoop/example1.har/test3.txt</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R har:///user/hadoop/example2.har</div><div class="line">-rw-r--r--   3 hadoop supergroup         23 2018-01-04 18:09 har:///user/hadoop/example2.har/test1.txt</div><div class="line">-rw-r--r--   3 hadoop supergroup         30 2018-01-04 18:10 har:///user/hadoop/example2.har/test2.txt</div><div class="line">-rw-r--r--   3 hadoop supergroup         15 2018-01-04 18:10 har:///user/hadoop/example2.har/test3.txt</div><div class="line"></div><div class="line">##解壓縮使用cp命令</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir -p /user/hadoop/example/lab_3</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cp har:///user/hadoop/example2.har/* hdfs:/user/hadoop/example/lab_3</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R /user/hadoop/example/lab_3</div><div class="line">-rw-r--r--   1 hadoop supergroup         23 2018-01-04 18:34 /user/hadoop/example/lab_3/test1.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup         30 2018-01-04 18:34 /user/hadoop/example/lab_3/test2.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup         15 2018-01-04 18:34 /user/hadoop/example/lab_3/test3.txt</div></pre></td></tr></table></figure>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;hadoop壓縮機制&quot;&gt;&lt;a href=&quot;#hadoop壓縮機制&quot; class=&quot;headerlink&quot; title=&quot;hadoop壓縮機制&quot;&gt;&lt;/a&gt;hadoop壓縮機制&lt;/h5&gt;&lt;blockquote&gt;
&lt;figure class=&quot;highlight pla
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hdfs snapshot command Memo</title>
    <link href="http://yoursite.com/2018/01/04/%5Bhadoop%5Dhadoop_snapshot_cmd/"/>
    <id>http://yoursite.com/2018/01/04/[hadoop]hadoop_snapshot_cmd/</id>
    <published>2018-01-03T16:00:00.000Z</published>
    <updated>2018-01-04T08:49:54.309Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Hadoop-Snapshot機制用到的相關語法"><a href="#Hadoop-Snapshot機制用到的相關語法" class="headerlink" title="Hadoop Snapshot機制用到的相關語法"></a>Hadoop Snapshot機制用到的相關語法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">hdfs lsSnapshottableDir</div><div class="line">hdfs snapshotDiff</div><div class="line">hdfs dfsadmin -allowSnapshot</div><div class="line">hdfs dfsadmin -disallowSnapshot</div><div class="line">hdfs dfs -createSnapshot</div><div class="line">hdfs dfs -deleteSnapshot</div><div class="line">hdfs dfs -renameSnapshot</div><div class="line">hdfs fsck -includeSnapshots</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="Snapshot機制操作說明"><a href="#Snapshot機制操作說明" class="headerlink" title="Snapshot機制操作說明"></a>Snapshot機制操作說明</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir /user/hadoop/snapshot</div><div class="line"></div><div class="line">##選定一個目錄做為Snapshot的起點</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -allowSnapshot /user/hadoop/snapshot</div><div class="line">Allowing snaphot on /user/hadoop/snapshot succeeded</div><div class="line"></div><div class="line">##列出所有要做Snapshot目錄</div><div class="line">hadoop@hadoop-master:~$ hdfs lsSnapshottableDir</div><div class="line">drwxr-xr-x 0 hadoop supergroup 0 2018-01-03 16:50 1 65536 /user/hadoop/snapshot</div><div class="line"></div><div class="line">##建立Snapshot</div><div class="line">hadoop@hadoop-master:~$  hadoop fs -createSnapshot /user/hadoop/snapshot snapshot</div><div class="line">Created snapshot /user/hadoop/snapshot/.snapshot/snapshot</div><div class="line"></div><div class="line">##在Snapshot目錄中新增一個檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/snapshot/test1.txt</div><div class="line">11111</div><div class="line">2222</div><div class="line">33333</div><div class="line">44444</div><div class="line">55555</div><div class="line"></div><div class="line">##再做一次Snapshot</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -createSnapshot /user/hadoop/snapshot snapshot_201801031654</div><div class="line">Created snapshot /user/hadoop/snapshot/.snapshot/snapshot_201801031654</div><div class="line"></div><div class="line">##比較兩個Snapshot之間的差異</div><div class="line">hadoop@hadoop-master:~$ hdfs snapshotDiff /user/hadoop/snapshot snapshot snapshot_201801031654</div><div class="line">Difference between snapshot snapshot and snapshot snapshot_201801031654 under directory /user/hadoop/snapshot:</div><div class="line">M       .</div><div class="line">+       ./test1.txt</div><div class="line"></div><div class="line">##以下為復原Snapshot方式</div><div class="line">hadoop@hadoop-master:~$hadoop fs -rm -r -skipTrash /user/hadoop/snapshot/*</div><div class="line">hadoop@hadoop-master:~$hadoop fs -cp /user/hadoop/snapshot/.snapshot/snapshot/* /user/hadoop/snapshot</div><div class="line"></div><div class="line"></div><div class="line">##刪除Snapshot</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir /tmp/important-data</div><div class="line">hadoop@hadoop-master:~$ echo &quot;important data&quot; | hdfs dfs -put - /tmp/important-dir/important-file.txt</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -allowSnapshot  /tmp/important-dir</div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -createSnapshot /tmp/important-dir first-snapshot</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs lsSnapshottableDir</div><div class="line">drwxr-xr-x 0 hadoop supergroup 0 2018-01-04 11:12 1 65536 /tmp/important-dir</div><div class="line">drwxr-xr-x 0 hadoop supergroup 0 2018-01-04 11:59 3 65536 /user/hadoop/snapshot</div><div class="line"></div><div class="line">##此目錄下尚有其他Snapshot存在無法操作disallowSnapshot</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -disallowSnapshot /tmp/important-dir</div><div class="line">disallowSnapshot: The directory /tmp/important-dir has snapshot(s). Please redo the operation after removing all the snapshots.</div><div class="line"></div><div class="line">##須將所有Snapshot刪除後,才能操作disallowSnapshot</div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -deleteSnapshot /tmp/important-dir first-snapshot</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -disallowSnapshot /tmp/important-dir</div><div class="line">Disallowing snaphot on /tmp/important-dir succeeded</div><div class="line">hadoop@hadoop-master:~$ hdfs lsSnapshottableDir</div><div class="line">drwxr-xr-x 0 hadoop supergroup 0 2018-01-04 11:59 3 65536 /user/hadoop/snapshot</div><div class="line"></div><div class="line">##更改Snapshot名稱,並顯示snapshottable下所有的Snapshot</div><div class="line">hadoop@hadoop-master:~$ hdfs dfs -renameSnapshot /user/hadoop/snapshot snapshot snapshot_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/snapshot/.snapshot</div><div class="line">Found 3 items</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-03 17:01 /user/hadoop/snapshot/.snapshot/snapshot_1</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-03 17:01 /user/hadoop/snapshot/.snapshot/snapshot_201801031654</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-03 18:01 /user/hadoop/snapshot/.snapshot/snapshot_201801031737</div></pre></td></tr></table></figure>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;Hadoop-Snapshot機制用到的相關語法&quot;&gt;&lt;a href=&quot;#Hadoop-Snapshot機制用到的相關語法&quot; class=&quot;headerlink&quot; title=&quot;Hadoop Snapshot機制用到的相關語法&quot;&gt;&lt;/a&gt;Hadoop Snapsho
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hdfs dfsadmin command Memo</title>
    <link href="http://yoursite.com/2018/01/03/%5Bhadoop%5Dhdfs_dfsadmin_cmd/"/>
    <id>http://yoursite.com/2018/01/03/[hadoop]hdfs_dfsadmin_cmd/</id>
    <published>2018-01-02T16:00:00.000Z</published>
    <updated>2018-01-04T06:24:18.600Z</updated>
    
    <content type="html"><![CDATA[<h5 id="hdfs-dfsadmin相關語法"><a href="#hdfs-dfsadmin相關語法" class="headerlink" title="hdfs dfsadmin相關語法"></a>hdfs dfsadmin相關語法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">Note: Administrative commands can only be run as the HDFS superuser.</div><div class="line">        [-report [-live] [-dead] [-decommissioning] [-enteringmaintenance] [-inmaintenance]]</div><div class="line">        [-safemode &lt;enter | leave | get | wait&gt;]</div><div class="line">        [-saveNamespace [-beforeShutdown]]</div><div class="line">        [-rollEdits]</div><div class="line">        [-restoreFailedStorage true|false|check]</div><div class="line">        [-refreshNodes]</div><div class="line">        [-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">        [-clrQuota &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">        [-setSpaceQuota &lt;quota&gt; [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">        [-clrSpaceQuota [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;]</div><div class="line">        [-finalizeUpgrade]</div><div class="line">        [-rollingUpgrade [&lt;query|prepare|finalize&gt;]]</div><div class="line">        [-refreshServiceAcl]</div><div class="line">        [-refreshUserToGroupsMappings]</div><div class="line">        [-refreshSuperUserGroupsConfiguration]</div><div class="line">        [-refreshCallQueue]</div><div class="line">        [-refresh &lt;host:ipc_port&gt; &lt;key&gt; [arg1..argn]</div><div class="line">        [-reconfig &lt;namenode|datanode&gt; &lt;host:ipc_port&gt; &lt;start|status|properties&gt;]</div><div class="line">        [-printTopology]</div><div class="line">        [-refreshNamenodes datanode_host:ipc_port]</div><div class="line">        [-getVolumeReport datanode_host:ipc_port]</div><div class="line">        [-deleteBlockPool datanode_host:ipc_port blockpoolId [force]]</div><div class="line">        [-setBalancerBandwidth &lt;bandwidth in bytes per second&gt;]</div><div class="line">        [-getBalancerBandwidth &lt;datanode_host:ipc_port&gt;]</div><div class="line">        [-fetchImage &lt;local directory&gt;]</div><div class="line">        [-allowSnapshot &lt;snapshotDir&gt;]</div><div class="line">        [-disallowSnapshot &lt;snapshotDir&gt;]</div><div class="line">        [-shutdownDatanode &lt;datanode_host:ipc_port&gt; [upgrade]]</div><div class="line">        [-evictWriters &lt;datanode_host:ipc_port&gt;]</div><div class="line">        [-getDatanodeInfo &lt;datanode_host:ipc_port&gt;]</div><div class="line">        [-metasave filename]</div><div class="line">        [-triggerBlockReport [-incremental] &lt;datanode_host:ipc_port&gt;]</div><div class="line">        [-listOpenFiles]</div><div class="line">        [-help [cmd]]</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-report"><a href="#hdfs-dfsadmin-report" class="headerlink" title="hdfs dfsadmin -report"></a>hdfs dfsadmin -report</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -report</div><div class="line">Configured Capacity: 42002972672 (39.12 GB)</div><div class="line">Present Capacity: 26379444224 (24.57 GB)</div><div class="line">DFS Remaining: 26331783168 (24.52 GB)</div><div class="line">DFS Used: 47661056 (45.45 MB)</div><div class="line">DFS Used%: 0.18%</div><div class="line">Replicated Blocks:</div><div class="line">        Under replicated blocks: 967</div><div class="line">        Blocks with corrupt replicas: 0</div><div class="line">        Missing blocks: 0</div><div class="line">        Missing blocks (with replication factor 1): 0</div><div class="line">        Pending deletion blocks: 0</div><div class="line">Erasure Coded Block Groups:</div><div class="line">        Low redundancy block groups: 0</div><div class="line">        Block groups with corrupt internal blocks: 0</div><div class="line">        Missing block groups: 0</div><div class="line">        Pending deletion blocks: 0</div><div class="line"></div><div class="line">-------------------------------------------------</div><div class="line">Live datanodes (2):</div><div class="line"></div><div class="line">Name: 192.168.51.5:9866 (hadoop-slave1)</div><div class="line">Hostname: hadoop-slave1</div><div class="line">Decommission Status : Normal</div><div class="line">Configured Capacity: 21001486336 (19.56 GB)</div><div class="line">DFS Used: 23842816 (22.74 MB)</div><div class="line">Non DFS Used: 6787604480 (6.32 GB)</div><div class="line">DFS Remaining: 13099626496 (12.20 GB)</div><div class="line">DFS Used%: 0.11%</div><div class="line">DFS Remaining%: 62.37%</div><div class="line">Configured Cache Capacity: 0 (0 B)</div><div class="line">Cache Used: 0 (0 B)</div><div class="line">Cache Remaining: 0 (0 B)</div><div class="line">Cache Used%: 100.00%</div><div class="line">Cache Remaining%: 0.00%</div><div class="line">Xceivers: 1</div><div class="line">Last contact: Wed Jan 03 17:52:32 CST 2018</div><div class="line">Last Block Report: Wed Jan 03 15:42:49 CST 2018</div><div class="line"></div><div class="line"></div><div class="line">Name: 192.168.51.6:9866 (hadoop-slave2)</div><div class="line">Hostname: hadoop-slave2</div><div class="line">Decommission Status : Normal</div><div class="line">Configured Capacity: 21001486336 (19.56 GB)</div><div class="line">DFS Used: 23818240 (22.71 MB)</div><div class="line">Non DFS Used: 6655098880 (6.20 GB)</div><div class="line">DFS Remaining: 13232156672 (12.32 GB)</div><div class="line">DFS Used%: 0.11%</div><div class="line">DFS Remaining%: 63.01%</div><div class="line">Configured Cache Capacity: 0 (0 B)</div><div class="line">Cache Used: 0 (0 B)</div><div class="line">Cache Remaining: 0 (0 B)</div><div class="line">Cache Used%: 100.00%</div><div class="line">Cache Remaining%: 0.00%</div><div class="line">Xceivers: 1</div><div class="line">Last contact: Wed Jan 03 17:52:33 CST 2018</div><div class="line">Last Block Report: Wed Jan 03 16:33:35 CST 2018</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-printTopology"><a href="#hdfs-dfsadmin-printTopology" class="headerlink" title="hdfs dfsadmin -printTopology"></a>hdfs dfsadmin -printTopology</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -printTopology</div><div class="line">Rack: /default-rack</div><div class="line">   192.168.51.5:9866 (hadoop-slave1)</div><div class="line">   192.168.51.6:9866 (hadoop-slave2)</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-getDatanodeInfo"><a href="#hdfs-dfsadmin-getDatanodeInfo" class="headerlink" title="hdfs dfsadmin -getDatanodeInfo"></a>hdfs dfsadmin -getDatanodeInfo</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getDatanodeInfo 192.168.51.6:9867</div><div class="line">Uptime: 5100, Software version: 3.0.0, Config version: core-3.0.0,hdfs-1</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getDatanodeInfo 192.168.51.5:9867</div><div class="line">Uptime: 10709, Software version: 3.0.0, Config version: core-3.0.0,hdfs-1</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-safemode"><a href="#hdfs-dfsadmin-safemode" class="headerlink" title="hdfs dfsadmin -safemode"></a>hdfs dfsadmin -safemode</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -safemode</div><div class="line">Usage: hdfs dfsadmin [-safemode enter | leave | get | wait | forceExit]</div><div class="line"></div><div class="line">##取得safemode模式</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -safemode get</div><div class="line">Safe mode is OFF</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/snapshot/test3.txt</div><div class="line">3333</div><div class="line">4444</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/snapshot</div><div class="line">Found 3 items</div><div class="line">-rw-r--r--   1 hadoop supergroup         29 2018-01-03 16:53 /user/hadoop/snapshot/test1.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup          7 2018-01-03 17:36 /user/hadoop/snapshot/test2.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup         10 2018-01-03 18:04 /user/hadoop/snapshot/test3.txt</div><div class="line"></div><div class="line">##進入safemode模式</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -safemode enter</div><div class="line">Safe mode is ON</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -safemode get</div><div class="line">Safe mode is ON</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/snapshot/test4.txt</div><div class="line">put: Cannot create file/user/hadoop/snapshot/test4.txt._COPYING_. Name node is in safe mode.</div><div class="line"></div><div class="line">##離開safemode模式</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -safemode leave</div><div class="line">Safe mode is OFF</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/snapshot/test4.txt</div><div class="line">55555</div><div class="line">66666</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/snapshot</div><div class="line">Found 4 items</div><div class="line">-rw-r--r--   1 hadoop supergroup         29 2018-01-03 16:53 /user/hadoop/snapshot/test1.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup          7 2018-01-03 17:36 /user/hadoop/snapshot/test2.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup         10 2018-01-03 18:04 /user/hadoop/snapshot/test3.txt</div><div class="line">-rw-r--r--   1 hadoop supergroup         12 2018-01-03 18:05 /user/hadoop/snapshot/test4.txt</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-shutdownDatanode"><a href="#hdfs-dfsadmin-shutdownDatanode" class="headerlink" title="hdfs dfsadmin -shutdownDatanode"></a>hdfs dfsadmin -shutdownDatanode</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">##停掉 Datanode</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -shutdownDatanode hadoop-slave2:9867</div><div class="line">Submitted a shutdown request to datanode hadoop-slave2:9867</div><div class="line"></div><div class="line">##觀察 Datanode Status</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -report -dead</div><div class="line">Safe mode is ON</div><div class="line">Configured Capacity: 21001486336 (19.56 GB)</div><div class="line">Present Capacity: 13123469312 (12.22 GB)</div><div class="line">DFS Remaining: 13099626496 (12.20 GB)</div><div class="line">DFS Used: 23842816 (22.74 MB)</div><div class="line">DFS Used%: 0.18%</div><div class="line">Replicated Blocks:</div><div class="line">        Under replicated blocks: 971</div><div class="line">        Blocks with corrupt replicas: 0</div><div class="line">        Missing blocks: 2</div><div class="line">        Missing blocks (with replication factor 1): 2</div><div class="line">        Pending deletion blocks: 0</div><div class="line">Erasure Coded Block Groups:</div><div class="line">        Low redundancy block groups: 1</div><div class="line">        Block groups with corrupt internal blocks: 0</div><div class="line">        Missing block groups: 0</div><div class="line">        Pending deletion blocks: 0</div><div class="line"></div><div class="line">-------------------------------------------------</div><div class="line">Dead datanodes (1):</div><div class="line"></div><div class="line">Name: 192.168.51.6:9866 (hadoop-slave2)</div><div class="line">Hostname: hadoop-slave2</div><div class="line">Decommission Status : Normal</div><div class="line">Configured Capacity: 21001486336 (19.56 GB)</div><div class="line">DFS Used: 23834624 (22.73 MB)</div><div class="line">Non DFS Used: 6655102976 (6.20 GB)</div><div class="line">DFS Remaining: 13232136192 (12.32 GB)</div><div class="line">DFS Used%: 0.11%</div><div class="line">DFS Remaining%: 63.01%</div><div class="line">Configured Cache Capacity: 0 (0 B)</div><div class="line">Cache Used: 0 (0 B)</div><div class="line">Cache Remaining: 0 (0 B)</div><div class="line">Cache Used%: 100.00%</div><div class="line">Cache Remaining%: 0.00%</div><div class="line">Xceivers: 0</div><div class="line">Last contact: Wed Jan 03 18:16:00 CST 2018</div><div class="line">Last Block Report: Wed Jan 03 16:33:35 CST 2018</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-metasave"><a href="#hdfs-dfsadmin-metasave" class="headerlink" title="hdfs dfsadmin -metasave"></a>hdfs dfsadmin -metasave</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">## 產生所有metadata資訊,產生後會存放於hdfs系統的Log資料夾內</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -metasave metasave.txt</div><div class="line">Created metasave file metasave.txt in the log directory of namenode hdfs://hadoop-master:8020</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-listOpenFiles"><a href="#hdfs-dfsadmin-listOpenFiles" class="headerlink" title="hdfs dfsadmin -listOpenFiles"></a>hdfs dfsadmin -listOpenFiles</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -listOpenFiles</div><div class="line">Client Host             Client Name             Open File Path</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-triggerBlockReport"><a href="#hdfs-dfsadmin-triggerBlockReport" class="headerlink" title="hdfs dfsadmin -triggerBlockReport"></a>hdfs dfsadmin -triggerBlockReport</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -triggerBlockReport 192.168.51.5:9867</div><div class="line">Triggering a full block report on 192.168.51.5:9867.</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hdfs-dfsadmin-BalancerBandwidth"><a href="#hdfs-dfsadmin-BalancerBandwidth" class="headerlink" title="hdfs dfsadmin BalancerBandwidth"></a>hdfs dfsadmin BalancerBandwidth</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">##取得Namenode與各Datanode之間的IPC頻寬(目前設定為10M bytes/s)</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getBalancerBandwidth hadoop-slave1:9867</div><div class="line">Balancer bandwidth is 10485760 bytes per second.</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getBalancerBandwidth hadoop-slave2:9867</div><div class="line">Balancer bandwidth is 10485760 bytes per second.</div><div class="line"></div><div class="line">##設定Namenode與Datanode之間的頻寬大小</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -setBalancerBandwidth 15M</div><div class="line">Balancer bandwidth is set to 15728640</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getBalancerBandwidth hadoop-slave2:9867</div><div class="line">Balancer bandwidth is 15728640 bytes per second.</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -getBalancerBandwidth hadoop-slave1:9867</div><div class="line">Balancer bandwidth is 15728640 bytes per second.</div></pre></td></tr></table></figure>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;hdfs-dfsadmin相關語法&quot;&gt;&lt;a href=&quot;#hdfs-dfsadmin相關語法&quot; class=&quot;headerlink&quot; title=&quot;hdfs dfsadmin相關語法&quot;&gt;&lt;/a&gt;hdfs dfsadmin相關語法&lt;/h5&gt;&lt;blockquote&gt;

    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hdfs fsck command Memo</title>
    <link href="http://yoursite.com/2018/01/03/%5Bhadoop%5Dhadoop_fsck_cmd/"/>
    <id>http://yoursite.com/2018/01/03/[hadoop]hadoop_fsck_cmd/</id>
    <published>2018-01-02T16:00:00.000Z</published>
    <updated>2018-01-04T10:44:48.754Z</updated>
    
    <content type="html"><![CDATA[<h4 id="hadoop-fsck使用方式"><a href="#hadoop-fsck使用方式" class="headerlink" title="hadoop fsck使用方式"></a>hadoop fsck使用方式</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:15:13 CST 2018</div><div class="line"></div><div class="line">Status: HEALTHY</div><div class="line"> Number of data-nodes:  2</div><div class="line"> Number of racks:               1</div><div class="line"> Total dirs:                    0</div><div class="line"> Total symlinks:                0</div><div class="line"></div><div class="line">Replicated Blocks:</div><div class="line"> Total size:    133 B</div><div class="line"> Total files:   1</div><div class="line"> Total blocks (validated):      1 (avg. block size 133 B)</div><div class="line"> Minimally replicated blocks:   1 (100.0 %)</div><div class="line"> Over-replicated blocks:        0 (0.0 %)</div><div class="line"> Under-replicated blocks:       0 (0.0 %)</div><div class="line"> Mis-replicated blocks:         0 (0.0 %)</div><div class="line"> Default replication factor:    2</div><div class="line"> Average block replication:     2.0</div><div class="line"> Missing blocks:                0</div><div class="line"> Corrupt blocks:                0</div><div class="line"> Missing replicas:              0 (0.0 %)</div><div class="line"></div><div class="line">Erasure Coded Block Groups:</div><div class="line"> Total size:    0 B</div><div class="line"> Total files:   0</div><div class="line"> Total block groups (validated):        0</div><div class="line"> Minimally erasure-coded block groups:  0</div><div class="line"> Over-erasure-coded block groups:       0</div><div class="line"> Under-erasure-coded block groups:      0</div><div class="line"> Unsatisfactory placement block groups: 0</div><div class="line"> Average block group size:      0.0</div><div class="line"> Missing block groups:          0</div><div class="line"> Corrupt block groups:          0</div><div class="line"> Missing internal blocks:       0</div><div class="line">FSCK ended at Wed Jan 03 13:15:13 CST 2018 in 1 milliseconds</div><div class="line"></div><div class="line"></div><div class="line">The filesystem under path &apos;/user/hadoop/test1.txt&apos; is HEALTHY</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="hdfs-fsck-相關參數操作說明"><a href="#hdfs-fsck-相關參數操作說明" class="headerlink" title="hdfs fsck 相關參數操作說明"></a>hdfs fsck 相關參數操作說明</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line">##檢查並列出所有文件的狀態</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:17:02 CST 2018</div><div class="line">/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s):  OK</div><div class="line"></div><div class="line">##列出所有Blocks資訊</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files -blocks</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:18:46 CST 2018</div><div class="line">/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s):  OK</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_1073743138_2323 len=133 Live_repl=2</div><div class="line"></div><div class="line">##列出所有Blocks的位置訊息</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files -blocks -locations</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;locations=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:19:53 CST 2018</div><div class="line">/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s):  OK</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_1073743138_2323 len=133 Live_repl=2  </div><div class="line">[DatanodeInfoWithStorage[192.168.51.6:9866,DS-482194d9-aa70-4ab8-8253-907739d5b1a1,DISK], </div><div class="line"> DatanodeInfoWithStorage[192.168.51.5:9866,DS-f59711c6-801d-4ebc-b55c-228f225117b8,DISK]]</div><div class="line"></div><div class="line">##列出檔案所有的訊息包含Rack位置</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files -blocks -locations -racks</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;locations=1&amp;racks=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Thu Jan 04 18:43:12 CST 2018</div><div class="line">/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s):  OK</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_1073743138_2323 len=133 Live_repl=2  [/default-rack/192.168.51.5:9866, /default-rack/192.168.51.6:9866]</div><div class="line"></div><div class="line"></div><div class="line">##列出所有完整的replication的位置訊息</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files -blocks -replicaDetails</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;replicadetails=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:21:56 CST 2018</div><div class="line">/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s):  OK</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_1073743138_2323 len=133 Live_repl=2  </div><div class="line">[DatanodeInfoWithStorage[192.168.51.6:9866,DS-482194d9-aa70-4ab8-8253-907739d5b1a1,DISK](LIVE), </div><div class="line"> DatanodeInfoWithStorage[192.168.51.5:9866,DS-f59711c6-801d-4ebc-b55c-228f225117b8,DISK](LIVE)]</div><div class="line"></div><div class="line">##查看文件中損壞Blocks的狀況</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -list-corruptfileblocks</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;listcorruptfileblocks=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">The filesystem under path &apos;/user/hadoop/test1.txt&apos; has 0 CORRUPT files</div><div class="line"></div><div class="line">##列出指定的block的詳細資訊</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -blockId blk_1073743138</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;blockId=blk_1073743138+&amp;path=%2Fuser%2Fhadoop%2Ftest1.txt</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 at Wed Jan 03 13:34:40 CST 2018</div><div class="line"></div><div class="line">Block Id: blk_1073743138</div><div class="line">Block belongs to: /user/hadoop/test1.txt</div><div class="line">No. of Expected Replica: 2</div><div class="line">No. of live Replica: 2</div><div class="line">No. of excess Replica: 0</div><div class="line">No. of stale Replica: 0</div><div class="line">No. of decommissioned Replica: 0</div><div class="line">No. of decommissioning Replica: 0</div><div class="line">No. of corrupted Replica: 0</div><div class="line">Block replica on datanode/rack: hadoop-slave1/default-rack is HEALTHY</div><div class="line">Block replica on datanode/rack: hadoop-slave2/default-rack is HEALTHY</div></pre></td></tr></table></figure></blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;hadoop-fsck使用方式&quot;&gt;&lt;a href=&quot;#hadoop-fsck使用方式&quot; class=&quot;headerlink&quot; title=&quot;hadoop fsck使用方式&quot;&gt;&lt;/a&gt;hadoop fsck使用方式&lt;/h4&gt;&lt;blockquote&gt;
&lt;figure 
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hdfs ec command Memo</title>
    <link href="http://yoursite.com/2018/01/03/%5Bhadoop%5Dhdfs_ec_cmd/"/>
    <id>http://yoursite.com/2018/01/03/[hadoop]hdfs_ec_cmd/</id>
    <published>2018-01-02T16:00:00.000Z</published>
    <updated>2018-01-03T08:17:51.595Z</updated>
    
    <content type="html"><![CDATA[<h4 id="hdfs-ec-使用方式-以XOR-2-1-1024k-Policy做為測試"><a href="#hdfs-ec-使用方式-以XOR-2-1-1024k-Policy做為測試" class="headerlink" title="hdfs ec 使用方式(以XOR-2-1-1024k Policy做為測試)"></a>hdfs ec 使用方式(以XOR-2-1-1024k Policy做為測試)</h4><blockquote>
<p>EC相關架構的介紹可以參考網路相關的文章<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div></pre></td><td class="code"><pre><div class="line">Usage: bin/hdfs ec [COMMAND]</div><div class="line">          [-listPolicies]</div><div class="line">          [-addPolicies -policyFile &lt;file&gt;]</div><div class="line">          [-getPolicy -path &lt;path&gt;]</div><div class="line">          [-removePolicy -policy &lt;policy&gt;]</div><div class="line">          [-setPolicy -path &lt;path&gt; [-policy &lt;policy&gt;] [-replicate]]</div><div class="line">          [-unsetPolicy -path &lt;path&gt;]</div><div class="line">          [-listCodecs]</div><div class="line">          [-enablePolicy -policy &lt;policy&gt;]</div><div class="line">          [-disablePolicy -policy &lt;policy&gt;]</div><div class="line">          [-help &lt;command-name&gt;]</div><div class="line"></div><div class="line">##列出所有Erasure Coding可用的相關Policies</div><div class="line">hadoop@hadoop-master:~$ hdfs ec -listPolicies</div><div class="line">Erasure Coding Policies:</div><div class="line">ErasureCodingPolicy=[Name=RS-10-4-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=10, numParityUnits=4]], CellSize=1048576, Id=5], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-3-2-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=3, numParityUnits=2]], CellSize=1048576, Id=2], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-LEGACY-6-3-1024k, Schema=[ECSchema=[Codec=rs-legacy, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=3], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=XOR-2-1-1024k, Schema=[ECSchema=[Codec=xor, numDataUnits=2, numParityUnits=1]], CellSize=1048576, Id=4], State=DISABLED</div><div class="line"></div><div class="line">##列出所有Erasure Coding Codec列表</div><div class="line">hadoop@hadoop-master:~$ hdfs ec -listCodecs</div><div class="line">Erasure Coding Codecs: Codec [Coder List]</div><div class="line">        RS [RS_NATIVE, RS_JAVA]</div><div class="line">        RS-LEGACY [RS-LEGACY_JAVA]</div><div class="line">        XOR [XOR_NATIVE, XOR_JAVA]</div><div class="line"></div><div class="line">##Enable XOR-2-1-1024k的EC Policy		</div><div class="line">hadoop@hadoop-master:~$ hdfs ec -enablePolicy -policy XOR-2-1-1024k</div><div class="line">Erasure coding policy XOR-2-1-1024k is enabled</div><div class="line">hadoop@hadoop-master:~$ hdfs ec -listPolicies</div><div class="line">Erasure Coding Policies:</div><div class="line">ErasureCodingPolicy=[Name=RS-10-4-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=10, numParityUnits=4]], CellSize=1048576, Id=5], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-3-2-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=3, numParityUnits=2]], CellSize=1048576, Id=2], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=RS-LEGACY-6-3-1024k, Schema=[ECSchema=[Codec=rs-legacy, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=3], State=DISABLED</div><div class="line">ErasureCodingPolicy=[Name=XOR-2-1-1024k, Schema=[ECSchema=[Codec=xor, numDataUnits=2, numParityUnits=1]], CellSize=1048576, Id=4], State=ENABLED</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir -p /user/hadoop/example/lab_2</div><div class="line"></div><div class="line">##將目錄設定為XOR-2-1-1024k的EC Policy</div><div class="line">hadoop@hadoop-master:~$ hdfs ec -setPolicy -path /user/hadoop/example/lab_2 -policy XOR-2-1-1024k</div><div class="line">Set erasure coding policy XOR-2-1-1024k on /user/hadoop/example/lab_2</div><div class="line"></div><div class="line">##上傳檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put ~/test9.tar.gz /user/hadoop/example/lab_2</div><div class="line">2018-01-03 15:41:12,678 WARN erasurecode.ErasureCodeNative: ISA-L support is not available in your platform... using builtin-java codec where applicable</div><div class="line">2018-01-03 15:41:12,741 WARN hdfs.DFSOutputStream: Cannot allocate parity block(index=2, policy=XOR-2-1-1024k). Not enough datanodes? Exclude nodes=[]</div><div class="line">2018-01-03 15:41:12,953 WARN hdfs.DFSOutputStream: Block group &lt;1&gt; has 1 corrupt blocks. It&apos;s at high risk of losing data.</div><div class="line"></div><div class="line">##觀察上傳檔案的複本數</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_2</div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   1 hadoop supergroup        373 2018-01-03 15:41 /user/hadoop/example/lab_2/test9.tar.gz</div><div class="line"></div><div class="line">##以下為使用fsck指令觀察該檔案Block相關訊息</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/example/lab_2/test9.tar.gz -files -blocks -locations</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;locations=1&amp;path=%2Fuser%2Fhadoop%2Fexample%2Flab_2%2Ftest9.tar.gz</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/example/lab_2/test9.tar.gz at Wed Jan 03 15:46:48 CST 2018</div><div class="line">/user/hadoop/example/lab_2/test9.tar.gz 373 bytes, erasure-coded: policy=XOR-2-1-1024k, 1 block(s):  OK</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_-9223372036854775776_2346 len=373 Live_repl=2  </div><div class="line">[blk_-9223372036854775776:DatanodeInfoWithStorage[192.168.51.5:9866,DS-f59711c6-801d-4ebc-b55c-228f225117b8,DISK], </div><div class="line"> blk_-9223372036854775774:DatanodeInfoWithStorage[192.168.51.6:9866,DS-482194d9-aa70-4ab8-8253-907739d5b1a1,DISK]]</div><div class="line"></div><div class="line"></div><div class="line">Status: HEALTHY</div><div class="line"> Number of data-nodes:  2</div><div class="line"> Number of racks:               1</div><div class="line"> Total dirs:                    0</div><div class="line"> Total symlinks:                0</div><div class="line"></div><div class="line">Replicated Blocks:</div><div class="line"> Total size:    0 B</div><div class="line"> Total files:   0</div><div class="line"> Total blocks (validated):      0</div><div class="line"> Minimally replicated blocks:   0</div><div class="line"> Over-replicated blocks:        0</div><div class="line"> Under-replicated blocks:       0</div><div class="line"> Mis-replicated blocks:         0</div><div class="line"> Default replication factor:    1</div><div class="line"> Average block replication:     0.0</div><div class="line"> Missing blocks:                0</div><div class="line"> Corrupt blocks:                0</div><div class="line"> Missing replicas:              0</div><div class="line"></div><div class="line">Erasure Coded Block Groups:</div><div class="line"> Total size:    373 B</div><div class="line"> Total files:   1</div><div class="line"> Total block groups (validated):        1 (avg. block group size 373 B)</div><div class="line"> Minimally erasure-coded block groups:  1 (100.0 %)</div><div class="line"> Over-erasure-coded block groups:       0 (0.0 %)</div><div class="line"> Under-erasure-coded block groups:      0 (0.0 %)</div><div class="line"> Unsatisfactory placement block groups: 0 (0.0 %)</div><div class="line"> Average block group size:      2.0</div><div class="line"> Missing block groups:          0</div><div class="line"> Corrupt block groups:          0</div><div class="line"> Missing internal blocks:       0 (0.0 %)</div><div class="line">FSCK ended at Wed Jan 03 15:46:48 CST 2018 in 2 milliseconds</div><div class="line"></div><div class="line"></div><div class="line">The filesystem under path &apos;/user/hadoop/example/lab_2/test9.tar.gz&apos; is HEALTHY</div><div class="line"></div><div class="line">##以blockId觀察複本狀況</div><div class="line">hadoop@hadoop-master:~$ hdfs fsck -blockId blk_-9223372036854775776</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;blockId=blk_-9223372036854775776+&amp;path=%2F</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 at Wed Jan 03 15:49:12 CST 2018</div><div class="line"></div><div class="line">Block Id: blk_-9223372036854775776</div><div class="line">Block belongs to: /user/hadoop/example/lab_2/test9.tar.gz</div><div class="line">No. of Expected Replica: 2</div><div class="line">No. of live Replica: 2</div><div class="line">No. of excess Replica: 0</div><div class="line">No. of stale Replica: 0</div><div class="line">No. of decommissioned Replica: 0</div><div class="line">No. of decommissioning Replica: 0</div><div class="line">No. of corrupted Replica: 0</div><div class="line">null</div><div class="line"></div><div class="line"></div><div class="line">Fsck on blockId &apos;blk_-9223372036854775776</div><div class="line"></div><div class="line">##以下為模擬shutdown 一台Datanode,資料是否還會存在??</div><div class="line">hadoop@hadoop-master:~$ hdfs dfsadmin -shutdownDatanode hadoop-slave2:9867</div><div class="line">Submitted a shutdown request to datanode hadoop-slave2:9867</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/example/lab_2/test9.tar.gz -files -blocks -locations</div><div class="line">Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;locations=1&amp;path=%2Fuser%2Fhadoop%2Fexample%2Flab_2%2Ftest9.tar.gz</div><div class="line">FSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/example/lab_2/test9.tar.gz at Wed Jan 03 16:11:01 CST 2018</div><div class="line">/user/hadoop/example/lab_2/test9.tar.gz 373 bytes, erasure-coded: policy=XOR-2-1-1024k, 1 block(s):  </div><div class="line">Under replicated BP-1139418417-192.168.51.4-1514272982687:blk_-9223372036854775776_2346. </div><div class="line">Target Replicas is 2 but found 1 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).</div><div class="line">0. BP-1139418417-192.168.51.4-1514272982687:blk_-9223372036854775776_2346 len=373 Live_repl=1  </div><div class="line">[blk_-9223372036854775776:DatanodeInfoWithStorage[192.168.51.5:9866,DS-f59711c6-801d-4ebc-b55c-228f225117b8,DISK]]</div><div class="line"></div><div class="line"></div><div class="line">Status: HEALTHY</div><div class="line"> Number of data-nodes:  1</div><div class="line"> Number of racks:               1</div><div class="line"> Total dirs:                    0</div><div class="line"> Total symlinks:                0</div><div class="line"></div><div class="line">Replicated Blocks:</div><div class="line"> Total size:    0 B</div><div class="line"> Total files:   0</div><div class="line"> Total blocks (validated):      0</div><div class="line"> Minimally replicated blocks:   0</div><div class="line"> Over-replicated blocks:        0</div><div class="line"> Under-replicated blocks:       0</div><div class="line"> Mis-replicated blocks:         0</div><div class="line"> Default replication factor:    1</div><div class="line"> Average block replication:     0.0</div><div class="line"> Missing blocks:                0</div><div class="line"> Corrupt blocks:                0</div><div class="line"> Missing replicas:              0</div><div class="line"></div><div class="line">Erasure Coded Block Groups:</div><div class="line"> Total size:    373 B</div><div class="line"> Total files:   1</div><div class="line"> Total block groups (validated):        1 (avg. block group size 373 B)</div><div class="line"> Minimally erasure-coded block groups:  1 (100.0 %)</div><div class="line"> Over-erasure-coded block groups:       0 (0.0 %)</div><div class="line"> Under-erasure-coded block groups:      1 (100.0 %)</div><div class="line"> Unsatisfactory placement block groups: 0 (0.0 %)</div><div class="line"> Average block group size:      1.0</div><div class="line"> Missing block groups:          0</div><div class="line"> Corrupt block groups:          0</div><div class="line"> Missing internal blocks:       1 (50.0 %)</div><div class="line">FSCK ended at Wed Jan 03 16:11:01 CST 2018 in 3 milliseconds</div><div class="line"></div><div class="line"></div><div class="line">The filesystem under path &apos;/user/hadoop/example/lab_2/test9.tar.gz&apos; is HEALTHY</div><div class="line"></div><div class="line">雖然複本數是設定為1,但因為使用的是EC策略,資料仍然是可下載而且資料內容也是正確的</div></pre></td></tr></table></figure></p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;hdfs-ec-使用方式-以XOR-2-1-1024k-Policy做為測試&quot;&gt;&lt;a href=&quot;#hdfs-ec-使用方式-以XOR-2-1-1024k-Policy做為測試&quot; class=&quot;headerlink&quot; title=&quot;hdfs ec 使用方式(以XO
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hdfs client command Memo</title>
    <link href="http://yoursite.com/2018/01/03/%5Bhadoop%5Dhdfs_other_cmd/"/>
    <id>http://yoursite.com/2018/01/03/[hadoop]hdfs_other_cmd/</id>
    <published>2018-01-02T16:00:00.000Z</published>
    <updated>2018-01-04T09:56:48.999Z</updated>
    
    <content type="html"><![CDATA[<h5 id="hdfs-getconf相關用法"><a href="#hdfs-getconf相關用法" class="headerlink" title="hdfs getconf相關用法"></a>hdfs getconf相關用法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">Usage:</div><div class="line">hadoop getconf</div><div class="line">        [-namenodes]            gets list of namenodes in the cluster.</div><div class="line">        [-secondaryNameNodes]   gets list of secondary namenodes in the cluster.</div><div class="line">        [-backupNodes]          gets list of backup nodes in the cluster.</div><div class="line">        [-includeFile]          gets the include file path that defines the datanodes that can join the cluster.</div><div class="line">        [-excludeFile]          gets the exclude file path that defines the datanodes that need to decommissioned.</div><div class="line">        [-nnRpcAddresses]       gets the namenode rpc addresses</div><div class="line">        [-confKey [key]]		gets a specific key from the configuration</div><div class="line">---</div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -namenodes</div><div class="line">hadoop-master</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -secondaryNameNodes</div><div class="line">hadoop-master</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -backupNodes</div><div class="line">0.0.0.0</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -nnRpcAddresses</div><div class="line">hadoop-master:8020</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -includeFile</div><div class="line">Configuration dfs.hosts is missing.</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -excludeFile</div><div class="line">Configuration dfs.hosts.exclude is missing.</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hdfs getconf -confKey &quot;fs.trash.interval&quot;</div><div class="line">1440</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hdfs-envvars相關用法"><a href="#hdfs-envvars相關用法" class="headerlink" title="hdfs envvars相關用法"></a>hdfs envvars相關用法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs envvars</div><div class="line">JAVA_HOME=&apos;/bgdt/java/jdk1.8.0_101&apos;</div><div class="line">HADOOP_HDFS_HOME=&apos;/bgdt/hadoop-3.0.0&apos;</div><div class="line">HDFS_DIR=&apos;share/hadoop/hdfs&apos;</div><div class="line">HDFS_LIB_JARS_DIR=&apos;share/hadoop/hdfs/lib&apos;</div><div class="line">HADOOP_CONF_DIR=&apos;/bgdt/hadoop-3.0.0/etc/hadoop&apos;</div><div class="line">HADOOP_TOOLS_HOME=&apos;/bgdt/hadoop-3.0.0&apos;</div><div class="line">HADOOP_TOOLS_DIR=&apos;share/hadoop/tools&apos;</div><div class="line">HADOOP_TOOLS_LIB_JARS_DIR=&apos;share/hadoop/tools/lib&apos;</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hdfs-classpath相關用法"><a href="#hdfs-classpath相關用法" class="headerlink" title="hdfs classpath相關用法"></a>hdfs classpath相關用法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs classpath</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop:/bgdt/hadoop-3.0.0/share/hadoop/common/lib/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/common/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/hdfs:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/hdfs/lib/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/hdfs/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/mapreduce/lib/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/mapreduce/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/yarn:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/yarn/lib/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/yarn/*:</div><div class="line">/bgdt/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hdfs-version相關用法"><a href="#hdfs-version相關用法" class="headerlink" title="hdfs version相關用法"></a>hdfs version相關用法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs version</div><div class="line">Hadoop 3.0.0</div><div class="line">Source code repository https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533</div><div class="line">Compiled by andrew on 2017-12-08T19:16Z</div><div class="line">Compiled with protoc 2.5.0</div><div class="line">From source with checksum 397832cb5529187dc8cd74ad54ff22</div><div class="line">This command was run using /bgdt/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hdfs-groups相關用法"><a href="#hdfs-groups相關用法" class="headerlink" title="hdfs groups相關用法"></a>hdfs groups相關用法</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hdfs groups</div><div class="line">hadoop : hadoop</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hadoop-conftest"><a href="#hadoop-conftest" class="headerlink" title="hadoop conftest"></a>hadoop conftest</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop conftest</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/hadoop-policy.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/yarn-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/hdfs-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/core-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/mapred-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/capacity-scheduler.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/kms-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/httpfs-site.xml: valid</div><div class="line">/bgdt/hadoop-3.0.0/etc/hadoop/kms-acls.xml: valid</div><div class="line">OK</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hadoop-jnipath"><a href="#hadoop-jnipath" class="headerlink" title="hadoop jnipath"></a>hadoop jnipath</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop jnipath</div><div class="line">/bgdt/hadoop-3.0.0/lib/native</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="hadoop-checknative"><a href="#hadoop-checknative" class="headerlink" title="hadoop checknative"></a>hadoop checknative</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop checknative</div><div class="line">2018-01-04 17:18:04,659 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native</div><div class="line">2018-01-04 17:18:04,667 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</div><div class="line">2018-01-04 17:18:04,682 ERROR snappy.SnappyCompressor: failed to load SnappyCompressor</div><div class="line">java.lang.UnsatisfiedLinkError: Cannot load libsnappy.so.1 (libsnappy.so.1: cannot open shared object file: No such file or directory)!</div><div class="line">        at org.apache.hadoop.io.compress.snappy.SnappyCompressor.initIDs(Native Method)</div><div class="line">        at org.apache.hadoop.io.compress.snappy.SnappyCompressor.&lt;clinit&gt;(SnappyCompressor.java:57)</div><div class="line">        at org.apache.hadoop.io.compress.SnappyCodec.isNativeCodeLoaded(SnappyCodec.java:82)</div><div class="line">        at org.apache.hadoop.util.NativeLibraryChecker.main(NativeLibraryChecker.java:100)</div><div class="line">2018-01-04 17:18:04,691 WARN erasurecode.ErasureCodeNative: ISA-L support is not available in your platform... using builtin-java codec where applicable</div><div class="line">Native library checking:</div><div class="line">hadoop:  true /bgdt/hadoop-3.0.0/lib/native/libhadoop.so.1.0.0</div><div class="line">zlib:    true /lib/x86_64-linux-gnu/libz.so.1</div><div class="line">zstd  :  false</div><div class="line">snappy:  false</div><div class="line">lz4:     true revision:10301</div><div class="line">bzip2:   true /lib/x86_64-linux-gnu/libbz2.so.1</div><div class="line">openssl: false Cannot load libcrypto.so (libcrypto.so: cannot open shared object file: No such file or directory)!</div><div class="line">ISA-L:   false libhadoop was built without ISA-L support</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="hadoop-daemonlog"><a href="#hadoop-daemonlog" class="headerlink" title="hadoop daemonlog"></a>hadoop daemonlog</h6><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">##取得Class Log Level級別</div><div class="line">hadoop@hadoop-master:~$ hadoop daemonlog -getlevel hadoop-master:9870 org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Connecting to http://hadoop-master:9870/logLevel?log=org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Submitted Class Name: org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Log Class: org.apache.commons.logging.impl.Log4JLogger</div><div class="line">Effective Level: INFO</div><div class="line"></div><div class="line">##設定Class Log Level級別</div><div class="line">hadoop@hadoop-master:~$ hadoop daemonlog -setlevel  hadoop-master:9870 org.apache.hadoop.yarn.server.nodemanager.NodeManager DEBUG</div><div class="line">Connecting to http://hadoop-master:9870/logLevel?log=org.apache.hadoop.yarn.server.nodemanager.NodeManager&amp;level=DEBUG</div><div class="line">Submitted Class Name: org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Log Class: org.apache.commons.logging.impl.Log4JLogger</div><div class="line">Submitted Level: DEBUG</div><div class="line">Setting Level to DEBUG ...</div><div class="line">Effective Level: DEBUG</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop daemonlog -getlevel hadoop-master:9870 org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Connecting to http://hadoop-master:9870/logLevel?log=org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Submitted Class Name: org.apache.hadoop.yarn.server.nodemanager.NodeManager</div><div class="line">Log Class: org.apache.commons.logging.impl.Log4JLogger</div><div class="line">Effective Level: DEBUG</div></pre></td></tr></table></figure></blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;hdfs-getconf相關用法&quot;&gt;&lt;a href=&quot;#hdfs-getconf相關用法&quot; class=&quot;headerlink&quot; title=&quot;hdfs getconf相關用法&quot;&gt;&lt;/a&gt;hdfs getconf相關用法&lt;/h5&gt;&lt;blockquote&gt;
&lt;fig
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>hadoop fs command Memo</title>
    <link href="http://yoursite.com/2018/01/02/%5Bhadoop%5Dhadoop_fs_cmd/"/>
    <id>http://yoursite.com/2018/01/02/[hadoop]hadoop_fs_cmd/</id>
    <published>2018-01-01T16:00:00.000Z</published>
    <updated>2018-01-03T05:00:48.798Z</updated>
    
    <content type="html"><![CDATA[<h4 id="建立與刪除目錄"><a href="#建立與刪除目錄" class="headerlink" title="建立與刪除目錄"></a>建立與刪除目錄</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">##建立目錄但不會將上層目錄一併建立完成</div><div class="line">hadoop@hadoop-master:~$hadoop fs -mkdir /user/hadoop/example/lab_1</div><div class="line">mkdir: `/user/hadoop/example/lab_1&apos;: No such file or directory</div><div class="line"></div><div class="line">##建立目錄,會連同上層目錄一併建立</div><div class="line">hadoop@hadoop-master:~$hadoop fs -mkdir -p /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$hadoop fs ls -d /user/hadoop/example</div><div class="line"></div><div class="line">##刪除一個空目錄</div><div class="line">hadoop@hadoop-master:~$hadoop fs -rmdir /user/hadoop/example/lab_1</div><div class="line"></div><div class="line">##透過std in 建立一個test1.txt檔案</div><div class="line">hadoop@hadoop-master:~$hadoop fs -put -f - /user/hadoop/example/test1.txt</div><div class="line"></div><div class="line">##無法刪除,目錄下有檔案</div><div class="line">hadoop@hadoop-master:~$hadoop fs -rmdir  /user/hadoop/example</div><div class="line">&quot;rmdir: `/user/hadoop/example&apos;: Directory is not empty&quot;</div><div class="line"></div><div class="line">##將所有檔案刪除完成後,刪除目錄</div><div class="line">hadoop@hadoop-master:~$hadoop fs -rm -r -f  /user/hadoop/example			</div><div class="line"></div><div class="line">##建立目錄</div><div class="line">hadoop@hadoop-master:~$hadoop fs -mkdir -p /user/hadoop/example/lab_1</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="檔案建立-複製-刪除-搬移"><a href="#檔案建立-複製-刪除-搬移" class="headerlink" title="檔案建立/複製/刪除/搬移"></a>檔案建立/複製/刪除/搬移</h4><h5 id="put-get操作檔案"><a href="#put-get操作檔案" class="headerlink" title="put/get操作檔案"></a>put/get操作檔案</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$rm -rf  ~/get_hdfs_test2.txt</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$dd if=/dev/urandom of=test2.txt bs=1K count=1</div><div class="line">1+0 records in</div><div class="line">1+0 records out</div><div class="line">1024 bytes (1.0 kB, 1.0 KiB) copied, 0.00234966 s, 436 kB/s</div><div class="line"></div><div class="line">##使用PUT上傳檔案到HDFS</div><div class="line">hadoop@hadoop-master:~$hadoop fs -put ~/test2.txt /user/hadoop/example/</div><div class="line">hadoop@hadoop-master:~$hadoop fs -ls /user/hadoop/example</div><div class="line">Found 2 items</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 15:40 /user/hadoop/example/lab_1</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/test2.txt</div><div class="line"></div><div class="line">##使用GET下載檔案到Local</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -get /user/hadoop/example/test2.txt ~/get_hdfs_test2.txt</div><div class="line">hadoop@hadoop-master:~$ ls -la ~/get*</div><div class="line">-rw-r--r-- 1 hadoop hadoop 1024 Jan  2 15:44 /home/hadoop/get_hdfs_test2.txt</div><div class="line"></div><div class="line"></div><div class="line">##使用getmerge結合數個檔案內容,並下載至Local</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/lab_1/test1.txt</div><div class="line">1,&quot;test_001&quot;,10000</div><div class="line">2,&quot;test_002&quot;,20000</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/lab_1/test2.txt</div><div class="line">3,&quot;test_003&quot;,30000</div><div class="line">4,&quot;test_004&quot;,40000</div><div class="line">hadoop@hadoop-master:~$ rm -rf ~/merge_file.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -getmerge -nl /user/hadoop/example/lab_1/* ~/merge_file.txt</div><div class="line">hadoop@hadoop-master:~$ ls -la ~/merge_file.txt</div><div class="line">-rw-r--r-- 1 hadoop hadoop 78 Jan  2 15:53 /home/hadoop/merge_file.txt</div><div class="line">hadoop@hadoop-master:~$ cat ~/merge_file.txt</div><div class="line">1,&quot;test_001&quot;,10000</div><div class="line">2,&quot;test_002&quot;,20000</div><div class="line"></div><div class="line">3,&quot;test_003&quot;,30000</div><div class="line">4,&quot;test_004&quot;,40000</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="copy-move操作檔案"><a href="#copy-move操作檔案" class="headerlink" title="copy/move操作檔案"></a>copy/move操作檔案</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div></pre></td><td class="code"><pre><div class="line">## copyfromlocal/copytolocal操作</div><div class="line">hadoop@hadoop-master:~$ dd if=/dev/urandom of=test3.txt bs=1K count=1</div><div class="line">1+0 records in</div><div class="line">1+0 records out</div><div class="line">1024 bytes (1.0 kB, 1.0 KiB) copied, 0.00109903 s, 932 kB/s</div><div class="line">hadoop@hadoop-master:~$ rm -rf ~/cp_hdfs_tolocal_test3.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -copyFromLocal ~/test3.txt /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 3 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -copyToLocal  /user/hadoop/example/lab_1/test3.txt ~/cp_hdfs_tolocal_test3.txt</div><div class="line">hadoop@hadoop-master:~$ ls -la ~/cp_hdfs_tolocal_test3.txt</div><div class="line">-rw-r--r-- 1 hadoop hadoop 1024 Jan  2 16:01 /home/hadoop/cp_hdfs_tolocal_test3.txt</div><div class="line">hadoop@hadoop-master:~$</div><div class="line"></div><div class="line">## cp操作(同一個HDFS中資料複製)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 3 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cp /user/hadoop/example/test2.txt  /user/hadoop/example/lab_1/test4.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 4 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example</div><div class="line">Found 2 items</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 16:14 /user/hadoop/example/lab_1</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/test2.txt</div><div class="line"></div><div class="line">## cp操作(不同HDFS中資料複製)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cp hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test5.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 5 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test5.txt</div><div class="line">id,end_date,start_date,location</div><div class="line">1,2015-10-14 00:00:00,2015-09-14 00:00:00,CA-SF</div><div class="line">2,2015-10-15 01:00:20,2015-08-14 00:00:00,CA-SD</div><div class="line">3,2015-10-16 02:30:00,2015-01-14 00:00:00,NY-NY</div><div class="line">4,2015-10-17 03:00:20,2015-02-14 00:00:00,NY-NY</div><div class="line">5,2015-10-18 04:30:00,2014-04-14 00:00:00,CA-SD</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls hdfs://172.20.22.95:8020/user/hadoop/tmp</div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2017-11-22 17:09 hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv</div><div class="line"></div><div class="line">## moveFromLocal/moveToLocal</div><div class="line">hadoop@hadoop-master:~$ dd if=/dev/urandom of=test6.txt bs=1K count=1</div><div class="line">1+0 records in</div><div class="line">1+0 records out</div><div class="line">1024 bytes (1.0 kB, 1.0 KiB) copied, 0.00116886 s, 876 kB/s</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -moveFromLocal ~/test6.txt /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 6 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -moveToLocal /user/hadoop/example/test6.txt ~/mv_hdfs_tolocal_test6.txt</div><div class="line">moveToLocal: Option &apos;-moveToLocal&apos; is not implemented yet.</div><div class="line"></div><div class="line">## mv操作(同一個HDFS中資料複製),被搬移之後檔案會不見</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 6 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mv /user/hadoop/example/test2.txt /user/hadoop/example/lab_1/test7.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 7 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example</div><div class="line">Found 1 items</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 16:21 /user/hadoop/example/lab_1</div><div class="line"></div><div class="line">## mv操作(不同一個HDFS中資料複製),如果hadoop版本不一致無法使用mv搬移檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls hdfs://172.20.22.95:8020/user/hadoop/tmp</div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2017-11-22 17:09 hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls hdfs://192.168.51.4:8020/user/hadoop/example/lab_1</div><div class="line">Found 7 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test7.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mv hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test8.txt</div><div class="line">mv: `hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv&apos;: Does not match target filesystem</div><div class="line">hadoop@hadoop-master:~$  hadoop fs -cp hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv  hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test8.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls hdfs://192.168.51.4:8020/user/hadoop/example/lab_1</div><div class="line">Found 8 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test8.txt</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="查看檔案內容"><a href="#查看檔案內容" class="headerlink" title="查看檔案內容"></a>查看檔案內容</h4><h5 id="檢視壓縮檔案內容-tar-gz"><a href="#檢視壓縮檔案內容-tar-gz" class="headerlink" title="檢視壓縮檔案內容(.tar.gz)"></a>檢視壓縮檔案內容(.tar.gz)</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ tar -zcvf ~/test9.tar.gz &quot;test1.csv&quot; &quot;test2.csv&quot; &quot;test3.csv&quot;</div><div class="line">test1.csv</div><div class="line">test2.csv</div><div class="line">test3.csv</div><div class="line">hadoop@hadoop-master:~$ tar -ztvf ~/test9.tar.gz</div><div class="line">-rw-rw-r-- hadoop/hadoop   272 2017-11-08 15:49 test1.csv</div><div class="line">-rw-rw-r-- hadoop/hadoop   300 2017-11-06 19:14 test2.csv</div><div class="line">-rw-rw-r-- hadoop/hadoop   422 2017-08-21 16:30 test3.csv</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -put ~/test9.tar.gz /user/hadoop/example/lab_1/</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 9 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">##可以使用hadoop fs -text來查看壓縮檔案內容</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -text /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line">test1.csv0000664000175100017510000000042013200533434012136 0ustar  hadoophadoop100,1,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.1&quot;</div><div class="line">200,2,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.2&quot;</div><div class="line">150,3,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.3&quot;</div><div class="line">300,4,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.4&quot;</div><div class="line">..........</div><div class="line">5,&quot;http://www.google.com&quot;;&quot;192.168.1.5&quot;;&quot;201&quot;,&quot;005&quot;;&quot;test1_005&quot;;&quot;500&quot;</div><div class="line">6,&quot;http://www.google.com&quot;;&quot;192.168.1.6&quot;;&quot;403&quot;,&quot;006&quot;;&quot;test1_006&quot;;&quot;1600&quot;</div><div class="line"></div><div class="line">##hadoop fs -cat查看壓縮檔時,會出現亂碼,需要配合zcat指令來查看</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test9.tar.gz | zcat</div><div class="line">test1.csv0000664000175100017510000000042013200533434012136 0ustar  hadoophadoop100,1,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.1&quot;</div><div class="line">200,2,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.2&quot;</div><div class="line">150,3,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.3&quot;</div><div class="line">300,4,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.4&quot;</div><div class="line">..........</div><div class="line">5,&quot;http://www.google.com&quot;;&quot;192.168.1.5&quot;;&quot;201&quot;,&quot;005&quot;;&quot;test1_005&quot;;&quot;500&quot;</div><div class="line">6,&quot;http://www.google.com&quot;;&quot;192.168.1.6&quot;;&quot;403&quot;,&quot;006&quot;;&quot;test1_006&quot;;&quot;1600&quot;</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="檢視檔案內容-且資料append到檔案時-會馬上呈現"><a href="#檢視檔案內容-且資料append到檔案時-會馬上呈現" class="headerlink" title="檢視檔案內容,且資料append到檔案時,會馬上呈現"></a>檢視檔案內容,且資料append到檔案時,會馬上呈現</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">##hadoop fs -tail指令的功能(與cat/text相同可以觀察檔案內容)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -tail /user/hadoop/example/lab_1/test1.txt</div><div class="line">1,&quot;test_001&quot;,10000</div><div class="line">2,&quot;test_002&quot;,20000</div></pre></td></tr></table></figure>
</blockquote>
<h6 id="以下使用圖片說明"><a href="#以下使用圖片說明" class="headerlink" title="以下使用圖片說明"></a>以下使用圖片說明</h6><blockquote>
<p>hadoop fs -tail時,可以查看檔案完整內容<br><img src="/images/hadoop_fs_cmd/tail_1.jpg" alt=""><br>hadoop fs -tail -f 時,持續呈現檔案Append的相關內容<br><img src="/images/hadoop_fs_cmd/tail_2_1.jpg" alt=""><br><img src="/images/hadoop_fs_cmd/tail_2_2.jpg" alt=""></p>
</blockquote>
<h4 id="建立空檔案vs-清空檔案內容"><a href="#建立空檔案vs-清空檔案內容" class="headerlink" title="建立空檔案vs.清空檔案內容"></a>建立空檔案vs.清空檔案內容</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line">##使用 -touchz建立一個空的檔案,常用於建立時戳或者flag檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 9 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         76 2018-01-02 17:28 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -touchz /user/hadoop/example/lab_1/test10.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 10 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         76 2018-01-02 17:28 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test1.txt</div><div class="line">1,&quot;test_001&quot;,10000</div><div class="line">2,&quot;test_002&quot;,20000</div><div class="line">3,&quot;test_003&quot;,30000</div><div class="line">4,&quot;test_004&quot;,40000</div><div class="line"></div><div class="line">## -truncate -w 10,只留10 bytes資料</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -truncate -w 10 /user/hadoop/example/lab_1/test1.txt</div><div class="line">Waiting for /user/hadoop/example/lab_1/test1.txt ...</div><div class="line">Truncated /user/hadoop/example/lab_1/test1.txt to length: 10</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test1.txt</div><div class="line">1,&quot;test_00</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         10 2018-01-02 17:42 /user/hadoop/example/lab_1/test1.txt</div><div class="line"></div><div class="line">## -truncate -w 0,資料全部清空</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -truncate -w 0 /user/hadoop/example/lab_1/test1.txt</div><div class="line">Truncated /user/hadoop/example/lab_1/test1.txt to length: 0</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test1.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-find找檔案"><a href="#使用hadoop-fs-find找檔案" class="headerlink" title="使用hadoop fs -find找檔案"></a>使用hadoop fs -find找檔案</h4><blockquote>
<p>此部分可以參考Linux find指令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">說明:</div><div class="line">-iname pattern 所要查找的文檔名，不區分大小寫</div><div class="line">-name pattern  所要查找的文檔名，區分大小寫</div><div class="line">-print 換行列印</div><div class="line">-print0 連續列印</div><div class="line"></div><div class="line">##-name後,可接欲找尋的檔案名稱,&quot;*&quot;代表任何值</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -find / -name &quot;test*.txt&quot; -print</div><div class="line">/user/hadoop/example/lab_1/test1.txt</div><div class="line">/user/hadoop/example/lab_1/test10.txt</div><div class="line">/user/hadoop/example/lab_1/test2.txt</div><div class="line">/user/hadoop/example/lab_1/test3.txt</div><div class="line">/user/hadoop/example/lab_1/test4.txt</div><div class="line">/user/hadoop/example/lab_1/test5.txt</div><div class="line">/user/hadoop/example/lab_1/test6.txt</div><div class="line">/user/hadoop/example/lab_1/test7.txt</div><div class="line">/user/hadoop/example/lab_1/test8.txt</div><div class="line">/user/hadoop/test1.txt</div><div class="line">##-name後,可接正規式的相關參數</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -find / -name &quot;[A-Z,a-z]*.db&quot; -print</div><div class="line">/user/hive/warehouse/test1.db</div><div class="line">/user/hive/warehouse/test2.db</div><div class="line">/user/hive/warehouse/test3.db</div><div class="line">/user/hive/warehouse/test4.db</div><div class="line">/user/hive/warehouse/test5.db</div><div class="line">/user/hive/warehouse/test6.db</div></pre></td></tr></table></figure></p>
</blockquote>
<h4 id="使用hadoop-fs-df-du查看檔案目錄的大小"><a href="#使用hadoop-fs-df-du查看檔案目錄的大小" class="headerlink" title="使用hadoop fs -df/-du查看檔案目錄的大小"></a>使用hadoop fs -df/-du查看檔案目錄的大小</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">##後面如果都沒接任何參數,看各個檔案的大小(以byte為單位)及檔名</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -du /user/hadoop/example/lab_1</div><div class="line">0     0     /user/hadoop/example/lab_1/test1.txt</div><div class="line">0     0     /user/hadoop/example/lab_1/test10.txt</div><div class="line">38    76    /user/hadoop/example/lab_1/test2.txt</div><div class="line">1024  2048  /user/hadoop/example/lab_1/test3.txt</div><div class="line">1024  2048  /user/hadoop/example/lab_1/test4.txt</div><div class="line">272   544   /user/hadoop/example/lab_1/test5.txt</div><div class="line">1024  2048  /user/hadoop/example/lab_1/test6.txt</div><div class="line">1024  2048  /user/hadoop/example/lab_1/test7.txt</div><div class="line">272   544   /user/hadoop/example/lab_1/test8.txt</div><div class="line">373   746   /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">##加入&quot;-h&quot;,在顯示檔案大小時,會將單位標示出來</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -du -h /user/hadoop/example/lab_1</div><div class="line">0    0    /user/hadoop/example/lab_1/test1.txt</div><div class="line">0    0    /user/hadoop/example/lab_1/test10.txt</div><div class="line">38   76   /user/hadoop/example/lab_1/test2.txt</div><div class="line">1 K  2 K  /user/hadoop/example/lab_1/test3.txt</div><div class="line">1 K  2 K  /user/hadoop/example/lab_1/test4.txt</div><div class="line">272  544  /user/hadoop/example/lab_1/test5.txt</div><div class="line">1 K  2 K  /user/hadoop/example/lab_1/test6.txt</div><div class="line">1 K  2 K  /user/hadoop/example/lab_1/test7.txt</div><div class="line">272  544  /user/hadoop/example/lab_1/test8.txt</div><div class="line">373  746  /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">##加入&quot;-s&quot;,最主要是用來顯示該目錄下所有檔案的summary後的大小</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -du -h -s /user/hadoop/example/lab_1</div><div class="line">4.9 K  9.9 K  /user/hadoop/example/lab_1</div><div class="line"></div><div class="line">##hadoop fs -du 加入&quot;-v&quot;,可以看到每個欄位的標題</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -du -h -v  /user/hadoop/example/lab_1</div><div class="line">SIZE  DISK_SPACE_CONSUMED_WITH_ALL_REPLICAS  FULL_PATH_NAME</div><div class="line">0     0                                      /user/hadoop/example/lab_1/test1.txt</div><div class="line">0     0                                      /user/hadoop/example/lab_1/test10.txt</div><div class="line">38    76                                     /user/hadoop/example/lab_1/test2.txt</div><div class="line">1 K   2 K                                    /user/hadoop/example/lab_1/test3.txt</div><div class="line">1 K   2 K                                    /user/hadoop/example/lab_1/test4.txt</div><div class="line">272   544                                    /user/hadoop/example/lab_1/test5.txt</div><div class="line">1 K   2 K                                    /user/hadoop/example/lab_1/test6.txt</div><div class="line">1 K   2 K                                    /user/hadoop/example/lab_1/test7.txt</div><div class="line">272   544                                    /user/hadoop/example/lab_1/test8.txt</div><div class="line">373   746                                    /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">## hadoop fs -df 顯示HDFS系統(多個)剩餘空間</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -df hdfs://172.20.22.95:8020 hdfs://192.168.51.4:8020</div><div class="line">Filesystem                         Size         Used      Available  Use%</div><div class="line">hdfs://172.20.22.95:8020  2232839094272  29245636608  1911548899328    1%</div><div class="line">hdfs://192.168.51.4:8020    42002972672     47734784    26333745152    0%</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-ls查看檔案目錄相關資訊"><a href="#使用hadoop-fs-ls查看檔案目錄相關資訊" class="headerlink" title="使用hadoop fs -ls查看檔案目錄相關資訊"></a>使用hadoop fs -ls查看檔案目錄相關資訊</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">##</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example</div><div class="line">Found 2 items</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1</div><div class="line">-rw-r--r--   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">##-r  Reverse the order of the sort</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -r /user/hadoop/example</div><div class="line">Found 2 items</div><div class="line">-rw-r--r--   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1</div><div class="line"></div><div class="line">## 檢視&quot;-R&quot;遞回目錄下所有檔案及目錄</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R /user/hadoop/example</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line">-rw-r--r--   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">## &quot;-d&quot;只顯示目前只定的檔案或目錄</div><div class="line">hadoop fs -ls -d /user/hadoop/example</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-02 18:29 /user/hadoop/example</div><div class="line"></div><div class="line">## &quot;-e&quot; Display the erasure coding policy of files and directories.</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -e /user/hadoop/example/lab_1</div><div class="line">Found 10 items</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2  hadoop supergroup Replicated        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-count查看檔案目錄相關資訊"><a href="#使用hadoop-fs-count查看檔案目錄相關資訊" class="headerlink" title="使用hadoop fs -count查看檔案目錄相關資訊"></a>使用hadoop fs -count查看檔案目錄相關資訊</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">##計算指定的目錄或檔案的數量大小</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count &quot;/user/hadoop/example/*&quot;</div><div class="line">           1           10               5051 /user/hadoop/example/lab_1</div><div class="line">           0            1                 36 /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">## &quot;-q&quot;選項會多出以下欄位</div><div class="line">## QUOTA,REM_QUOTA,SPACE_QUOTA,REM_SPACE_QUOTA,DIR_COUNT,FILE_COUNT,CONTENT_SIZE,PATHNAME	   </div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -q &quot;/user/hadoop/example/*&quot;</div><div class="line">        none             inf            none             inf            1           10               5051 /user/hadoop/example/lab_1</div><div class="line">        none             inf            none             inf            0            1                 36 /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">## &quot;-u&quot; option shows the quota and the usage against the quota without the detailed content summary</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -u &quot;/user/hadoop/example/*&quot;</div><div class="line">        none             inf            none             inf /user/hadoop/example/lab_1</div><div class="line">        none             inf            none             inf /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">## &quot;-e&quot; option shows the erasure coding policy.</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -count -e &quot;/user/hadoop/example/*&quot;</div><div class="line">           1           10               5051 EC: /user/hadoop/example/lab_1</div><div class="line">           0            1                 36 Replicated /user/hadoop/example/test.txt</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-stat查看檔案目錄相關資訊"><a href="#使用hadoop-fs-stat查看檔案目錄相關資訊" class="headerlink" title="使用hadoop fs -stat查看檔案目錄相關資訊"></a>使用hadoop fs -stat查看檔案目錄相關資訊</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">##相關-stat Format說明</div><div class="line">%a: octal</div><div class="line">%A: symbolic</div><div class="line">%b: filesize in bytes</div><div class="line">%F: type</div><div class="line">%g: group name of owner</div><div class="line">%n: name</div><div class="line">%o: block size</div><div class="line">%r: replication</div><div class="line">%u: user name of owner</div><div class="line">%x,%X :access date</div><div class="line">%y,%Y :modification date</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -stat /user/*</div><div class="line">2018-01-02 09:05:22</div><div class="line">2017-12-26 10:44:41</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -stat &quot;%A,perm:%a,type:%F,%u:%g,size:%b,block size:%o,%r,mtime:%y atime:%x name:%n&quot; &quot;/user/hadoop/example/lab_1/*&quot;</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:0,block size:67108864,2,mtime:2018-01-02 09:45:22 atime:2018-01-02 09:18:30 name:test1.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:0,block size:67108864,2,mtime:2018-01-02 09:38:21 atime:2018-01-02 09:38:21 name:test10.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:38,block size:67108864,2,mtime:2018-01-02 07:51:41 atime:2018-01-02 07:51:16 name:test2.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:1024,block size:67108864,2,mtime:2018-01-02 07:59:28 atime:2018-01-02 07:59:28 name:test3.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:1024,block size:67108864,2,mtime:2018-01-02 08:07:12 atime:2018-01-02 08:07:12 name:test4.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:272,block size:67108864,2,mtime:2018-01-02 08:11:21 atime:2018-01-02 08:11:21 name:test5.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:1024,block size:67108864,2,mtime:2018-01-02 08:14:24 atime:2018-01-02 08:14:23 name:test6.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:1024,block size:67108864,2,mtime:2018-01-02 07:40:46 atime:2018-01-02 07:40:46 name:test7.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:272,block size:67108864,2,mtime:2018-01-02 08:31:28 atime:2018-01-02 08:31:28 name:test8.txt</div><div class="line">rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:373,block size:67108864,2,mtime:2018-01-02 09:05:38 atime:2018-01-02 09:05:37 name:test9.tar.gz</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-test檔案測試"><a href="#使用hadoop-fs-test檔案測試" class="headerlink" title="使用hadoop fs -test檔案測試"></a>使用hadoop fs -test檔案測試</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line">##相關參數說明:</div><div class="line">-d  return 0 if &lt;path&gt; is a directory.</div><div class="line">-e  return 0 if &lt;path&gt; exists.</div><div class="line">-f  return 0 if &lt;path&gt; is a file.</div><div class="line">-s  return 0 if file &lt;path&gt; is greater than zero bytes in size.</div><div class="line">-w  return 0 if file &lt;path&gt; exists and write permission is granted.</div><div class="line">-r  return 0 if file &lt;path&gt; exists and read permission is granted.</div><div class="line">-z  return 0 if file &lt;path&gt; is zero bytes in size, else return 1.</div><div class="line">---</div><div class="line"></div><div class="line">## &quot;-e&quot; 判斷指定的路徑是否存在(0:是,1:否)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -e /user/hadoop</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -e /user/vagrant</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line"></div><div class="line">## &quot;-d&quot; 判斷是否為目錄(0:是,1:否)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -d /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -d /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line"></div><div class="line"></div><div class="line">## &quot;-f&quot; 判斷是否為目錄(0:是,1:否)</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -f /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -f /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line"></div><div class="line">## &quot;-w&quot;判斷檔案是否有寫的權限</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/test.txt</div><div class="line">-rwxr-xr-x   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -w  /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line">hadoop@hadoop-master:~$ su - popal</div><div class="line">Password:</div><div class="line">popal@hadoop-master:~$ /bgdt/hadoop-3.0.0/bin/hadoop fs -test -w  /user/hadoop/example/test.txt</div><div class="line">popal@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -chmod 766 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/test.txt</div><div class="line">-rwxrw-rw-   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ su - popal</div><div class="line">Password:</div><div class="line">popal@hadoop-master:~$ /bgdt/hadoop-3.0.0/bin/hadoop fs -test -w /user/hadoop/example/test.txt</div><div class="line">popal@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line"></div><div class="line">## &quot;-r&quot;判斷檔案是否有讀的權限</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/test.txt</div><div class="line">-rwxrw-rw-   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ su - popal</div><div class="line">Password:</div><div class="line">popal@hadoop-master:~$ /bgdt/hadoop-3.0.0/bin/hadoop fs -test -r /user/hadoop/example/test.txt</div><div class="line">popal@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line">popal@hadoop-master:~$ exit</div><div class="line">logout</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -chmod 700 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/test.txt</div><div class="line">-rwx------   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line">hadoop@hadoop-master:~$ su - popal</div><div class="line">Password:</div><div class="line">popal@hadoop-master:~$ /bgdt/hadoop-3.0.0/bin/hadoop fs -test -r /user/hadoop/example/test.txt</div><div class="line">popal@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line"></div><div class="line">## &quot;-z&quot;判斷檔案是否為0 bytes</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 10 items</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -z /user/hadoop/example/lab_1/test1.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -z /user/hadoop/example/lab_1/test2.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line"></div><div class="line">## &quot;-s&quot;判斷檔案是否不為0 bytes</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -s /user/hadoop/example/lab_1/test1.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -test -s /user/hadoop/example/lab_1/test2.txt</div><div class="line">hadoop@hadoop-master:~$ echo $?</div><div class="line">0</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-setrep-設定檔案複本數"><a href="#使用hadoop-fs-setrep-設定檔案複本數" class="headerlink" title="使用hadoop fs -setrep 設定檔案複本數"></a>使用hadoop fs -setrep 設定檔案複本數</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 10 items</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -setrep -w 1 /user/hadoop/example/lab_1/test1.txt</div><div class="line">Replication 1 set: /user/hadoop/example/lab_1/test1.txt</div><div class="line">Waiting for /user/hadoop/example/lab_1/test1.txt ... done</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">Found 10 items</div><div class="line">-rw-r--r--   1 hadoop supergroup          0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup         38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup       1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt</div><div class="line">-rw-r--r--   2 hadoop supergroup        373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="使用hadoop-fs-rm刪除檔案"><a href="#使用hadoop-fs-rm刪除檔案" class="headerlink" title="使用hadoop fs -rm刪除檔案"></a>使用hadoop fs -rm刪除檔案</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line">## hadoop fs -rm指令,只能刪檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rm  /user/hadoop/example/lab_1/*</div><div class="line">Deleted /user/hadoop/example/lab_1/test1.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test10.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test2.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test3.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test4.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test5.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test6.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test7.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test8.txt</div><div class="line">Deleted /user/hadoop/example/lab_1/test9.tar.gz</div><div class="line"></div><div class="line">## &quot;-rm&quot;無法刪目錄,如要刪除目錄需要用&quot;-rmdir or -rm -r&quot;</div><div class="line">hadoop fs -rm  /user/hadoop/example</div><div class="line">rm: `/user/hadoop/example&apos;: Is a directory</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rmdir  /user/hadoop/example</div><div class="line">rmdir: `/user/hadoop/example&apos;: Directory is not empty</div><div class="line"></div><div class="line">## &quot;-rmdir&quot;只能刪除空目錄</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rmdir /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1</div><div class="line">ls: `/user/hadoop/example/lab_1&apos;: No such file or directory</div><div class="line"></div><div class="line">## &quot;-rm -r&quot;,可以刪除所有檔案</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -mkdir /user/hadoop/example/lab_1</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -touchz /user/hadoop/example/lab_1/test1.txt</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rmdir /user/hadoop/example/lab_1</div><div class="line">rmdir: `/user/hadoop/example/lab_1&apos;: Directory is not empty</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rm /user/hadoop/example/lab_1</div><div class="line">rm: `/user/hadoop/example/lab_1&apos;: Is a directory</div><div class="line"></div><div class="line">## &quot;-rm -r&quot;,會將刪除的檔案或目錄先丟到垃圾桶</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rm -r /user/hadoop/example/lab_1</div><div class="line">2018-01-03 11:38:37,066 INFO fs.TrashPolicyDefault: </div><div class="line">Moved: &apos;hdfs://hadoop-master:8020/user/hadoop/example/lab_1&apos; to trash </div><div class="line">at: hdfs://hadoop-master:8020/user/hadoop/.Trash/Current/user/hadoop/example/lab_1</div><div class="line"></div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example</div><div class="line">Found 1 items</div><div class="line">-rwx------   2 hadoop supergroup         36 2018-01-02 18:29 /user/hadoop/example/test.txt</div><div class="line"></div><div class="line">##檢查垃圾桶</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -ls -R /user/hadoop/.Trash</div><div class="line">drwx------   - hadoop supergroup          0 2018-01-03 11:38 /user/hadoop/.Trash/Current</div><div class="line">drwx------   - hadoop supergroup          0 2018-01-03 11:38 /user/hadoop/.Trash/Current/user</div><div class="line">drwx------   - hadoop supergroup          0 2018-01-03 11:38 /user/hadoop/.Trash/Current/user/hadoop</div><div class="line">drwx------   - hadoop supergroup          0 2018-01-03 11:38 /user/hadoop/.Trash/Current/user/hadoop/example</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2018-01-03 11:37 /user/hadoop/.Trash/Current/user/hadoop/example/lab_1</div><div class="line">-rw-r--r--   2 hadoop supergroup          0 2018-01-03 11:37 /user/hadoop/.Trash/Current/user/hadoop/example/lab_1/test1.txt</div><div class="line"></div><div class="line">## &quot;-rm -r -skipTrash&quot;,會直接刪除目錄或檔案,並且不會進垃圾桶</div><div class="line">hadoop@hadoop-master:~$ hadoop fs -rm -r -skipTrash /user/hadoop/example/lab_1</div><div class="line">Deleted /user/hadoop/example/lab_1</div></pre></td></tr></table></figure>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;建立與刪除目錄&quot;&gt;&lt;a href=&quot;#建立與刪除目錄&quot; class=&quot;headerlink&quot; title=&quot;建立與刪除目錄&quot;&gt;&lt;/a&gt;建立與刪除目錄&lt;/h4&gt;&lt;blockquote&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>Hadoop權限操作說明</title>
    <link href="http://yoursite.com/2018/01/02/%5Bhadoop%5DHadoop_permission_memo/"/>
    <id>http://yoursite.com/2018/01/02/[hadoop]Hadoop_permission_memo/</id>
    <published>2018-01-01T16:00:00.000Z</published>
    <updated>2018-01-02T06:37:50.373Z</updated>
    
    <content type="html"><![CDATA[<h4 id="使用到的HDFS語法整理"><a href="#使用到的HDFS語法整理" class="headerlink" title="使用到的HDFS語法整理"></a>使用到的HDFS語法整理</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hadoop fs -ls -d /user/hadoop</div><div class="line">hadoop fs -put ~/test1.txt /user/hadoop</div><div class="line">hadoop fs -chmod -R 757 /user/hadoop</div><div class="line">hadoop fs -ls  /user/hadoop</div><div class="line">hadoop fs -chown hadoop /user/hadoop/test1.txt</div><div class="line">hadoop fs -appendToFile - /user/hadoop/test1.txt</div><div class="line">hadoop fs -cat /user/hadoop/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="以下為操作驗證"><a href="#以下為操作驗證" class="headerlink" title="以下為操作驗證"></a>以下為操作驗證</h4><h5 id="使用hadoop語法查看目錄權限"><a href="#使用hadoop語法查看目錄權限" class="headerlink" title="使用hadoop語法查看目錄權限"></a>使用hadoop語法查看目錄權限</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$hadoop fs -ls -d /user/hadoop</div><div class="line">drwxr-xr-x   - hadoop supergroup          0 2017-12-27 18:39 /user/hadoop</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="使用putty-以hadoop帳號-登入-hadoop-master主機建立一個新帳號"><a href="#使用putty-以hadoop帳號-登入-hadoop-master主機建立一個新帳號" class="headerlink" title="使用putty(以hadoop帳號)登入,hadoop-master主機建立一個新帳號"></a>使用putty(以hadoop帳號)登入,hadoop-master主機建立一個新帳號</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$sudo useradd -m popal</div><div class="line">hadoop@hadoop-master:~$ls -la /home</div><div class="line">hadoop@hadoop-master:~$sudo passwd popal</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="建立一個文字檔"><a href="#建立一個文字檔" class="headerlink" title="建立一個文字檔"></a>建立一個文字檔</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$su - popal</div><div class="line">popal@hadoop-master:~$vi ~/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="使用hadoop語法-上傳至HDFS時-出現錯誤訊息"><a href="#使用hadoop語法-上傳至HDFS時-出現錯誤訊息" class="headerlink" title="使用hadoop語法,上傳至HDFS時,出現錯誤訊息"></a>使用hadoop語法,上傳至HDFS時,出現錯誤訊息</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -put ~/test1.txt /user/hadoop</div><div class="line">Error:&quot;put: Permission denied: user=popal, access=WRITE, inode=&quot;/user/hadoop&quot;:hadoop:supergroup:drwxr-xr-x&quot;</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="將-user-hadoop權限變更"><a href="#將-user-hadoop權限變更" class="headerlink" title="將/user/hadoop權限變更"></a>將/user/hadoop權限變更</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">popal@hadoop-master:~$su - hadoop</div><div class="line">hadoop@hadoop-master:~$hadoop fs -chmod -R 757 /user/hadoop</div><div class="line">hadoop@hadoop-master:~$hadoop fs -ls -d /user/hadoop</div><div class="line">drwxr-xrwx   - hadoop supergroup          0 2017-12-27 18:39 /user/hadoop</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="使用popal帳號再傳送一次檔案"><a href="#使用popal帳號再傳送一次檔案" class="headerlink" title="使用popal帳號再傳送一次檔案"></a>使用popal帳號再傳送一次檔案</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$su - popal</div><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -put ~/test1.txt /user/hadoop</div><div class="line">(沒有任何錯誤訊息,並傳送成功)</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="觀察檔案是否有上傳至HDFS"><a href="#觀察檔案是否有上傳至HDFS" class="headerlink" title="觀察檔案是否有上傳至HDFS"></a>觀察檔案是否有上傳至HDFS</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -ls  /user/hadoop</div><div class="line">-rw-r--r--   3 popal  supergroup         95 2018-01-02 10:43 /user/hadoop/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="將原本權限復原"><a href="#將原本權限復原" class="headerlink" title="將原本權限復原"></a>將原本權限復原</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">popal@hadoop-master:~$su - hadoop</div><div class="line">hadoop@hadoop-master:~$hadoop fs -chmod -R 755 /user/hadoop</div><div class="line">hadoop@hadoop-master:~$hadoop fs -ls /user/hadoop</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="將test1-txt的擁有者變更成hadoop"><a href="#將test1-txt的擁有者變更成hadoop" class="headerlink" title="將test1.txt的擁有者變更成hadoop"></a>將test1.txt的擁有者變更成hadoop</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$hadoop fs -chown hadoop /user/hadoop/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="使用popal使用者變更test1-txt內容"><a href="#使用popal使用者變更test1-txt內容" class="headerlink" title="使用popal使用者變更test1.txt內容"></a>使用popal使用者變更test1.txt內容</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$su - popal</div><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -appendToFile - /user/hadoop/test1.txt</div><div class="line">Error&quot;appendToFile: Permission denied: user=popal, access=WRITE, inode=&quot;/user/hadoop/test1.txt&quot;:hadoop:supergroup:-rwxr-xr-x&quot;</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="將-user-hadoop-test1-txt的擁有者變更"><a href="#將-user-hadoop-test1-txt的擁有者變更" class="headerlink" title="將/user/hadoop/test1.txt的擁有者變更"></a>將/user/hadoop/test1.txt的擁有者變更</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">popal@hadoop-master:~$su - hadoop</div><div class="line">hadoop@hadoop-master:~$hadoop fs -chown popal /user/hadoop/test1.txt</div></pre></td></tr></table></figure>
</blockquote>
<h5 id="使用popal的帳號再變更-user-hadoop-test1-txt"><a href="#使用popal的帳號再變更-user-hadoop-test1-txt" class="headerlink" title="使用popal的帳號再變更/user/hadoop/test1.txt"></a>使用popal的帳號再變更/user/hadoop/test1.txt</h5><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$su - popal</div><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -appendToFile - /user/hadoop/test1.txt</div><div class="line">popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -cat /user/hadoop/test1.txt</div></pre></td></tr></table></figure></blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;使用到的HDFS語法整理&quot;&gt;&lt;a href=&quot;#使用到的HDFS語法整理&quot; class=&quot;headerlink&quot; title=&quot;使用到的HDFS語法整理&quot;&gt;&lt;/a&gt;使用到的HDFS語法整理&lt;/h4&gt;&lt;blockquote&gt;
&lt;figure class=&quot;highl
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark2.2.1環境安裝說明</title>
    <link href="http://yoursite.com/2017/12/29/%5Bspark%5Dspark2.2.1_Install/"/>
    <id>http://yoursite.com/2017/12/29/[spark]spark2.2.1_Install/</id>
    <published>2017-12-28T16:00:00.000Z</published>
    <updated>2018-01-02T06:40:23.190Z</updated>
    
    <content type="html"><![CDATA[<h3 id="相關環境說明"><a href="#相關環境說明" class="headerlink" title="相關環境說明"></a>相關環境說明</h3><blockquote>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>HostName</th>
<th>角色</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.168.51.4</td>
<td>hadoop-master</td>
<td>NameNode(NN),SecondaryNameNode, HiveServer2</td>
</tr>
<tr>
<td>192.168.51.5</td>
<td>hadoop-slave1</td>
<td>DataNode(DN1)</td>
</tr>
<tr>
<td>192.168.51.6</td>
<td>hadoop-slave2</td>
<td>DataNode(DN2)</td>
</tr>
</tbody>
</table>
</blockquote>
<h3 id="環境準備"><a href="#環境準備" class="headerlink" title="環境準備"></a>環境準備</h3><h4 id="Install-Linux-OS-略"><a href="#Install-Linux-OS-略" class="headerlink" title="Install Linux OS(略)"></a>Install Linux OS(略)</h4><h4 id="Install-Linux-JDK8-略"><a href="#Install-Linux-JDK8-略" class="headerlink" title="Install Linux JDK8(略)"></a>Install Linux JDK8(略)</h4><h4 id="Install-Linux-Hadoop3-0-0-略"><a href="#Install-Linux-Hadoop3-0-0-略" class="headerlink" title="Install Linux Hadoop3.0.0(略)"></a>Install Linux Hadoop3.0.0(略)</h4><h4 id="Install-MariaDB-略"><a href="#Install-MariaDB-略" class="headerlink" title="Install MariaDB(略)"></a>Install MariaDB(略)</h4><h4 id="Install-Hive2-3-略"><a href="#Install-Hive2-3-略" class="headerlink" title="Install Hive2.3(略)"></a>Install Hive2.3(略)</h4><h3 id="Download-Spark2-2-1-tagz-file"><a href="#Download-Spark2-2-1-tagz-file" class="headerlink" title="Download Spark2.2.1 tagz file"></a>Download Spark2.2.1 tagz file</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wget -P /bgdt http://apache.stu.edu.tw/spark/spark-2.2.1/spark-2.2.1-bin-hadoop2.7.tgz</div><div class="line">wget -P /bgdt http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.39/mysql-connector-java-5.1.39.jar</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="解壓縮下載的spark-2-2-1-bin-hadoop2-7-tgz-file-並更改目錄名稱-過長"><a href="#解壓縮下載的spark-2-2-1-bin-hadoop2-7-tgz-file-並更改目錄名稱-過長" class="headerlink" title="解壓縮下載的spark-2.2.1-bin-hadoop2.7.tgz file,並更改目錄名稱(過長)"></a>解壓縮下載的spark-2.2.1-bin-hadoop2.7.tgz file,並更改目錄名稱(過長)</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tar -zxvf /bgdt/spark-2.2.1-bin-hadoop2.7.tgz -C /bgdt</div><div class="line">mv /bgdt/spark-2.2.1-bin-hadoop2.7 /bgdt/spark-2.2.1</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="加入SPARK-HOME相關的環境變數"><a href="#加入SPARK-HOME相關的環境變數" class="headerlink" title="加入SPARK_HOME相關的環境變數"></a>加入SPARK_HOME相關的環境變數</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">vi ~/.bashrc</div><div class="line"></div><div class="line">##Spark Segement</div><div class="line">export SPARK_HOME=/bgdt/spark-2.2.1</div><div class="line">export PATH=$PATH:$SPARK_HOME/sbin:$SPARK_HOME/bin</div><div class="line"></div><div class="line">source ~/.bashrc</div><div class="line"></div><div class="line">cp /bgdt/mysql-connector-java-5.1.39.jar /bgdt/spark-2.2.1/jars</div><div class="line">ln -s /bgdt/hive-2.3.2/conf/hive-site.xml /bgdt/spark-2.2.1/conf/hive-site.xml</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="執行-spark-shell-for-python"><a href="#執行-spark-shell-for-python" class="headerlink" title="執行 spark shell(for python)"></a>執行 spark shell(for python)</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ pyspark</div><div class="line">Python 2.7.13 (default, Jan 19 2017, 14:48:08)</div><div class="line">[GCC 6.3.0 20170118] on linux2</div><div class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</div><div class="line">Setting default log level to &quot;WARN&quot;.</div><div class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</div><div class="line">2017-12-29 11:34:53,515 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">2017-12-29 11:35:06,762 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</div><div class="line">Welcome to</div><div class="line">      ____              __</div><div class="line">     / __/__  ___ _____/ /__</div><div class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</div><div class="line">   /__ / .__/\_,_/_/ /_/\_\   version 2.2.1</div><div class="line">      /_/</div><div class="line"></div><div class="line">Using Python version 2.7.13 (default, Jan 19 2017 14:48:08)</div><div class="line">SparkSession available as &apos;spark&apos;.</div><div class="line">&gt;&gt;&gt; spark</div><div class="line">&lt;pyspark.sql.session.SparkSession object at 0x7f62b8e37610&gt;</div><div class="line">&gt;&gt;&gt; sc</div><div class="line">&lt;SparkContext master=local[*] appName=PySparkShell&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="執行-spark-shell-for-scala"><a href="#執行-spark-shell-for-scala" class="headerlink" title="執行 spark shell(for scala)"></a>執行 spark shell(for scala)</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ spark-shell</div><div class="line">Setting default log level to &quot;WARN&quot;.</div><div class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</div><div class="line">2017-12-29 11:44:58,352 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">Spark context Web UI available at http://192.168.51.4:4040</div><div class="line">Spark context available as &apos;sc&apos; (master = local[*], app id = local-1514519099800).</div><div class="line">Spark session available as &apos;spark&apos;.</div><div class="line">Welcome to</div><div class="line">      ____              __</div><div class="line">     / __/__  ___ _____/ /__</div><div class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</div><div class="line">   /___/ .__/\_,_/_/ /_/\_\   version 2.2.1</div><div class="line">      /_/</div><div class="line"></div><div class="line">Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_101)</div><div class="line">Type in expressions to have them evaluated.</div><div class="line">Type :help for more information.</div><div class="line"></div><div class="line">scala&gt; spark</div><div class="line">res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7afb9c93</div><div class="line"></div><div class="line">scala&gt; sc</div><div class="line">res1: org.apache.spark.SparkContext = org.apache.spark.SparkContext@14292d71</div><div class="line"></div><div class="line">scala&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="執行spark-sql-Shell"><a href="#執行spark-sql-Shell" class="headerlink" title="執行spark-sql Shell"></a>執行spark-sql Shell</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop-master:~$ spark-sql</div><div class="line">2017-12-29 18:04:01,165 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">2017-12-29 18:04:01,307 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</div><div class="line">2017-12-29 18:04:01,344 INFO metastore.ObjectStore: ObjectStore, initialize called</div><div class="line">2017-12-29 18:04:01,642 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored</div><div class="line">2017-12-29 18:04:01,642 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored</div><div class="line">2017-12-29 18:04:05,450 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=&quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order&quot;</div><div class="line">2017-12-29 18:04:07,495 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</div><div class="line">2017-12-29 18:04:07,497 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</div><div class="line">......</div><div class="line">......</div><div class="line">......</div><div class="line">2017-12-29 18:04:20,967 INFO metastore.HiveMetaStore: 0: get_database: global_temp</div><div class="line">2017-12-29 18:04:20,967 INFO HiveMetaStore.audit: ugi=hadoop    ip=unknown-ip-addr      cmd=get_database: global_temp</div><div class="line">2017-12-29 18:04:20,970 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</div><div class="line">2017-12-29 18:04:21,293 INFO session.SessionState: Created local directory: /tmp/d8b65bb8-f4f6-43f2-b544-39331b237673_resources</div><div class="line">2017-12-29 18:04:21,298 INFO session.SessionState: Created HDFS directory: /tmp/hive/hadoop/d8b65bb8-f4f6-43f2-b544-39331b237673</div><div class="line">2017-12-29 18:04:21,306 INFO session.SessionState: Created local directory: /tmp/hadoop/d8b65bb8-f4f6-43f2-b544-39331b237673</div><div class="line">2017-12-29 18:04:21,310 INFO session.SessionState: Created HDFS directory: /tmp/hive/hadoop/d8b65bb8-f4f6-43f2-b544-39331b237673/_tmp_space.db</div><div class="line">2017-12-29 18:04:21,311 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse</div><div class="line">2017-12-29 18:04:21,406 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint</div><div class="line"></div><div class="line">spark-sql&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="Spark2-2-1-amp-Hive2-3-2整合測試"><a href="#Spark2-2-1-amp-Hive2-3-2整合測試" class="headerlink" title="Spark2.2.1 &amp; Hive2.3.2整合測試"></a>Spark2.2.1 &amp; Hive2.3.2整合測試</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">###開啟第1個putty連線</div><div class="line">hadoop@hadoop-master:~$spark-sql</div><div class="line">spark-sql&gt;create database test1;</div><div class="line">..... INFO execution.SparkSqlParser: Parsing command: create database test1</div><div class="line">..... INFO metastore.HiveMetaStore: 0: create_database: Database(name:test1, description:, locationUri:hdfs://hadoop-master:8020/user/hive/warehouse/test1.db, parameters:&#123;&#125;)</div><div class="line">..... INFO HiveMetaStore.audit: ugi=hadoop    ip=unknown-ip-addr      cmd=create_database: Database(name:test1, description:, locationUri:hdfs://hadoop-master:8020/user/hive/warehouse/test1.db, parameters:&#123;&#125;)</div><div class="line">..... WARN metastore.ObjectStore: Failed to get database test1, returning NoSuchObjectException</div><div class="line">..... INFO common.FileUtils: Creating directory if it doesn&apos;t exist: hdfs://hadoop-master:8020/user/hive/warehouse/test1.db</div><div class="line">Time taken: 0.326 seconds</div><div class="line">..... INFO CliDriver: Time taken: 0.326 seconds</div><div class="line"></div><div class="line">spark-sql&gt;show databases;</div><div class="line">INFO execution.SparkSqlParser: Parsing command: show databases</div><div class="line">INFO metastore.HiveMetaStore: 0: get_databases: *</div><div class="line">HiveMetaStore.audit: ugi=hadoop    ip=unknown-ip-addr      cmd=get_databases: *</div><div class="line">INFO codegen.CodeGenerator: Code generated in 544.163987 ms</div><div class="line">default</div><div class="line">test1</div><div class="line"></div><div class="line">###開啟第2個putty連線</div><div class="line">hadoop@hadoop-master:~$hive</div><div class="line">hive&gt;show databases;</div><div class="line">OK</div><div class="line">default</div><div class="line">test1</div><div class="line">......</div><div class="line"></div><div class="line">hive&gt;create database test2;</div><div class="line">OK</div><div class="line">Time taken: 0.341 seconds</div><div class="line"></div><div class="line">hive&gt;show databases;</div><div class="line">OK</div><div class="line">default</div><div class="line">test1</div><div class="line">test2</div><div class="line">......</div><div class="line"></div><div class="line">spark-sql&gt;show databases;</div><div class="line">default</div><div class="line">test1</div><div class="line">test2</div><div class="line"></div><div class="line">###以上驗證由兩種不同方式所建立的資料庫是可以同步的</div></pre></td></tr></table></figure></blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;相關環境說明&quot;&gt;&lt;a href=&quot;#相關環境說明&quot; class=&quot;headerlink&quot; title=&quot;相關環境說明&quot;&gt;&lt;/a&gt;相關環境說明&lt;/h3&gt;&lt;blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;IP Address&lt;/th&gt;
&lt;th
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive2.3+Hadooop3.0.0環境安裝說明</title>
    <link href="http://yoursite.com/2017/12/28/%5Bhive%5DHive2.3_for_Hadoop3_Install/"/>
    <id>http://yoursite.com/2017/12/28/[hive]Hive2.3_for_Hadoop3_Install/</id>
    <published>2017-12-27T16:00:00.000Z</published>
    <updated>2017-12-29T09:50:56.906Z</updated>
    
    <content type="html"><![CDATA[<h3 id="相關環境說明"><a href="#相關環境說明" class="headerlink" title="相關環境說明"></a>相關環境說明</h3><blockquote>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>HostName</th>
<th>角色</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.168.51.4</td>
<td>hadoop-master</td>
<td>NameNode(NN),SecondaryNameNode, HiveServer2</td>
</tr>
<tr>
<td>192.168.51.5</td>
<td>hadoop-slave1</td>
<td>DataNode(DN1)</td>
</tr>
<tr>
<td>192.168.51.6</td>
<td>hadoop-slave2</td>
<td>DataNode(DN2)</td>
</tr>
</tbody>
</table>
</blockquote>
<h3 id="環境準備"><a href="#環境準備" class="headerlink" title="環境準備"></a>環境準備</h3><h4 id="Install-Linux-OS-略"><a href="#Install-Linux-OS-略" class="headerlink" title="Install Linux OS(略)"></a>Install Linux OS(略)</h4><h4 id="Install-Linux-JDK8-略"><a href="#Install-Linux-JDK8-略" class="headerlink" title="Install Linux JDK8(略)"></a>Install Linux JDK8(略)</h4><h4 id="Install-Linux-Hadoop3-0-0-略"><a href="#Install-Linux-Hadoop3-0-0-略" class="headerlink" title="Install Linux Hadoop3.0.0(略)"></a>Install Linux Hadoop3.0.0(略)</h4><h4 id="Install-MariaDB-略"><a href="#Install-MariaDB-略" class="headerlink" title="Install MariaDB(略)"></a>Install MariaDB(略)</h4><h3 id="Hive2-3-Install-File-Download"><a href="#Hive2-3-Install-File-Download" class="headerlink" title="Hive2.3 Install File Download"></a>Hive2.3 Install File Download</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wget -P /bgdt https://archive.apache.org/dist/hive/hive-2.3.2/apache-hive-2.3.2-bin.tar.gz </div><div class="line">wget -P /bgdt http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.39/mysql-connector-java-5.1.39.jar</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="解壓縮"><a href="#解壓縮" class="headerlink" title="解壓縮"></a>解壓縮</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tar -zxvf /bgdt/apache-hive-2.3.2-bin.tar.gz -C /bgdt</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="使用Mariadb建立Hive2-3所使用的MetaStore-DataBase"><a href="#使用Mariadb建立Hive2-3所使用的MetaStore-DataBase" class="headerlink" title="使用Mariadb建立Hive2.3所使用的MetaStore DataBase"></a>使用Mariadb建立Hive2.3所使用的MetaStore DataBase</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">step1:</div><div class="line">sudo -S /usr/local/mysql/bin/mysqld --user=root &amp;	##啟動mariadb</div><div class="line">hadoop@hadoop-master:~$ netstat -tnl | grep &quot;3306&quot;</div><div class="line">tcp6       0      0 :::3306                 :::*                    LISTEN</div><div class="line"></div><div class="line">step2:</div><div class="line">/usr/local/mysql/bin/mysql -u root --password=\!QAZxsw2	##進入Mariadb CLI</div><div class="line">MariaDB [(none)]&gt;</div><div class="line"></div><div class="line">step3:</div><div class="line">MariaDB [(none)]&gt;grant all privileges on *.* to &apos;root&apos;@&apos;hadoop-master&apos; identified by &apos;!QAZxsw2&apos; with grant option;</div><div class="line"></div><div class="line">step4:</div><div class="line">MariaDB [(none)]&gt; create database metastore3_db;	##這邊建立的database就是之後ConnectionURL中,使用的DB名稱</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="在HDFS中建立hive儲存目錄"><a href="#在HDFS中建立hive儲存目錄" class="headerlink" title="在HDFS中建立hive儲存目錄"></a>在HDFS中建立hive儲存目錄</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop fs -mkdir -p /user/hive/warehouse</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="使用SchemaTool建立Hive2-3-Meta-Data-Table-Schema"><a href="#使用SchemaTool建立Hive2-3-Meta-Data-Table-Schema" class="headerlink" title="使用SchemaTool建立Hive2.3 Meta Data Table Schema"></a>使用SchemaTool建立Hive2.3 Meta Data Table Schema</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">schematool -dbType mysql -initSchema				##此語法會重新建立新的Table,如果需要upgrade就不行用這個語法</div><div class="line">schematool -dbType mysql -upgradeSchemaFrom 0.7.0 -dryRun	##更新新的schema,但不會影響到既有的Table</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="相關配置檔設定"><a href="#相關配置檔設定" class="headerlink" title="相關配置檔設定"></a>相關配置檔設定</h3><blockquote>
<p>hive-site.xml檔案設定內容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</div><div class="line">&lt;configuration&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</div><div class="line">    &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</div><div class="line">    &lt;description&gt;location of default database for the warehouse&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</div><div class="line">	&lt;value&gt;jdbc:mysql://hadoop-master:3306/metastore3_db&lt;/value&gt;</div><div class="line">	&lt;description&gt;JDBC connect string for a JDBC metastore &lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</div><div class="line">	&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</div><div class="line">	&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</div><div class="line">	&lt;value&gt;root&lt;/value&gt;</div><div class="line">	&lt;description&gt;username to use against metastore database&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</div><div class="line">	&lt;value&gt;!QAZxsw2&lt;/value&gt;</div><div class="line">	&lt;description&gt;password to use against metastore database&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">   &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;  </div><div class="line">   &lt;value&gt;false&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure></p>
</blockquote>
<h3 id="修改-bashrc"><a href="#修改-bashrc" class="headerlink" title="修改~/.bashrc"></a>修改~/.bashrc</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">vi ~/.bashrc</div><div class="line"></div><div class="line">##Hive Segement</div><div class="line">export HIVE_HOME=/bgdt/hive-2.3.2</div><div class="line">export PATH=$PATH:$HIVE_HOME/bin</div><div class="line"></div><div class="line"></div><div class="line">source ~/.bashrc</div><div class="line"></div><div class="line">cp /bgdt/mysql-connector-java-5.1.39.jar /bgdt/hive-2.3.2/lib</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="啟動Hive"><a href="#啟動Hive" class="headerlink" title="啟動Hive"></a>啟動Hive</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hive	##進入Hive CLI</div><div class="line">nohup /bgdt/hive-2.3.2/bin/hive --service hiveserver2 &amp;		##啟動Hive2.3 for JDBC Service</div></pre></td></tr></table></figure></blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;相關環境說明&quot;&gt;&lt;a href=&quot;#相關環境說明&quot; class=&quot;headerlink&quot; title=&quot;相關環境說明&quot;&gt;&lt;/a&gt;相關環境說明&lt;/h3&gt;&lt;blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;IP Address&lt;/th&gt;
&lt;th
    
    </summary>
    
      <category term="hive" scheme="http://yoursite.com/categories/hive/"/>
    
    
  </entry>
  
  <entry>
    <title>Hadoop3.0.0 Cluster環境安裝說明</title>
    <link href="http://yoursite.com/2017/12/27/%5Bhadoop%5Dhadoop_3_install/"/>
    <id>http://yoursite.com/2017/12/27/[hadoop]hadoop_3_install/</id>
    <published>2017-12-26T16:00:00.000Z</published>
    <updated>2017-12-27T10:44:12.148Z</updated>
    
    <content type="html"><![CDATA[<h3 id="相關環境說明"><a href="#相關環境說明" class="headerlink" title="相關環境說明"></a>相關環境說明</h3><blockquote>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>HostName</th>
<th>角色</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.168.51.4</td>
<td>hadoop-master</td>
<td>NameNode(NN),SecondaryNameNode</td>
</tr>
<tr>
<td>192.168.51.5</td>
<td>hadoop-slave1</td>
<td>DataNode(DN1)</td>
</tr>
<tr>
<td>192.168.51.6</td>
<td>hadoop-slave2</td>
<td>DataNode(DN2)</td>
</tr>
</tbody>
</table>
</blockquote>
<h3 id="環境準備"><a href="#環境準備" class="headerlink" title="環境準備"></a>環境準備</h3><h4 id="Install-Linux-OS-略"><a href="#Install-Linux-OS-略" class="headerlink" title="Install Linux OS(略)"></a>Install Linux OS(略)</h4><h4 id="Install-JDK8-略"><a href="#Install-JDK8-略" class="headerlink" title="Install JDK8(略)"></a>Install JDK8(略)</h4><h3 id="下載hadoop-3-0-0-tar-gz"><a href="#下載hadoop-3-0-0-tar-gz" class="headerlink" title="下載hadoop-3.0.0.tar.gz"></a>下載hadoop-3.0.0.tar.gz</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget -P /bgdt http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-3.0.0/hadoop-3.0.0.tar.gz</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="檔案解壓縮"><a href="#檔案解壓縮" class="headerlink" title="檔案解壓縮"></a>檔案解壓縮</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">tar -zxvf /bgdt/hadoop-3.0.0.tar.gz -C /bgdt</div><div class="line">---</div><div class="line">解完壓縮檔之後進入hadoop-3.0.0目錄查看目錄結構</div><div class="line">[hadoop@hadoop-master:~$]ls -la /bgdt/hadoop-3.0.0</div><div class="line">total 220</div><div class="line">drwxr-xr-x 12 hadoop hadoop   4096 Dec 26 15:23 .</div><div class="line">drwxr-xr-x 10 hadoop hadoop   4096 Dec 27 16:54 ..</div><div class="line">drwxr-xr-x  2 hadoop hadoop   4096 Dec  9 03:42 bin</div><div class="line">drwxr-xr-x  3 hadoop hadoop   4096 Dec  9 03:17 etc</div><div class="line">drwxr-xr-x  2 hadoop hadoop   4096 Dec  9 03:42 include</div><div class="line">drwxr-xr-x  3 hadoop hadoop   4096 Dec  9 03:42 lib</div><div class="line">drwxr-xr-x  4 hadoop hadoop   4096 Dec  9 03:42 libexec</div><div class="line">-rw-r--r--  1 hadoop hadoop 147066 Nov 15 03:19 LICENSE.txt</div><div class="line">drwxrwxr-x  3 hadoop hadoop   4096 Dec 27 10:58 logs</div><div class="line">-rw-r--r--  1 hadoop hadoop  20891 Nov 15 03:19 NOTICE.txt</div><div class="line">-rw-r--r--  1 hadoop hadoop   1366 Jul  9  2016 README.txt</div><div class="line">drwxr-xr-x  3 hadoop hadoop   4096 Dec  9 03:17 sbin</div><div class="line">drwxr-xr-x  4 hadoop hadoop   4096 Dec  9 03:53 share</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="設定檔編輯"><a href="#設定檔編輯" class="headerlink" title="設定檔編輯"></a>設定檔編輯</h3><h4 id="修改-bashrc"><a href="#修改-bashrc" class="headerlink" title="修改~/.bashrc"></a>修改~/.bashrc</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">vi ~/.bashrc</div><div class="line">export JAVA_HOME=/bgdt/java/jdk1.8.0_101</div><div class="line">export HADOOP_HOME=/bgdt/hadoop-3.0.0</div><div class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</div><div class="line">export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin	</div><div class="line">---</div><div class="line">修改完成後:wq存檔</div><div class="line"></div><div class="line">[hadoop@hadoop-master:~$]source ~/.bashrc</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</div><div class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</div><div class="line">&lt;configuration&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;fs.defaultFS&lt;/name&gt;</div><div class="line">	&lt;value&gt;hdfs://hadoop-master:8020&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</div><div class="line">	&lt;value&gt;file:/bgdt/hadoop-3.0.0/tmp&lt;/value&gt;</div><div class="line">	&lt;description&gt;Abase for other temporary directories.&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt;</div><div class="line">	&lt;value&gt;*&lt;/value&gt;    </div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt;</div><div class="line">	&lt;value&gt;*&lt;/value&gt;    </div><div class="line">&lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</div><div class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</div><div class="line">&lt;configuration&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</div><div class="line">	&lt;value&gt;hadoop-master:50090&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;dfs.replication&lt;/name&gt;</div><div class="line">	&lt;value&gt;3&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</div><div class="line">	&lt;value&gt;file:/bgdt/hadoop-3.0.0/tmp/dfs/name&lt;/value&gt;</div><div class="line">&lt;/property&gt; </div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</div><div class="line">	&lt;value&gt;file:/bgdt/hadoop-3.0.0/tmp/dfs/data&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;dfs.block.size&lt;/name&gt;</div><div class="line">	&lt;value&gt;64M&lt;/value&gt;   </div><div class="line">&lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="maperd-site-xml"><a href="#maperd-site-xml" class="headerlink" title="maperd-site.xml"></a>maperd-site.xml</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</div><div class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</div><div class="line">&lt;configuration&gt;   </div><div class="line">&lt;property&gt;  </div><div class="line">	&lt;name&gt;mapreduce.framework.name&lt;/name&gt;  </div><div class="line">	&lt;value&gt;yarn&lt;/value&gt;  </div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</div><div class="line">	&lt;value&gt;mna1:10020&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</div><div class="line">	&lt;value&gt;mna1:19888&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;  </div><div class="line">	&lt;name&gt;mapreduce.application.classpath&lt;/name&gt;  </div><div class="line">	&lt;value&gt;  </div><div class="line">		$HADOOP_HOME/share/hadoop/common/*,</div><div class="line">		$HADOOP_HOME/share/hadoop/hdfs/*,  </div><div class="line">		$HADOOP_HOME/share/hadoop/mapreduce/*,  </div><div class="line">		$HADOOP_HOME/share/hadoop/yarn/*</div><div class="line">	&lt;/value&gt;</div><div class="line">&lt;/property&gt;  </div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</div><div class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</div><div class="line">&lt;configuration&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</div><div class="line">	&lt;value&gt;hadoop-master&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</div><div class="line">	&lt;value&gt;mapreduce_shuffle&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;yarn.log.server.url&lt;/name&gt;</div><div class="line">	&lt;value&gt;http://hadoop-master:19888/jobhistory/logs&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h4 id="worker"><a href="#worker" class="headerlink" title="worker"></a>worker</h4><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop-slave1</div><div class="line">hadoop-slave2</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="壓縮檔案後scp至DataNode"><a href="#壓縮檔案後scp至DataNode" class="headerlink" title="壓縮檔案後scp至DataNode"></a>壓縮檔案後scp至DataNode</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">將NameNode hadoop目錄壓縮後copy至其他DataNode,進行解壓縮</div><div class="line">---</div><div class="line">[hadoop@hadoop-master:~$]cd /bgdt</div><div class="line">[hadoop@hadoop-master:~$]tar -zcvf ~/hadoop-3.0.0.tar.gz hadoop-3.0.0</div><div class="line">[hadoop@hadoop-master:~$]scp ~/hadoop-3.0.0.tar.gz hadoop-slave1:/bgdt</div><div class="line">[hadoop@hadoop-master:~$]scp ~/hadoop-3.0.0.tar.gz hadoop-slave2:/bgdt</div><div class="line">---</div><div class="line">ssh至DataNode並將hadoop-3.0.0.tar.gz解壓縮</div><div class="line">ssh hadoop-slave1</div><div class="line">[hadoop@hadoop-slave1:~$]tar -zxvf /bgdt/hadoop-3.0.0.tar.gz -C /bgdt</div><div class="line">ssh hadoop-slave2</div><div class="line">[hadoop@hadoop-slave2:~$]tar -zxvf /bgdt/hadoop-3.0.0.tar.gz -C /bgdt</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="HDFS格式化"><a href="#HDFS格式化" class="headerlink" title="HDFS格式化"></a>HDFS格式化</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop namenode -format</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="Hadoop-3-0-0-Port-Number-List"><a href="#Hadoop-3-0-0-Port-Number-List" class="headerlink" title="Hadoop 3.0.0 Port Number List"></a>Hadoop 3.0.0 Port Number List</h3><blockquote>
<table>
<thead>
<tr>
<th style="text-align:left">Service Name</th>
<th style="text-align:left">Hadoop 2.X Port Number</th>
<th style="text-align:left">Hadoop 3.X Port Number</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Hadoop HDFS NameNode</td>
<td style="text-align:left">8020</td>
<td style="text-align:left">9820</td>
</tr>
<tr>
<td style="text-align:left">Hadoop HDFS NameNode HTTP UI</td>
<td style="text-align:left">50070</td>
<td style="text-align:left">9870</td>
</tr>
<tr>
<td style="text-align:left">Hadoop HDFS NameNode HTTPS UI</td>
<td style="text-align:left">50470</td>
<td style="text-align:left">9840</td>
</tr>
<tr>
<td style="text-align:left">Hadoop HDFS SecondaryNameNode HTTP UI</td>
<td style="text-align:left">50091</td>
<td style="text-align:left">9869</td>
</tr>
<tr>
<td style="text-align:left">Hadoop HDFS SecondaryNameNode HTTPS UI</td>
<td style="text-align:left">50090</td>
<td style="text-align:left">9868</td>
</tr>
<tr>
<td style="text-align:left">Hadoop HDFS</td>
<td style="text-align:left">8020</td>
<td style="text-align:left">9820</td>
</tr>
<tr>
<td style="text-align:left">Hadoop HDFS DataNode IPC Port</td>
<td style="text-align:left">50020</td>
<td style="text-align:left">9867</td>
</tr>
<tr>
<td style="text-align:left">Hadoop HDFS DataNode</td>
<td style="text-align:left">50010</td>
<td style="text-align:left">9866</td>
</tr>
<tr>
<td style="text-align:left">Hadoop HDFS DataNode HTTP UI</td>
<td style="text-align:left">50075</td>
<td style="text-align:left">9864</td>
</tr>
<tr>
<td style="text-align:left">Hadoop HDFS DataNode HTTPS UI</td>
<td style="text-align:left">50475</td>
<td style="text-align:left">9865</td>
</tr>
</tbody>
</table>
</blockquote>
<h3 id="Hadoop服務啟動命令"><a href="#Hadoop服務啟動命令" class="headerlink" title="Hadoop服務啟動命令"></a>Hadoop服務啟動命令</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">啟動方式1:使用start-*.sh</div><div class="line">-----</div><div class="line">[hadoop@hadoop-master:~$]start-dfs.sh</div><div class="line">[hadoop@hadoop-master:~$]start-yarn.sh</div><div class="line">[hadoop@hadoop-master:~$]jps</div><div class="line">2340 ResourceManager</div><div class="line">1834 NameNode</div><div class="line">2107 SecondaryNameNode</div><div class="line">[hadoop@hadoop-master:~$]ssh hadoop-slave1</div><div class="line">[hadoop@hadoop-slave1:~$]jps</div><div class="line">1568 DataNode</div><div class="line">1738 NodeManager</div><div class="line">[hadoop@hadoop-master:~$]ssh hadoop-slave2</div><div class="line">[hadoop@hadoop-slave2:~$]jps</div><div class="line">1715 NodeManager</div><div class="line">1545 DataNode</div><div class="line"></div><div class="line">啟動方式2:</div><div class="line">-----</div><div class="line">[hadoop@hadoop-master:~$]hdfs --daemon start namenode</div><div class="line">[hadoop@hadoop-master:~$]yarn --daemon start resourcemanager</div><div class="line">[hadoop@hadoop-master:~$]hdfs --daemon start proxyserver</div><div class="line">[hadoop@hadoop-master:~$]hdfs --daemon start httpfs</div><div class="line">[hadoop@hadoop-master:~$]mapred --daemon start historyserver</div><div class="line">-</div><div class="line">[hadoop@hadoop-slave1:~$]hdfs --daemon start datanode</div><div class="line">[hadoop@hadoop-slave1:~$]yarn --daemon start nodemanager</div><div class="line">-</div><div class="line">[hadoop@hadoop-slave2:~$]hdfs --daemon start datanode</div><div class="line">[hadoop@hadoop-slave2:~$]yarn --daemon start nodemanager</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="Hadoop服務關閉"><a href="#Hadoop服務關閉" class="headerlink" title="Hadoop服務關閉"></a>Hadoop服務關閉</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">停止方式1:使用stop-*.sh</div><div class="line">-----</div><div class="line">[hadoop@hadoop-master:~$]stop-dfs.sh</div><div class="line">[hadoop@hadoop-master:~$]stop-yarn.sh</div><div class="line"></div><div class="line">停止方式2:</div><div class="line">-----</div><div class="line">[hadoop@hadoop-master:~$]hdfs --daemon stop namenode</div><div class="line">[hadoop@hadoop-master:~$]yarn --daemon stop resourcemanager</div><div class="line">[hadoop@hadoop-master:~$]hdfs --daemon stop proxyserver</div><div class="line">[hadoop@hadoop-master:~$]hdfs --daemon stop httpfs</div><div class="line">[hadoop@hadoop-master:~$]mapred --daemon stop historyserver</div><div class="line">-</div><div class="line">[hadoop@hadoop-slave1:~$]hdfs --daemon stop datanode</div><div class="line">[hadoop@hadoop-slave1:~$]yarn --daemon stop nodemanager</div><div class="line">-</div><div class="line">[hadoop@hadoop-slave2:~$]hdfs --daemon stop datanode</div><div class="line">[hadoop@hadoop-slave2:~$]yarn --daemon stop nodemanager</div><div class="line"></div><div class="line">停止DataNode方式3:</div><div class="line">-----</div><div class="line">[hadoop@hadoop-master:~$]hdfs dfsadmin -shutdownDatanode 192.168.51.5:9867</div><div class="line">[hadoop@hadoop-master:~$]hdfs dfsadmin -shutdownDatanode 192.168.51.6:9867</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="Hadoop-MapReduce-Example"><a href="#Hadoop-MapReduce-Example" class="headerlink" title="Hadoop MapReduce Example"></a>Hadoop MapReduce Example</h3><blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop jar /bgdt/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar pi 4 100</div><div class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar grep input output &apos;dfs[a-z.]+&apos;</div></pre></td></tr></table></figure>
</blockquote>
<h3 id="Hadoop-Web-UI"><a href="#Hadoop-Web-UI" class="headerlink" title="Hadoop Web UI"></a>Hadoop Web UI</h3><h4 id="web-1"><a href="#web-1" class="headerlink" title="web-1"></a>web-1</h4><blockquote>
<p>Hadoop WebUI-1 Port:9870,此頁面可以觀察Namenode相關資訊<br><img src="/images/hadoop_3_install/hadoop_3_install_1.jpg" alt=""></p>
</blockquote>
<h4 id="web-2"><a href="#web-2" class="headerlink" title="web-2"></a>web-2</h4><blockquote>
<p>Hadoop WebUI-2 此頁面可以觀察DataNode狀態<br><img src="/images/hadoop_3_install/hadoop_3_install_2.jpg" alt=""></p>
</blockquote>
<h4 id="web-3"><a href="#web-3" class="headerlink" title="web-3"></a>web-3</h4><blockquote>
<p>Hadoop WebUI-3 此頁面可以觀察HDFS檔案結構,新版的Browser可以直接由介面上上傳檔案,但先決條件為沒有設定相關權限<br><img src="/images/hadoop_3_install/hadoop_3_install_3.jpg" alt=""></p>
</blockquote>
<h4 id="web-4"><a href="#web-4" class="headerlink" title="web-4"></a>web-4</h4><blockquote>
<p>Hadoop WebUI-4,Port:8088,此頁面為ResourceManager的管理介面,系統會列出所有Application的運行狀態<br><img src="/images/hadoop_3_install/hadoop_3_install_4.jpg" alt=""></p>
</blockquote>
<h4 id="UI-5"><a href="#UI-5" class="headerlink" title="UI-5"></a>UI-5</h4><blockquote>
<p>此圖為Run Hadoop MapReduce時,產生的錯誤訊息(此部分為Hadoop3.0.0的問題,Exception可以暫時略過不管)<br>  <a href="https://issues.apache.org/jira/browse/HDFS-10429" target="_blank" rel="external">HDFS-10429</a> DataStreamer interrupted warning always appears when using CLI upload file<br><img src="/images/hadoop_3_install/hadoop_3_install_5.jpg" alt=""></p>
</blockquote>
<h4 id="UI-6"><a href="#UI-6" class="headerlink" title="UI-6"></a>UI-6</h4><blockquote>
<p>程式Run完後,在此畫面可以觀察Application最後的執行狀態,如果有啟動JobHistory時可以觀察到相關的Log<br><img src="/images/hadoop_3_install/hadoop_3_install_6.jpg" alt=""></p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;相關環境說明&quot;&gt;&lt;a href=&quot;#相關環境說明&quot; class=&quot;headerlink&quot; title=&quot;相關環境說明&quot;&gt;&lt;/a&gt;相關環境說明&lt;/h3&gt;&lt;blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;IP Address&lt;/th&gt;
&lt;th
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>Excel之下拉式選單設計技巧-1</title>
    <link href="http://yoursite.com/2017/12/08/%5Bexcel%5DExcel_skill/"/>
    <id>http://yoursite.com/2017/12/08/[excel]Excel_skill/</id>
    <published>2017-12-07T16:00:00.000Z</published>
    <updated>2018-01-02T06:40:53.736Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>Excel操作的小小筆記</p>
</blockquote>
<ol>
<li><p>開啟Excel,之後先建立一個名叫部門代碼表的Sheet<br><img src="/images/excel_skill_1/List.jpg" alt=""></p>
</li>
<li><p>建立完工作表之後,需在”公式”-&gt;”名稱管理員”-&gt;”新增”中定義相關的名稱<br>其中有需要注意以下幾個地方:<br>1.名稱:部門代碼表<br>2.範圍:整個活頁簿<br>3.參照到:需要在要定義的工作表中選出定義的範圍(=部門代碼表!$A:$B)<br>“=部門代碼表!”–&gt;意思是部門代碼表這個工作表(Sheet)<br>“$A:$B”–&gt;表示工作表中的A欄和B欄全部<br>選取的內容是A和B兩個欄位全部,因為我們會想要隨時增加部門進來所以不限定列數<br><img src="/images/excel_skill_1/List_9.jpg" alt=""></p>
</li>
<li><p>我們再新增一個”部門名稱”的定義給名稱管理員,相關欄位同上圖,只是這次我們只選擇A欄<br>這個部份的定義是要給下拉式選單當成選擇的項目<br><img src="/images/excel_skill_1/List_10.jpg" alt=""></p>
</li>
<li><p>最後我們查看名稱管理員會出現兩個名稱定義(部門代碼表,部門名稱)<br><img src="/images/excel_skill_1/List_11.jpg" alt=""></p>
</li>
<li><p>再來我們開啟另一個工作表,隨便選一個cell做為下拉式選單測試<br>接著點選”資料”-&gt;”資料驗證”-&gt;”設定”<br>然後”儲存格內允許”–&gt;清單<br>“來源”–&gt;填入”=部門名稱”<br>“儲存格內的下拉式選單”記得勾選<br><img src="/images/excel_skill_1/List_12.jpg" alt=""></p>
</li>
<li><p>此時點選按鈕就會出現一個下拉式選單<br><img src="/images/excel_skill_1/List_13.jpg" alt=""></p>
</li>
<li><p>最後一個步驟,選擇一個要呈現結果的cell,填入以下公式<br>=VLOOKUP(A1,部門代碼表,2,FALSE)<br>以下說明VLOOKUP(自動查找函數)相關參數<br>查閱欄位：填入欄位編號<br>查閱範圍：定義的範圍名稱<br>欄位編號：取出欄位編號，在定義的範圍名稱欄位代號(由左至右1,2,….)<br>是否完全符合：搜尋時是否要完全符合查詢欄位的值，TRUE-部分符合，FALSE-完全符合。<br><img src="/images/excel_skill_1/List_14.jpg" alt=""></p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Excel操作的小小筆記&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;開啟Excel,之後先建立一個名叫部門代碼表的Sheet&lt;br&gt;&lt;img src=&quot;/images/excel_skill_1/List.jpg&quot; alt=&quot;&quot;&gt;
    
    </summary>
    
      <category term="excel" scheme="http://yoursite.com/categories/excel/"/>
    
    
  </entry>
  
</feed>
