{"meta":{"title":"Popal's Blog","subtitle":"邊做邊學","description":"技術之路學習歷程","author":"popal","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"Hadoop3.0.0 Cluster環境安裝說明","slug":"[hadoop]hadoop_3_install","date":"2017-12-26T16:00:00.000Z","updated":"2017-12-27T10:29:39.861Z","comments":true,"path":"2017/12/27/[hadoop]hadoop_3_install/","link":"","permalink":"http://yoursite.com/2017/12/27/[hadoop]hadoop_3_install/","excerpt":"","text":"相關環境說明 IP Address HostName 角色 192.168.51.4 hadoop-master NameNode(NN),SecondaryNameNode 192.168.51.5 hadoop-slave1 DataNode(DN1) 192.168.51.6 hadoop-slave2 DataNode(DN2) 環境準備Install Linux OS(略)Install JDK8(略)下載hadoop-3.0.0.tar.gz 1wget -P /bgdt http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-3.0.0/hadoop-3.0.0.tar.gz 檔案解壓縮 123456789101112131415161718tar -zxvf /bgdt/hadoop-3.0.0.tar.gz -C /bgdt---解完壓縮檔之後進入hadoop-3.0.0目錄查看目錄結構[hadoop@hadoop-master:~$]ls -la /bgdt/hadoop-3.0.0total 220drwxr-xr-x 12 hadoop hadoop 4096 Dec 26 15:23 .drwxr-xr-x 10 hadoop hadoop 4096 Dec 27 16:54 ..drwxr-xr-x 2 hadoop hadoop 4096 Dec 9 03:42 bindrwxr-xr-x 3 hadoop hadoop 4096 Dec 9 03:17 etcdrwxr-xr-x 2 hadoop hadoop 4096 Dec 9 03:42 includedrwxr-xr-x 3 hadoop hadoop 4096 Dec 9 03:42 libdrwxr-xr-x 4 hadoop hadoop 4096 Dec 9 03:42 libexec-rw-r--r-- 1 hadoop hadoop 147066 Nov 15 03:19 LICENSE.txtdrwxrwxr-x 3 hadoop hadoop 4096 Dec 27 10:58 logs-rw-r--r-- 1 hadoop hadoop 20891 Nov 15 03:19 NOTICE.txt-rw-r--r-- 1 hadoop hadoop 1366 Jul 9 2016 README.txtdrwxr-xr-x 3 hadoop hadoop 4096 Dec 9 03:17 sbindrwxr-xr-x 4 hadoop hadoop 4096 Dec 9 03:53 share 設定檔編輯修改~/.bashrc 123456789vi ~/.bashrcexport JAVA_HOME=/bgdt/java/jdk1.8.0_101export HADOOP_HOME=/bgdt/hadoop-3.0.0export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin ---修改完成後:wq存檔[hadoop@hadoop-master:~$]source ~/.bashrc core-site.xml 123456789101112131415161718192021&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop-master:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/bgdt/hadoop-3.0.0/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop-master:50090&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/bgdt/hadoop-3.0.0/tmp/dfs/name&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/bgdt/hadoop-3.0.0/tmp/dfs/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.block.size&lt;/name&gt; &lt;value&gt;64M&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; maperd-site.xml 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;mna1:10020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;mna1:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.application.classpath&lt;/name&gt; &lt;value&gt; $HADOOP_HOME/share/hadoop/common/*, $HADOOP_HOME/share/hadoop/hdfs/*, $HADOOP_HOME/share/hadoop/mapreduce/*, $HADOOP_HOME/share/hadoop/yarn/* &lt;/value&gt;&lt;/property&gt; &lt;/configuration&gt; yarn-site.xml 12345678910111213141516&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop-master&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop-master:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; worker 12hadoop-slave1hadoop-slave2 壓縮檔案後scp至DataNode 123456789101112將NameNode hadoop目錄壓縮後copy至其他DataNode,進行解壓縮---[hadoop@hadoop-master:~$]cd /bgdt[hadoop@hadoop-master:~$]tar -zcvf ~/hadoop-3.0.0.tar.gz hadoop-3.0.0[hadoop@hadoop-master:~$]scp ~/hadoop-3.0.0.tar.gz hadoop-slave1:/bgdt[hadoop@hadoop-master:~$]scp ~/hadoop-3.0.0.tar.gz hadoop-slave2:/bgdt---ssh至DataNode並將hadoop-3.0.0.tar.gz解壓縮ssh hadoop-slave1[hadoop@hadoop-slave1:~$]tar -zxvf /bgdt/hadoop-3.0.0.tar.gz -C /bgdtssh hadoop-slave2[hadoop@hadoop-slave2:~$]tar -zxvf /bgdt/hadoop-3.0.0.tar.gz -C /bgdt HDFS格式化 1hadoop namenode -format Hadoop 3.0.0 Port Number List Service Name Hadoop 2.X Port Number Hadoop 3.X Port Number Hadoop HDFS NameNode 8020 9820 Hadoop HDFS NameNode HTTP UI 50070 9870 Hadoop HDFS NameNode HTTPS UI 50470 9840 Hadoop HDFS SecondaryNameNode HTTP UI 50091 9869 Hadoop HDFS SecondaryNameNode HTTPS UI 50090 9868 Hadoop HDFS 8020 9820 Hadoop HDFS DataNode IPC Port 50020 9867 Hadoop HDFS DataNode 50010 9866 Hadoop HDFS DataNode HTTP UI 50075 9864 Hadoop HDFS DataNode HTTPS UI 50475 9865 Hadoop服務啟動命令 123456789101112131415161718192021222324252627282930啟動方式1:使用start-*.sh-----[hadoop@hadoop-master:~$]start-dfs.sh[hadoop@hadoop-master:~$]start-yarn.sh[hadoop@hadoop-master:~$]jps2340 ResourceManager1834 NameNode2107 SecondaryNameNode[hadoop@hadoop-master:~$]ssh hadoop-slave1[hadoop@hadoop-slave1:~$]jps1568 DataNode1738 NodeManager[hadoop@hadoop-master:~$]ssh hadoop-slave2[hadoop@hadoop-slave2:~$]jps1715 NodeManager1545 DataNode啟動方式2:-----[hadoop@hadoop-master:~$]hdfs --daemon start namenode[hadoop@hadoop-master:~$]yarn --daemon start resourcemanager[hadoop@hadoop-master:~$]hdfs --daemon start proxyserver[hadoop@hadoop-master:~$]hdfs --daemon start httpfs[hadoop@hadoop-master:~$]mapred --daemon start historyserver-[hadoop@hadoop-slave1:~$]hdfs --daemon start datanode[hadoop@hadoop-slave1:~$]yarn --daemon start nodemanager-[hadoop@hadoop-slave2:~$]hdfs --daemon start datanode[hadoop@hadoop-slave2:~$]yarn --daemon start nodemanager Hadoop服務關閉 1234567891011121314151617181920212223停止方式1:使用stop-*.sh-----[hadoop@hadoop-master:~$]stop-dfs.sh[hadoop@hadoop-master:~$]stop-yarn.sh停止方式2:-----[hadoop@hadoop-master:~$]hdfs --daemon stop namenode[hadoop@hadoop-master:~$]yarn --daemon stop resourcemanager[hadoop@hadoop-master:~$]hdfs --daemon stop proxyserver[hadoop@hadoop-master:~$]hdfs --daemon stop httpfs[hadoop@hadoop-master:~$]mapred --daemon stop historyserver-[hadoop@hadoop-slave1:~$]hdfs --daemon stop datanode[hadoop@hadoop-slave1:~$]yarn --daemon stop nodemanager-[hadoop@hadoop-slave2:~$]hdfs --daemon stop datanode[hadoop@hadoop-slave2:~$]yarn --daemon stop nodemanager停止DataNode方式3:-----[hadoop@hadoop-master:~$]hdfs dfsadmin -shutdownDatanode 192.168.51.5:9867[hadoop@hadoop-master:~$]hdfs dfsadmin -shutdownDatanode 192.168.51.6:9867 Hadoop MapReduce Example 12[hadoop@hadoop-master:~$]hadoop jar /bgdt/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar pi 100 100000000[hadoop@hadoop-master:~$]hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar grep input output &apos;dfs[a-z.]+&apos; Hadoop Web UIweb-1 Hadoop WebUI-1 Port:9870,此頁面可以觀察Namenode相關資訊 web-2 Hadoop WebUI-2 此頁面可以觀察DataNode狀態 web-3 Hadoop WebUI-3 此頁面可以觀察HDFS檔案結構,新版的Browser可以直接由介面上上傳檔案,但先決條件為沒有設定相關權限 web-4 Hadoop WebUI-4,Port:8088,此頁面為ResourceManager的管理介面,系統會列出所有Application的運行狀態","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/categories/hadoop/"}],"tags":[]},{"title":"Excel之下拉式選單設計技巧-1","slug":"[excel]Excel_skill","date":"2017-12-07T16:00:00.000Z","updated":"2017-12-12T02:02:44.657Z","comments":true,"path":"2017/12/08/[excel]Excel_skill/","link":"","permalink":"http://yoursite.com/2017/12/08/[excel]Excel_skill/","excerpt":"","text":"Excel操作的小小筆記 開啟Excel,之後先建立一個名叫部門代碼表的Sheet 建立完工作表之後,需在”公式”-&gt;”名稱管理員”-&gt;”新增”中定義相關的名稱其中有需要注意以下幾個地方:1.名稱:部門代碼表2.範圍:整個活頁簿3.參照到:需要在要定義的工作表中選出定義的範圍(=部門代碼表!$A:$B)“=部門代碼表!”–&gt;意思是部門代碼表這個工作表(Sheet)“$A:$B”–&gt;表示工作表中的A欄和B欄全部選取的內容是A和B兩個欄位全部,因為我們會想要隨時增加部門進來所以不限定列數 我們再新增一個”部門名稱”的定義給名稱管理員,相關欄位同上圖,只是這次我們只選擇A欄這個部份的定義是要給下拉式選單當成選擇的項目 最後我們查看名稱管理員會出現兩個名稱定義(部門代碼表,部門名稱) 再來我們開啟另一個工作表,隨便選一個cell做為下拉式選單測試接著點選”資料”-&gt;”資料驗證”-&gt;”設定”然後”儲存格內允許”–&gt;清單“來源”–&gt;填入”=部門名稱”“儲存格內的下拉式選單”記得勾選 此時點選按鈕就會出現一個下拉式選單 最後一個步驟,選擇一個要呈現結果的cell,填入以下公式=VLOOKUP(A1,部門代碼表,2,FALSE)以下說明VLOOKUP(自動查找函數)相關參數查閱欄位：填入欄位編號查閱範圍：定義的範圍名稱欄位編號：取出欄位編號，在定義的範圍名稱欄位代號(由左至右1,2,….)是否完全符合：搜尋時是否要完全符合查詢欄位的值，TRUE-部分符合，FALSE-完全符合。","categories":[{"name":"Excel","slug":"Excel","permalink":"http://yoursite.com/categories/Excel/"}],"tags":[]},{"title":"Hive相關語法整理-3(Partition Table)","slug":"[hive]hive_sql_3","date":"2017-11-06T16:00:00.000Z","updated":"2017-11-07T09:55:02.515Z","comments":true,"path":"2017/11/07/[hive]hive_sql_3/","link":"","permalink":"http://yoursite.com/2017/11/07/[hive]hive_sql_3/","excerpt":"","text":"Create Partition Table CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name [(col_name data_type [COMMENT col_comment], … [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], …)] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY ‘storage.handler.class.name’ [WITH SERDEPROPERTIES (…)] ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, …)] [AS select_statement];1234567891011Example:create table page_view_7( viewTime int, userid bigint, page_url string, referrer_url string, ip string)partitioned by(dt string, country string)row format delimited fields terminated by &apos;,&apos;stored as textfile; Alter PartitionAdd Partition ALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION ‘location’][, PARTITION partition_spec [LOCATION ‘location’], …];123Example:ALTER TABLE page_view_7 ADD PARTITION (dt=&apos;2008-08-08&apos;, country=&apos;us&apos;);ALTER TABLE page_view_7 ADD PARTITION (dt=&apos;2017-11-01&apos;, country=&apos;us&apos;) location &quot;/user/hadoop/data/page_view_7/partitions&quot;; Rename Partition ALTER TABLE table_name PARTITION partition_spec RENAME TO PARTITION partition_spec;1234567891011121314Example:alter table page_view_7 partition (dt=&quot;2017-11-07&quot;,country=&quot;taiwan&quot;) rename to partition (dt=&quot;test_2017-11-07&quot;,country=&quot;taiwan&quot;);hive&gt; show partitions page_view_7;OKdt=2008-08-08/country=Taiwandt=2008-08-08/country=usdt=2008-08-09/country=usdt=2008-08-10/country=usdt=2017-08-08/country=taiwandt=2017-08-08/country=usdt=2017-11-01/country=usdt=test_2017-11-07/country=taiwanTime taken: 0.054 seconds, Fetched: 8 row(s) Exchange Partition ALTER TABLE table_name_2 EXCHANGE PARTITION (partition_spec) WITH TABLE table_name_1;123456789101112131415161718192021222324Example:alter table pv_7 exchange partition (dt=&quot;test_2017-11-07&quot;,country=&quot;taiwan&quot;) with table page_view_7;Demo:hive&gt; create table pv_7 like page_view_7;OKTime taken: 0.59 secondshive&gt; alter table pv_7 exchange partition (dt=&quot;test_2017-11-07&quot;,country=&quot;taiwan&quot;) with table page_view_7;OKTime taken: 0.297 secondshive&gt; show partitions page_view_7;OKdt=2008-08-08/country=Taiwandt=2008-08-08/country=usdt=2008-08-09/country=usdt=2008-08-10/country=usdt=2017-08-08/country=taiwandt=2017-08-08/country=usdt=2017-11-01/country=usTime taken: 0.058 seconds, Fetched: 7 row(s)hive&gt; show partitions pv_7;OKdt=test_2017-11-07/country=taiwanTime taken: 0.055 seconds, Fetched: 1 row(s) Drop Partition ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, …] [IGNORE PROTECTION] [PURGE];123456789101112131415161718192021222324252627Example:alter table page_view_7 drop if exists partition (dt=&quot;2017-11-01&quot;,country=&quot;us&quot;);Demo:hive&gt; show partitions page_view_7;OKdt=2008-08-08/country=Taiwandt=2008-08-08/country=usdt=2008-08-09/country=usdt=2008-08-10/country=usdt=2017-08-08/country=taiwandt=2017-08-08/country=usdt=2017-11-01/country=usTime taken: 0.054 seconds, Fetched: 7 row(s)hive&gt; alter table page_view_7 drop if exists partition (dt=&quot;2017-11-01&quot;,country=&quot;us&quot;);Dropped the partition dt=2017-11-01/country=usOKTime taken: 0.36 secondshive&gt; show partitions page_view_7;OKdt=2008-08-08/country=Taiwandt=2008-08-08/country=usdt=2008-08-09/country=usdt=2008-08-10/country=usdt=2017-08-08/country=taiwandt=2017-08-08/country=usTime taken: 0.052 seconds, Fetched: 6 row(s) Show Partition","categories":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/categories/hive/"}],"tags":[]},{"title":"Hive相關語法整理-1","slug":"[hive]hive_sql_1","date":"2017-11-05T16:00:00.000Z","updated":"2017-11-06T10:12:00.165Z","comments":true,"path":"2017/11/06/[hive]hive_sql_1/","link":"","permalink":"http://yoursite.com/2017/11/06/[hive]hive_sql_1/","excerpt":"","text":"Create Database create (database|schema) [if not exists] database_name [comment database_comment] [location hdfs_path] [with dbproperties (property_name=property_value, …)];12345678910Example:create database test1;create database if not exists test1;create database if not exists mydatabasewith dbproperties(&apos;Experiment Name&apos; = &apos;Correlation age/sentiment&apos;,&apos;date&apos; = &apos;2013-07-11&apos;,&apos;Lead Developer&apos; = &apos;John Foo&apos;,&apos;Lead Developer Email&apos; = &apos;jfoo@somewhere.com&apos;); Alter Database ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, …); ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path;123Example: alter database mydatabase set dbproperties(&quot;date&quot;=&quot;2017-11-06&quot;); alter database mydatabase set dbproperties(&quot;test&quot;=&quot;2017-11-06&quot;); Show Database 123Example:show databases;show databases like &quot;test*&quot;; Desc Database describe database [extended] db_name;123Example:desc database test_db;desc database extended test_db; Use Database use db_name12Example:use test1; Drop Database drop (database|schema) [if exists] database_name [restrict|cascade];123Example:drop database if exists test_db; ##Database內有Table的話無法drop,必須是要空的Databasedrop database test_db cascade; ##Database刪除時會Database內所有的Table資料一併刪除","categories":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/categories/hive/"}],"tags":[]},{"title":"Hive相關語法整理-2(Table)","slug":"[hive]hive_sql_2","date":"2017-11-05T16:00:00.000Z","updated":"2017-11-07T10:27:57.726Z","comments":true,"path":"2017/11/06/[hive]hive_sql_2/","link":"","permalink":"http://yoursite.com/2017/11/06/[hive]hive_sql_2/","excerpt":"","text":"Create Table CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name [(col_name data_type [COMMENT col_comment], … [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], …)] [CLUSTERED BY (col_name, col_name, …) [SORTED BY (col_name [ASC|DESC], …)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, …) – (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, …), (col_value, col_value, …), …) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY ‘storage.handler.class.name’ [WITH SERDEPROPERTIES (…)] – (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, …)] – (Note: Available in Hive 0.6.0 and later) [AS select_statement]; – (Note: Available in Hive 0.5.0 and later; not supported for external tables) CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path]; 12345678Example1:以下範例為Table建立各種型態欄位create table test1(a1 tinyint,a2 smallint,a3 int,a4 bigint);create table test2(a1 float,a2 double);create table test3(a1 string,a2 varchar(2000),a3 char(4));create table test4(a1 boolean,a2 date,a3 timestamp);create table test5(a1 map&lt;string,string&gt;,a2 array&lt;string&gt;,a3 struct&lt;c1:string,c2:int,c3:date&gt;);create table test5_1(a1 map&lt;string,string&gt;,a2 array&lt;string&gt;,a3 struct&lt;c1:string,c2:int,c3:date&gt;,a4 uniontype&lt;string,int,date&gt;); 1234Example2:建立Partition Tablecreate table test6_1(a1 string,a2 int)partitioned by(a3 string); 12345Example3:建立Bluck Tablecreate table test6_2(a1 string,a2 int)clustered by(a2) into 5 buckets; 123Example4:CTAS 用法範例create table test7 as select &quot;ttt&quot;, concat(&quot;part_&quot;, &quot;aaa&quot;); 123Example5:建立外部表格,表格刪除時Table檔案並不會跟著一起被刪除create external table if not exists test8(a1 string,a2 int); 123Example6:建立暫存表格create temporary table if not exists test9(a1 string,a2 int); 123Example7:建立與某一個表格相同欄位的表格 (copy 某個Table的欄位)create table test10 like test1; --copy columns from table; 123456789101112131415161718Example8:create table taiwan_area( cityname string COMMENT &quot;行政區名稱&quot;, type string COMMENT &quot;行政區類別&quot;, area float COMMENT &quot;區域面積&quot;, village_num smallint COMMENT &quot;村里數&quot;, neighborhood_num smallint COMMENT &quot;鄰數&quot;, people_num int COMMENT &quot;人口數&quot;, density int COMMENT &quot;人口密度&quot;, zipcode smallint COMMENT &quot;郵遞區號&quot;)row format serde &apos;org.apache.hadoop.hive.serde2.OpenCSVSerde&apos;with SERDEPROPERTIES(&quot;separatorChar&quot; = &quot;,&quot;,&quot;quoteChar&quot; = &quot;\\&quot;&quot;,&quot;escapeChar&quot; = &quot;\\\\&quot;)STORED AS TEXTFILE; 123456789101112Result:hive&gt; desc taiwan_area;OKcityname string from deserializertype string from deserializerarea string from deserializervillage_num string from deserializerneighborhood_num string from deserializerpeople_num string from deserializerdensity string from deserializerzipcode string from deserializerTime taken: 0.254 seconds, Fetched: 8 row(s) 1234567891011121314Example9:create table page_view( viewTime int, userid bigint, page_url string, referrer_url string, ip string comment &apos;IP Address of the User&apos;)comment &apos;This is the page view table&apos;partitioned by(dt string, country string)row format delimited fields terminated by &apos;\\001&apos; collection items terminated by &apos;\\002&apos; map keys terminated by &apos;\\003&apos;stored as textfile; 1234567891011121314151617Result:hive&gt; desc page_view;OKviewtime intuserid bigintpage_url stringreferrer_url stringip string IP Address of the Userdt stringcountry string# Partition Information# col_name data_type commentdt stringcountry stringTime taken: 0.193 seconds, Fetched: 13 row(s) Example10:12345678Prepare CSV file:CSV檔內容如下:test1.csvecho 100,1,\\&quot;http://www.google.com\\&quot;,\\&quot;http://www.google.com\\&quot;,\\&quot;192.168.1.1\\&quot; &gt; ~/test1.csvecho 200,2,\\&quot;http://www.google.com\\&quot;,\\&quot;http://www.google.com\\&quot;,\\&quot;192.168.1.2\\&quot; &gt;&gt; ~/test1.csvecho 150,3,\\&quot;http://www.google.com\\&quot;,\\&quot;http://www.google.com\\&quot;,\\&quot;192.168.1.3\\&quot; &gt;&gt; ~/test1.csvecho 300,4,\\&quot;http://www.google.com\\&quot;,\\&quot;http://www.google.com\\&quot;,\\&quot;192.168.1.4\\&quot; &gt;&gt; ~/test1.csv直接在Linux系統上使用指令建立以上檔案 1234567891011create table page_view_1( viewTime int, userid bigint, page_url string, referrer_url string, ip string comment &apos;IP Address of the User&apos;)comment &apos;This is the page view table&apos;partitioned by(dt string, country string)row format delimited fields terminated by &apos;,&apos;stored as textfile; 12使用Load將資料insert至Tableload data local inpath &quot;/home/hadoop/test1.csv&quot; overwrite into table page_view_1 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;); 12345678Result:hive&gt; select * from page_view_1;OK100 1 &quot;http://www.google.com&quot; &quot;http://www.google.com&quot; &quot;192.168.1.1&quot; taipei taiwan200 2 &quot;http://www.google.com&quot; &quot;http://www.google.com&quot; &quot;192.168.1.2&quot; taipei taiwan150 3 &quot;http://www.google.com&quot; &quot;http://www.google.com&quot; &quot;192.168.1.3&quot; taipei taiwan300 4 &quot;http://www.google.com&quot; &quot;http://www.google.com&quot; &quot;192.168.1.4&quot; taipei taiwanTime taken: 0.335 seconds, Fetched: 4 row(s) 12345678910Example11Step1:建立CSV檔test2.csvecho 1,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;404\\&quot;,100 &gt;~/test2.csvecho 2,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;200\\&quot;,100 &gt;&gt;~/test2.csvecho 3,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;500\\&quot;,100 &gt;&gt;~/test2.csvecho 4,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;503\\&quot;,100 &gt;&gt;~/test2.csvecho 5,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;201\\&quot;,100 &gt;&gt;~/test2.csvecho 6,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;403\\&quot;,100 &gt;&gt;~/test2.csv 1234567891011Step2:create table page_view_2( userid bigint, data struct&lt;page_url:string,ip:string,status:string&gt;, view_count int)comment &apos;This is the page view table&apos;partitioned by(dt string, country string)row format delimited fields terminated by &apos;,&apos; collection items terminated by &apos;\\;&apos;stored as textfile; 12Step3:load data local inpath &quot;/home/hadoop/test2.csv&quot; overwrite into table page_view_2 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;); 12345678910Step4:hive&gt; select * from page_view_2;OK1 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;404\\&quot;&quot;&#125; 100 taipei taiwan2 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;200\\&quot;&quot;&#125; 100 taipei taiwan3 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;500\\&quot;&quot;&#125; 100 taipei taiwan4 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;503\\&quot;&quot;&#125; 100 taipei taiwan5 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;201\\&quot;&quot;&#125; 100 taipei taiwan6 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;403\\&quot;&quot;&#125; 100 taipei taiwanTime taken: 2.183 seconds, Fetched: 6 row(s) 1234567891011121314151617Example12:Step1:create table page_view_3( viewTime int, userid bigint, page_url string, referrer_url string, ip string comment &apos;IP Address of the User&apos;)comment &apos;This is the page view table&apos;partitioned by(dt string, country string)row format serde &apos;org.apache.hadoop.hive.serde2.OpenCSVSerde&apos;with serdeproperties(&quot;separatorChar&quot; = &quot;,&quot;,&quot;quoteChar&quot; = &quot;\\&quot;&quot;,&quot;escapeChar&quot; = &quot;\\\\&quot;) stored as textfile; 12Step2:load data local inpath &quot;/home/hadoop/test1.csv&quot; overwrite into table page_view_3 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;); 12345678Step3:hive&gt; select * from page_view_3;OK100 1 http://www.google.com http://www.google.com 192.168.1.1 taipei taiwan200 2 http://www.google.com http://www.google.com 192.168.1.2 taipei taiwan150 3 http://www.google.com http://www.google.com 192.168.1.3 taipei taiwan300 4 http://www.google.com http://www.google.com 192.168.1.4 taipei taiwanTime taken: 1.003 seconds, Fetched: 4 row(s) 12345678910Example13使用delimited切割欄位,以分號來做為集合區分Step1:echo 1,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;100\\&quot;,\\&quot;001\\&quot;\\;\\&quot;test1_001\\&quot;\\;100 &gt;~/test4.csvecho 2,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.2\\&quot;\\;\\&quot;200\\&quot;,\\&quot;002\\&quot;\\;\\&quot;test1_002\\&quot;\\;200 &gt;&gt;~/test4.csvecho 3,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.3\\&quot;\\;\\&quot;300\\&quot;,\\&quot;003\\&quot;\\;\\&quot;test1_003\\&quot;\\;300 &gt;&gt;~/test4.csvecho 4,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.4\\&quot;\\;\\&quot;400\\&quot;,\\&quot;004\\&quot;\\;\\&quot;test1_004\\&quot;\\;400 &gt;&gt;~/test4.csvecho 5,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.5\\&quot;\\;\\&quot;500\\&quot;,\\&quot;005\\&quot;\\;\\&quot;test1_005\\&quot;\\;500 &gt;&gt;~/test4.csvecho 6,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.6\\&quot;\\;\\&quot;600\\&quot;,\\&quot;006\\&quot;\\;\\&quot;test1_006\\&quot;\\;600 &gt;&gt;~/test4.csvecho 7,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.7\\&quot;\\;\\&quot;700\\&quot;,\\&quot;007\\&quot;\\;\\&quot;test1_007\\&quot;\\;700 &gt;&gt;~/test4.csv 12345678910111213Step2:建立Tablecreate table page_view_4( id bigint, data struct&lt;page_url:string,ip:string,status:string&gt;, userdata struct&lt;id:string,name:string,viewcnt:int&gt;)comment &apos;This is the page view table&apos;partitioned by(dt string, country string)row format delimited fields terminated by &apos;,&apos; collection items terminated by &quot;\\;&quot;stored as textfile; 123Step3:load data to page_view_4load data local inpath &quot;/home/hadoop/test4.csv&quot; overwrite into table page_view_4 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;); 1234567891011Step4:hive&gt; select * from page_view_4;OK1 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;100\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;001\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_001\\&quot;&quot;,&quot;viewcnt&quot;:100&#125; taipei taiwan2 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.2\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;200\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;002\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_002\\&quot;&quot;,&quot;viewcnt&quot;:200&#125; taipei taiwan3 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.3\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;300\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;003\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_003\\&quot;&quot;,&quot;viewcnt&quot;:300&#125; taipei taiwan4 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.4\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;400\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;004\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_004\\&quot;&quot;,&quot;viewcnt&quot;:400&#125; taipei taiwan5 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.5\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;500\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;005\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_005\\&quot;&quot;,&quot;viewcnt&quot;:500&#125; taipei taiwan6 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.6\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;600\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;006\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_006\\&quot;&quot;,&quot;viewcnt&quot;:600&#125; taipei taiwan7 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.7\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;700\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;007\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_007\\&quot;&quot;,&quot;viewcnt&quot;:700&#125; taipei taiwanTime taken: 0.13 seconds, Fetched: 7 row(s) 123456789Example14:使用冒號(:)來做為Map的key,value的區分範例Step1:echo 1,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;401\\&quot;,id:001\\;name:test1_001\\;count:100 &gt; ~/test5.csvecho 2,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.2\\&quot;\\;\\&quot;401\\&quot;,id:002\\;name:test1_002\\;count:200 &gt;&gt; ~/test5.csvecho 3,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.3\\&quot;\\;\\&quot;401\\&quot;,id:003\\;name:test1_003\\;count:300 &gt;&gt; ~/test5.csvecho 4,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.4\\&quot;\\;\\&quot;401\\&quot;,id:004\\;name:test1_004\\;count:400 &gt;&gt; ~/test5.csvecho 5,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.5\\&quot;\\;\\&quot;401\\&quot;,id:005\\;name:test1_005\\;count:500 &gt;&gt; ~/test5.csvecho 6,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.6\\&quot;\\;\\&quot;401\\&quot;,id:006\\;name:test1_006\\;count:600 &gt;&gt; ~/test5.csv 12345678910111213Step2:create table page_view_5( id bigint, data struct&lt;page_url:string,ip:string,status:string&gt;, userdata map&lt;string,string&gt;)comment &apos;This is the page view table&apos;partitioned by(dt string, country string)row format delimited fields terminated by &apos;,&apos; collection items terminated by &quot;\\;&quot; map keys terminated by &quot;\\:&quot;stored as textfile; 12Step3:load data local inpath &quot;/home/hadoop/test5.csv&quot; overwrite into table page_view_5 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;); 12345678910Step4:hive&gt; select * from page_view_5;OK1 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;001&quot;,&quot;name&quot;:&quot;test1_001&quot;,&quot;count&quot;:&quot;100&quot;&#125; taipei taiwan2 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.2\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;002&quot;,&quot;name&quot;:&quot;test1_002&quot;,&quot;count&quot;:&quot;200&quot;&#125; taipei taiwan3 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.3\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;003&quot;,&quot;name&quot;:&quot;test1_003&quot;,&quot;count&quot;:&quot;300&quot;&#125; taipei taiwan4 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.4\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;004&quot;,&quot;name&quot;:&quot;test1_004&quot;,&quot;count&quot;:&quot;400&quot;&#125; taipei taiwan5 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.5\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;005&quot;,&quot;name&quot;:&quot;test1_005&quot;,&quot;count&quot;:&quot;500&quot;&#125; taipei taiwan6 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.6\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;006&quot;,&quot;name&quot;:&quot;test1_006&quot;,&quot;count&quot;:&quot;600&quot;&#125; taipei taiwanTime taken: 1.217 seconds, Fetched: 6 row(s) 1234567Example15:Step1:echo 1,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;401\\&quot;,id:001\\;name:test1_001\\;gender:F\\;count:100,300\\;400\\;500 &gt; ~/test6.csvecho 2,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.2\\&quot;\\;\\&quot;401\\&quot;,id:002\\;name:test1_002\\;gender:M\\;count:200,600\\;700\\;800 &gt;&gt; ~/test6.csvecho 3,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.3\\&quot;\\;\\&quot;401\\&quot;,id:003\\;name:test1_003\\;gender:F\\;count:300,100\\;200\\;300 &gt;&gt; ~/test6.csvecho 4,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.4\\&quot;\\;\\&quot;401\\&quot;,id:004\\;name:test1_004\\;gender:M\\;count:400,400\\;500\\;600 &gt;&gt; ~/test6.csvecho 5,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.5\\&quot;\\;\\&quot;401\\&quot;,id:005\\;name:test1_005\\;gender:F\\;count:500,250\\;100\\;900 &gt;&gt; ~/test6.csv 1234567891011121314Step2:create table page_view_6( id bigint, data struct&lt;page_url:string,ip:string,status:string&gt;, userdata map&lt;string,string&gt;, countlist array&lt;int&gt;)comment &apos;This is the page view table&apos;partitioned by(city string, country string)row format delimited fields terminated by &apos;,&apos; collection items terminated by &quot;\\;&quot; map keys terminated by &quot;\\:&quot;stored as textfile; 12Step3:load data local inpath &quot;/home/hadoop/test6.csv&quot; overwrite into table page_view_6 partition(city=&quot;taipei&quot;,country=&quot;taiwan&quot;); 123456789Step4:hive&gt; select * from page_view_6;OK1 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;001&quot;,&quot;name&quot;:&quot;test1_001&quot;,&quot;gender&quot;:&quot;F&quot;,&quot;count&quot;:&quot;100&quot;&#125; [300,400,500] taipei taiwan2 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.2\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;002&quot;,&quot;name&quot;:&quot;test1_002&quot;,&quot;gender&quot;:&quot;M&quot;,&quot;count&quot;:&quot;200&quot;&#125; [600,700,800] taipei taiwan3 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.3\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;003&quot;,&quot;name&quot;:&quot;test1_003&quot;,&quot;gender&quot;:&quot;F&quot;,&quot;count&quot;:&quot;300&quot;&#125; [100,200,300] taipei taiwan4 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.4\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;004&quot;,&quot;name&quot;:&quot;test1_004&quot;,&quot;gender&quot;:&quot;M&quot;,&quot;count&quot;:&quot;400&quot;&#125; [400,500,600] taipei taiwan5 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.5\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;005&quot;,&quot;name&quot;:&quot;test1_005&quot;,&quot;gender&quot;:&quot;F&quot;,&quot;count&quot;:&quot;500&quot;&#125; [250,100,900] taipei taiwanTime taken: 1.126 seconds, Fetched: 5 row(s) Alter TableRename Table Name alter table table_name rename to new_table_name;1234Example1:將Table更改名稱或搬移至其他資料庫alter table page_view_3 rename to pv_3;alter table pv_3 rename to test.pv_3; Alter Table Properties alter table table_name set tblproperties table_properties;123Example1:修改Table的屬性alter table test.pv_3 set tblproperties (&quot;comment&quot; = &quot;page_view_3 rename to pv_3&quot;); Add SerDe Properties ALTER TABLE table_name [PARTITION partition_spec] SET SERDE serde_class_name [WITH SERDEPROPERTIES serde_properties]; ALTER TABLE table_name [PARTITION partition_spec] SET SERDEPROPERTIES serde_properties; serde_properties: : (property_name = property_value, property_name = property_value, … )1alter table test.pv_3 set serdeproperties(&apos;field.delim&apos;=&apos;\\u0001&apos;); Alter Either Table or Partition ALTER TABLE table_name [PARTITION partition_spec] SET FILEFORMAT file_format;file_format: :sequence,textfile,rcfile,orc12Example1alter table test.pv_3 set fileformat textfile; ALTER TABLE table_name [PARTITIONpartitionSpec] SET LOCATION “newlocation”修改表格存放路徑12Example:alter table test.pv_3 set location &quot;/user/hadoop/data&quot; Import Data to Tableload data LOAD DATA [local] INPATH ‘hdfs_file_or_directory_path’ [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 …)]1234567891011121314151617181920212223Example1:load data local inpath &quot;/home/hadoop/test1.csv&quot; overwrite into table page_view_3 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;);Result:hive&gt; select * from page_view_3;OKTime taken: 0.106 secondshive&gt; load data local inpath &quot;/home/hadoop/test1.csv&quot; overwrite into table page_view_3 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;);Loading data to table default.page_view_3 partition (dt=taipei, country=taiwan)OKTime taken: 0.658 secondshive&gt; select * from page_view_3;OK100 1 http://www.google.com http://www.google.com 192.168.1.1 taipei taiwan200 2 http://www.google.com http://www.google.com 192.168.1.2 taipei taiwan150 3 http://www.google.com http://www.google.com 192.168.1.3 taipei taiwan300 4 http://www.google.com http://www.google.com 192.168.1.4 taipei taiwanhive&gt; truncate table page_view_3;OKTime taken: 0.345 secondshive&gt; select * from page_view_3;OKTime taken: 0.143 seconds 123456789101112131415161718Example2:load data inpath &quot;/user/hadoop/data/test1.csv&quot; overwrite into table page_view_3 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;);Result:hive&gt; select * from page_view_3;OKTime taken: 0.129 secondshive&gt; load data inpath &quot;/user/hadoop/data/test1.csv&quot; overwrite into table page_view_3 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;);Loading data to table default.page_view_3 partition (dt=taipei, country=taiwan)OKTime taken: 0.647 secondshive&gt; select * from page_view_3;OK100 1 http://www.google.com http://www.google.com 192.168.1.1 taipei taiwan200 2 http://www.google.com http://www.google.com 192.168.1.2 taipei taiwan150 3 http://www.google.com http://www.google.com 192.168.1.3 taipei taiwan300 4 http://www.google.com http://www.google.com 192.168.1.4 taipei taiwanTime taken: 0.109 seconds, Fetched: 4 row(s) create table ….as select create table table_name as select_statement1234567891011121314Example:複製一個Table到新Table中(包含資料)create table test.pv_4 as select * from page_view_4;hive&gt; select * from test.pv_4;OK1 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;100\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;001\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_001\\&quot;&quot;,&quot;viewcnt&quot;:100&#125; taipei taiwan2 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.2\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;200\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;002\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_002\\&quot;&quot;,&quot;viewcnt&quot;:200&#125; taipei taiwan3 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.3\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;300\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;003\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_003\\&quot;&quot;,&quot;viewcnt&quot;:300&#125; taipei taiwan4 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.4\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;400\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;004\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_004\\&quot;&quot;,&quot;viewcnt&quot;:400&#125; taipei taiwan5 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.5\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;500\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;005\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_005\\&quot;&quot;,&quot;viewcnt&quot;:500&#125; taipei taiwan6 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.6\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;600\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;006\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_006\\&quot;&quot;,&quot;viewcnt&quot;:600&#125; taipei taiwan7 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.7\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;700\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;007\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_007\\&quot;&quot;,&quot;viewcnt&quot;:700&#125; taipei taiwanTime taken: 0.087 seconds, Fetched: 7 row(s) create table …like CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path];12345678Example:複製一個Table到新Table中(僅 columns)create table test.pv_4_like like page_view_4;create table if not exists test.pv_4_new like page_view_4;hive&gt; select * from test.pv_4_new;OKTime taken: 0.099 seconds from …insert select FROM from_statementINSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 …) [IF NOT EXISTS]] select_statement1[INSERT OVERWRITE TABLE tablename2 [PARTITION … [IF NOT EXISTS]] select_statement2][INSERT INTO TABLE tablename2 [PARTITION …] select_statement2] …;1234Example:from page_view_3 insert overwrite table page_view_3_1 partition(dt=&quot;taipei&quot;,country=&quot;test1&quot;) if not exists select page_url,viewTime,userid,viewTime,ip; Show TableShow Tables SHOW TABLES [IN database_name] [‘identifier_with_wildcards’];123456789101112131415161718192021222324252627282930313233343536373839Demo:hive&gt; show tables from test;OKlist_bucket_singlepv_3pv_4pv_4_likepv_4_newtest_atest_btest_ctest_dtest_etest_fTime taken: 0.024 seconds, Fetched: 11 row(s)hive&gt; show tables in test;OKlist_bucket_singlepv_3pv_4pv_4_likepv_4_newtest_atest_btest_ctest_dtest_etest_fTime taken: 0.018 seconds, Fetched: 11 row(s)hive&gt; show tables in test like &quot;test*&quot;;OKtest_atest_btest_ctest_dtest_etest_fTime taken: 0.019 seconds, Fetched: 6 row(s)hive&gt; Show Table SHOW TABLE EXTENDED [IN|FROM database_name] LIKE ‘identifier_with_wildcards’ [PARTITION(partition_spec)];12345678910111213141516171819202122Example:show table extended from test like &quot;pv_3&quot;Demo:hive&gt; show table extended from test like &quot;pv_3&quot;;OKtableName:pv_3owner:hadooplocation:hdfs://mna1:8020/user/hadoop/datainputformat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormatoutputformat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormatcolumns:struct columns &#123; i32 viewtime, i64 userid, string page_url, string referrer_url, string ip&#125;partitioned:truepartitionColumns:struct partition_columns &#123; string dt, string country&#125;totalNumberFiles:1totalFileSize:272maxFileSize:272minFileSize:272lastAccessTime:1510037040061lastUpdateTime:1510043296966Time taken: 0.037 seconds, Fetched: 15 row(s) Show Table Properties 12345678910Example:hive&gt; SHOW TBLPROPERTIES page_view_7;OKtransient_lastDdlTime 1510042890Time taken: 0.029 seconds, Fetched: 1 row(s)hive&gt; SHOW TBLPROPERTIES page_view_7(&quot;transient_lastDdlTime&quot;);OK1510042890Time taken: 0.029 seconds, Fetched: 1 row(s) Show Create Table SHOW CREATE TABLE ([db_name.]table_name|view_name);1234567891011121314151617181920212223242526272829Example:show create table page_view_7;Demo:hive&gt; show create table page_view_7;OKCREATE TABLE `page_view_7`( `viewtime` int, `userid` bigint, `page_url` string, `referrer_url` string, `ip` string)PARTITIONED BY ( `dt` string, `country` string)ROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&apos;WITH SERDEPROPERTIES ( &apos;field.delim&apos;=&apos;,&apos;, &apos;serialization.format&apos;=&apos;,&apos;)STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos;OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;LOCATION &apos;hdfs://mna1:8020/user/hive/warehouse/page_view_7&apos;TBLPROPERTIES ( &apos;transient_lastDdlTime&apos;=&apos;1510042890&apos;)Time taken: 0.126 seconds, Fetched: 22 row(s) Describe Tabledesc formatted DESCRIBE [EXTENDED|FORMATTED] [db_name.]table_name[.col_name ( [.field_name] | [.’$elem$’] | [.’$key$’] | [.’$value$’] )* ];1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950Example:desc formatted page_view_7;demo:desc formatted page_view_7;OK# col_name data_type commentviewtime intuserid bigintpage_url stringreferrer_url stringip string# Partition Information# col_name data_type commentdt stringcountry string# Detailed Table InformationDatabase: defaultOwner: hadoopCreateTime: Tue Nov 07 16:21:30 CST 2017LastAccessTime: UNKNOWNRetention: 0Location: hdfs://mna1:8020/user/hive/warehouse/page_view_7Table Type: MANAGED_TABLETable Parameters: transient_lastDdlTime 1510042890# Storage InformationSerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDeInputFormat: org.apache.hadoop.mapred.TextInputFormatOutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormatCompressed: NoNum Buckets: -1Bucket Columns: []Sort Columns: []Storage Desc Params: field.delim , serialization.format ,Time taken: 0.105 seconds, Fetched: 36 row(s)hive&gt; desc formatted page_view_7 page_url;OK# col_name data_type min max num_nulls distinct_count avg_col_len max_col_len num_trues num_falses commentpage_url string from deserializerTime taken: 0.043 seconds, Fetched: 3 row(s) desc extended desc [extended] [db_name.]table_name[.col_name];123456789101112131415161718192021222324252627282930Example:hive&gt; desc extended page_view_5 userdata;OKuserdata map&lt;string,string&gt; from deserializerTime taken: 0.03 seconds, Fetched: 1 row(s)hive&gt; desc extended page_view_5 userdata.$key$;OK$key$ string from deserializerTime taken: 0.029 seconds, Fetched: 1 row(s)hive&gt; desc extended page_view_5 userdata.$value$;OK$value$ string from deserializerTime taken: 0.03 seconds, Fetched: 1 row(s)hive&gt; desc formatted page_view_6;OK# col_name data_type commentid bigintdata struct&lt;page_url:string,ip:string,status:string&gt;userdata map&lt;string,string&gt;countlist array&lt;int&gt;hive&gt; desc extended page_view_6 countlist.$elem$;OK$elem$ int from deserializerTime taken: 0.031 seconds, Fetched: 1 row(s)hive&gt; desc extended page_view_6 data.ip;OKip string from deserializerTime taken: 0.064 seconds, Fetched: 1 row(s) Drop Tabledrop table drop table [db_name.]table_name;1234567891011121314151617Example:hive&gt; show tables from test like &quot;pv*&quot;;OKpv_3pv_4pv_4_likepv_4_newTime taken: 0.017 seconds, Fetched: 4 row(s)hive&gt; drop table test.pv_3;OKTime taken: 1.598 secondshive&gt; show tables from test like &quot;pv*&quot;;OKpv_4pv_4_likepv_4_newTime taken: 0.018 seconds, Fetched: 3 row(s) truncate table truncate table [db_name.]table_name;1234567891011121314151617181920212223Example:hive&gt; select * from test.pv_4;OK1 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;100\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;001\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_001\\&quot;&quot;,&quot;viewcnt&quot;:100&#125; taipei taiwan2 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.2\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;200\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;002\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_002\\&quot;&quot;,&quot;viewcnt&quot;:200&#125; taipei taiwan3 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.3\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;300\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;003\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_003\\&quot;&quot;,&quot;viewcnt&quot;:300&#125; taipei taiwan4 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.4\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;400\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;004\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_004\\&quot;&quot;,&quot;viewcnt&quot;:400&#125; taipei taiwan5 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.5\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;500\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;005\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_005\\&quot;&quot;,&quot;viewcnt&quot;:500&#125; taipei taiwan6 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.6\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;600\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;006\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_006\\&quot;&quot;,&quot;viewcnt&quot;:600&#125; taipei taiwan7 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.7\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;700\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;007\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_007\\&quot;&quot;,&quot;viewcnt&quot;:700&#125; taipei taiwanTime taken: 0.071 seconds, Fetched: 7 row(s)hive&gt; truncate table test.pv_4;OKTime taken: 0.124 secondshive&gt; select * from test.pv_4;OKTime taken: 0.063 secondshive&gt; show tables from test like &quot;pv*&quot;;OKpv_4pv_4_likepv_4_newTime taken: 0.018 seconds, Fetched: 3 row(s) multi drop table 123456789101112Example:hive&gt; show tables from test like &quot;pv*&quot;;OKpv_4pv_4_likepv_4_newTime taken: 0.017 seconds, Fetched: 3 row(s)[hadoop@mna1 ~]$ hive -e &quot;show tables from test like &apos;pv*&apos;&quot; |xargs -I &apos;&#123;&#125;&apos; hive -e &quot;drop table test.&#123;&#125;&quot;hive&gt; show tables from test like &quot;pv*&quot;;OKTime taken: 1.548 seconds","categories":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/categories/hive/"}],"tags":[]},{"title":"透過vagrant打包一個VM Box","slug":"[vagrant]打造一個自己開發境境","date":"2017-07-18T16:00:00.000Z","updated":"2017-07-18T07:15:28.059Z","comments":true,"path":"2017/07/19/[vagrant]打造一個自己開發境境/","link":"","permalink":"http://yoursite.com/2017/07/19/[vagrant]打造一個自己開發境境/","excerpt":"","text":"Download &amp; Install Virtual BoxDownload Virtual Box Install EXE file Install VirtualBox Modify Virtual Box Preferences點選”File”–&gt;”Preferences”修改預設機器資料夾位置 DownLoad &amp; Install VagrantDownload vagrant_1.9.1.msi Install vagrant Setting VAGRANT_HOME Create VM DownLoad Ubuntu iso &amp; Install Ubuntu LinuxDownload ubuntu-17.04-server-amd64.iso Install Linux Server restart Linux ServerSetting Ubuntu LinuxModify LocalesModify visudoCreate SSH KeyInstall VBoxGuestAdditionvagrant package &amp; addCreate vagrantfile(vagrant init)vagrant up","categories":[{"name":"vagrant","slug":"vagrant","permalink":"http://yoursite.com/categories/vagrant/"}],"tags":[]},{"title":"Spark程式開發系列_for Java1.8_001","slug":"[spark]spark_001","date":"2017-05-21T16:00:00.000Z","updated":"2017-05-22T11:01:54.357Z","comments":true,"path":"2017/05/22/[spark]spark_001/","link":"","permalink":"http://yoursite.com/2017/05/22/[spark]spark_001/","excerpt":"","text":"SparkSession Example 1 使用vi,或先用記事本建立以下json檔案(我的是建立在/home/vagrant/src/job/person5_data.json) 12&#123;&quot;id&quot;:&quot;0000000001&quot;,&quot;name&quot;:&quot;Test001&quot;,&quot;gender&quot;:&quot;M&quot;,&quot;age&quot;:&quot;40&quot;,&quot;telphone&quot;:&quot;123456789&quot;,&quot;address&quot;:&#123;&quot;country&quot;:&quot;Taiwan&quot;,&quot;city&quot;:&quot;Taipei&quot;,&quot;state&quot;:&quot;Sanxia&quot;,&quot;street&quot;:&quot;test.&quot;&#125;,&quot;child&quot;:[&#123;&quot;age&quot;:&quot;5&quot;,&quot;name&quot;:&quot;sub_001&quot;,&quot;gender&quot;:&quot;M&quot;&#125;,&#123;&quot;age&quot;:&quot;6&quot;,&quot;name&quot;:&quot;sub_002&quot;,&quot;gender&quot;:&quot;M&quot;&#125;]&#125;,&#123;&quot;id&quot;:&quot;0000000002&quot;,&quot;name&quot;:&quot;Test002&quot;,&quot;gender&quot;:&quot;F&quot;,&quot;age&quot;:&quot;29&quot;,&quot;telphone&quot;:&quot;223456670&quot;,&quot;address&quot;:&#123;&quot;country&quot;:&quot;Taiwan&quot;,&quot;city&quot;:&quot;Taipei&quot;,&quot;state&quot;:&quot;Sanzhi&quot;,&quot;street&quot;:&quot;test.&quot;&#125;,&quot;child&quot;:[&#123;&quot;age&quot;:&quot;7&quot;,&quot;name&quot;:&quot;sub_003&quot;,&quot;gender&quot;:&quot;M&quot;&#125;,&#123;&quot;age&quot;:&quot;8&quot;,&quot;name&quot;:&quot;sub_004&quot;,&quot;gender&quot;:&quot;F&quot;&#125;]&#125; 將person5_data.json檔案放到HDFS目錄上 1hadoop fs -put -f /home/vagrant/src/job/person5_data.json /user/hadoop/data Create SparkSession 123456789101112131415161718192021222324252627282930import org.apache.spark.sql.AnalysisException;import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;import org.apache.spark.sql.SaveMode;import org.apache.spark.sql.SparkSession;import org.apache.spark.sql.catalog.Database;public class SparkExample0 &#123; public static void main(String[] args) throws AnalysisException &#123; //1. Create SparkSession SparkSession spark = SparkSession.builder() .master(&quot;local&quot;) .config(&quot;spark.executor.memory&quot;, &quot;2g&quot;) .appName(&quot;SparkExample0&quot;) .enableHiveSupport() .getOrCreate(); //2. set new runtime options spark.conf().set(&quot;spark.sql.shuffle.partitions&quot;, 6); //3. SparkSession-透過read方式來讀取各種不同來源的資料 Dataset&lt;Row&gt; ds1 = spark.read().format(&quot;json&quot;).load(&quot;/user/hadoop/data/person5_data.json&quot;); ds1.printSchema(); //4. 將檔案存成Hive資料表 ds1.repartition(1).write().mode(SaveMode.Overwrite).saveAsTable(&quot;example.person_data5&quot;); &#125;&#125; 使用spark-submit執行Java Spark程式 1spark-submit --class &quot;SparkExample0&quot; /home/vagrant/src/job/Spark2Example.jar","categories":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/categories/Spark/"}],"tags":[]},{"title":"Vagrant file 配置檔案說明","slug":"[vagrant]Vagrantfile配置檔說明","date":"2017-02-19T16:00:00.000Z","updated":"2017-02-20T06:02:47.965Z","comments":true,"path":"2017/02/20/[vagrant]Vagrantfile配置檔說明/","link":"","permalink":"http://yoursite.com/2017/02/20/[vagrant]Vagrantfile配置檔說明/","excerpt":"","text":"vagrant file配置檔案說明 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# -*- mode: ruby -*-# vi: set ft=ruby :MASTER_IP = &apos;192.168.51.4&apos;DN1_IP = &apos;192.168.51.5&apos;DN2_IP = &apos;192.168.51.6&apos;Vagrant.configure(&quot;2&quot;) do |config| config.ssh.insert_key =false #define data1 server config.vm.define &quot;slave1&quot; do |slave1| slave1.vm.hostname = &quot;hadoop-slave1&quot; ##設定VM主機名稱 slave1.vm.network &quot;private_network&quot;, ip: DN1_IP ##設定主機 ip slave1.vm.box = &quot;ubuntu/yakkety64&quot; ##設定VM使用的OS Box slave1.vm.synced_folder &quot;.&quot;, &quot;/home/vagrant/src&quot;, mount_options: [&quot;dmode=775,fmode=664&quot;] ##設定公用分享目錄 slave1.vm.provider &quot;virtualbox&quot; do |v| v.name = &quot;slave1&quot; v.cpus = 1 v.memory = 2500 end slave1.vm.provision &quot;shell&quot;, path: &quot;bootstrap-slave.sh&quot; ##設定VM第一次被啟動時,所需執行的Shell,且使用root帳號執行 end #define data2 server config.vm.define &quot;slave2&quot; do |slave2| slave2.vm.hostname = &quot;hadoop-slave2&quot; ##設定VM主機名稱 slave2.vm.network &quot;private_network&quot;, ip: DN2_IP ##設定主機 ip slave2.vm.box = &quot;ubuntu/yakkety64&quot; ##設定VM使用的OS Box slave2.vm.synced_folder &quot;.&quot;, &quot;/home/vagrant/src&quot;, mount_options: [&quot;dmode=775,fmode=664&quot;] ##設定公用分享目錄 slave2.vm.provider &quot;virtualbox&quot; do |v| v.name = &quot;slave2&quot; ##設定VM名稱 v.cpus = 1 ##設定VM所使用的CPU core數量 v.memory = 2500 ##設定VM記憶體大小 end slave2.vm.provision &quot;shell&quot;, path: &quot;bootstrap-slave.sh&quot; ##設定VM第一次被啟動時,所需執行的Shell,且使用root帳號執行 end #define Master server config.vm.define &quot;master&quot; do |master| master.vm.hostname = &quot;hadoop-master&quot; ##設定VM主機名稱 master.vm.network &quot;private_network&quot;, ip: MASTER_IP ##設定主機 ip master.vm.box = &quot;ubuntu/yakkety64&quot; ##設定VM使用的OS Box master.vm.synced_folder &quot;.&quot;, &quot;/home/vagrant/src&quot;, mount_options: [&quot;dmode=775,fmode=664&quot;] ##設定公用分享目錄 master.vm.provider &quot;virtualbox&quot; do |v| v.name = &quot;master&quot; ##設定VM名稱 v.cpus = 1 ##設定VM所使用的CPU core數量 v.memory = 3500 ##設定VM記憶體大小 end master.vm.provision &quot;shell&quot;, path: &quot;bootstrap-master.sh&quot; ##設定VM第一次被啟動時,所需執行的Shell,且使用root帳號執行 #master.vm.provision &quot;shell&quot;, path: &quot;bootstrap-complete.sh&quot;, run: &quot;always&quot; ##每次啟動VM時,都必須執行的Shell,且使用root帳號執行 #master.vm.provision &quot;shell&quot;, path: &quot;bootstrap-complete.sh&quot;, run: &quot;always&quot;,privileged: false ##設定VM第一次被啟動時,所需執行的Shell,且使非root帳號執行 end end","categories":[{"name":"vagrant","slug":"vagrant","permalink":"http://yoursite.com/categories/vagrant/"}],"tags":[]},{"title":"Vagrant Command Line說明筆記","slug":"[vagrant]VagrantMemo","date":"2017-02-19T16:00:00.000Z","updated":"2017-07-18T05:24:19.802Z","comments":true,"path":"2017/02/20/[vagrant]VagrantMemo/","link":"","permalink":"http://yoursite.com/2017/02/20/[vagrant]VagrantMemo/","excerpt":"","text":"vagrant package 1vagrant package --base &lt;vm_name&gt; --output &lt;box_name&gt;.box vagrant box 1234vagrant box add ubuntu/precise64 ##https://atlas.hashicorp.com/vagrant box add precise64 http://files.vagrantup.com/precise64.boxvagrant box listvagrant box remove precise64 vagrant init 1vagrant init precise64 ##會在執行語法的目錄下產生一個Vagrantfile的配置檔 vagrant 啟動/停止/重啟/移除 全部VM 1234567891011121314151617##以下為啟動/停止/重啟/移除全部VM的用法vagrant up ##啟動全部VMvagrant halt ##停止全部VMvagrant reload ##重啟全部VMvagrant destroy ##移除全部VM##以下為啟動/停止/重啟/移除單一VM的用法vagrant up mastervagrant halt mastervagrant reload mastervagrant destroy master##以下為啟動/停止/重啟/移除多個VM的用法(VM名稱中間空格,以做為多個VM名稱區隔)vagrant up master slave1vagrant halt master slave1vagrant reload master slave1vagrant destroy master slave1 other command 1234vagrant versionvagrant statusvagrant ssh-config &lt;vm_name&gt;vagrant ssh","categories":[{"name":"vagrant","slug":"vagrant","permalink":"http://yoursite.com/categories/vagrant/"}],"tags":[]},{"title":"Hadoop常用命令說明","slug":"[hadoop]Hadoop_CMD","date":"2017-01-09T16:00:00.000Z","updated":"2017-01-10T10:14:07.994Z","comments":true,"path":"2017/01/10/[hadoop]Hadoop_CMD/","link":"","permalink":"http://yoursite.com/2017/01/10/[hadoop]Hadoop_CMD/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647hadoop fs -mkdir /user/hadoop/data hadoop fs -mkdir -p /user/hadoop/data ##連Partants Directories建立hadoop fs -ls -h[-d] /user/hadoophadoop fs -lsr /user/hadoophadoop fs -put /opt/staff.csv /user/hadoop/data ##上傳檔案至HDFS,不會overwritehadoop fs -put -f /opt/staff.csv /user/hadoop/data ##上傳檔案至HDFS,overwrite當前檔案hadoop fs -df -h /user/hadoophadoop fs -du -h /user/hadoophadoop fs -dus /user/hadoophadoop fs -Ddfs.replication=4 -put /opt/staff.csv /user/hadoop/datahadoop fs -setrep 10 /user/hadoop/data/staff.csvhadoop fs -mkdir -p /user/hadoop/ccchadoop fs -touchz /user/hadoop/ccc/test.xml ##建立一個空檔案hadoop fs -cat /user/hadoop/ccc/test.xmlhadoop fs -rm /user/hadoop/ccc/test.xml ##刪除HDFS上的檔案hadoop fs -rmdir /user/hadoop/ccc ##刪除HDFS上的空目錄hadoop fs -rmr /user/hadoop/ccc ##刪除指定目錄下所有的檔案及目錄hadoop fs -copyFromLocal /opt/staff.csv /user/hadoop/ccc ##檔案上傳到HDFShadoop fs -copyToLocal /user/hadoop/ccc/staff.csv /opt/staff_download.csv ##檔案下載到/opthadoop fs -moveFromLocalhadoop fs -moveToLocalhadoop fsck /user/hadoop/data/staff.csv -files -blocks -locationshadoop fsck -blockId blk_&lt;id&gt;hadoop dfsadmin -safemode enter ##進入安全模式hadoop dfsadmin -safemode leave ##離開安全模式hadoop dfsadmin -safemode get ##取得目前狀態hadoop dfsadmin -safemode wait hadoop dfsadmin -reporthadoop dfsadmin -refreshNodeshadoop dfsadmin -printTopologyhadoop namenode -format ##格式化namenode,在HDFS系統上的資料會全部刪除","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/categories/hadoop/"}],"tags":[]},{"title":"Hadoop資料儲存於多個目錄中的模擬測試","slug":"[hadoop]hadoopMultiDirectoryStore","date":"2017-01-09T16:00:00.000Z","updated":"2017-01-10T09:59:13.871Z","comments":true,"path":"2017/01/10/[hadoop]hadoopMultiDirectoryStore/","link":"","permalink":"http://yoursite.com/2017/01/10/[hadoop]hadoopMultiDirectoryStore/","excerpt":"","text":"環境說明 12namenode *1(for Linux OS)datanode *4(for Linux OS) 相關設定說明 1234567891011mkdir -p /bgdt/data/data2##修改hdfs-site.xml&lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/bgdt/hadoop-2.7.2/tmp/dfs/data,/bgdt/data/data2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.failed.volumes.tolerated&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 進入劇本之前 123456##重啟dfs與yarnstart-dfs.shstart-yarn.sh##並下達以下指令:hadoop balancer 劇本1 系統Run了一段時間後,發現空間不夠了,想增加一個空間給目前的Hadoop cluster環境,因此有了以下的劇本與想法1.用新加一個目錄,來取代模擬新分割區2.copy 一個檔案至HDFS系統後,透過介面查詢相關block id,找出相對實體檔案的儲存位置 12345678910111213hadoop fs -mkdir -P /user/hadoop/datahadoop fs -put /opt/staff.csv /user/hadoop/data##打開Web UI觀察 /user/hadoop/data/staff.csv block Id=XXXXXX在檔案下載畫面中會有檔案相關資訊block Id: Avaailabledna1dna3##確認block Id,與存放的Datanode位置後,直接查找存放的實體位置ssh dna1 &quot;find /bgdt -name &quot;blk_&lt;ID&gt;&quot;&quot;/bgdt/data/data2/current/BP-1008347755-192.168.11.96-1474254342729/current/finalized/subdir0/subdir0/blk_XXXXXX 劇本2 將某個Datanode的某個目錄權限及owner設定成(chown root:root chmod 000),模擬硬碟損壞的狀況 1.測試當某個正在運行的hadoop cluster節點的儲存目錄被設為無讀寫權限時,系統是否會Crash2.當目錄修復完成後資料是否有辦法從其他的node,reconvert回來3.當該某個正在運行的節點,所有目錄都無法讀取時,系統是否會正常運行1234567891011121314151617181920212223242526272829303132333435##將測試檔案存放到HDFS,並設定副本數為4hadoop fs -Ddfs.replication=4 -put /opt/staff_1.csv /user/hadoop/data##查詢block Id 及block儲存位置ssh dna1 &quot;find /bgdt -name &quot;blk_&lt;ID&gt;&quot;&quot;##模擬故障sudo chown -R root:root /bgdt/hadoop-2.7.2/tmpsudo chmod -R 000 /bgdt/hadoop-2.7.2/tmphadoop balancer##Web UI上Datanode Volume Failures會出現有問題的目錄##balencer執行完成後,會將原本在/tmp目錄下的資料搬移到 /bgdt/data/data2##此時若再把另外一個目錄也模擬成無權限狀態sudo chown -R root:root /bgdt/datasudo chmod -R 000 /bgdt/data##該node的Status會變成Dead,並且DataNode的process會停止##此時資料的Repl雖然一樣是4,但實際上的資料份數是3##接著將/bgdt/data目錄設回原本權限,並將資料刪除sudo chown -R hadoop:hadoop /bgdt/datasudo chmod -R 755 /bgdt/datarm -rf /bgdt/data##並將/bgdt/data目錄的資料刪除後,重啟datanodehadoop-daemon.sh start datanode##重啟成功後,進行balancerhadoop balancer##看到所有檔案的副本數與與資料又恢復正常","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/categories/hadoop/"}],"tags":[]},{"title":"Hadoop Cluster-動態新增一個節點","slug":"[hadoop]CreatNewDataNode","date":"2017-01-09T16:00:00.000Z","updated":"2017-01-10T10:08:35.570Z","comments":true,"path":"2017/01/10/[hadoop]CreatNewDataNode/","link":"","permalink":"http://yoursite.com/2017/01/10/[hadoop]CreatNewDataNode/","excerpt":"","text":"hadoop cluster Run了一段時間後,想要增加運算速度,因此想動態新增一個DataNode1.動態新增一個DataNode 123456789101112131415161718##新主機請安裝以下1. 安裝Linux OS2. 在/etc/hosts加入所有Hadoop cluster節點的hostname3. 進入NameNode下執行 ssh-copy-id hadoop@hadoop-slave5 scp hadoop-bin-2.7.2.tar.gz hadoop@hadoop-slave5:/bgdt/install_src4. 進入新增的DataNode5. tar -zxvf jdk-8u101-linux-x64.tar.gz -C /bgdt/java6. tar -zxvf hadoop-bin-2.7.2.tar.gz -C /bgdt7. Modify ~/.bashrc8. scp 相關設定檔 scp /bgdt/hadoop-2.7.2/etc/hadoop/hdfs-site.xml hadoop@hadoop-slave5:/bgdt/hadoop-2.7.2/etc/hadoop/hdfs-site.xml scp /bgdt/hadoop-2.7.2/etc/hadoop/core-site.xml hadoop@hadoop-slave5:/bgdt/hadoop-2.7.2/etc/hadoop/core-site.xml scp /bgdt/hadoop-2.7.2/etc/hadoop/mapred-site.xml hadoop@hadoop-slave5:/bgdt/hadoop-2.7.2/etc/hadoop/mapred-site.xml scp /bgdt/hadoop-2.7.2/etc/hadoop/yarn-site.xml hadoop@hadoop-slave5:/bgdt/hadoop-2.7.2/etc/hadoop/yarn-site.xml scp /bgdt/hadoop-2.7.2/etc/hadoop/slaves hadoop@hadoop-slave5:/bgdt/hadoop-2.7.2/etc/hadoop/slaves9. hadoop-daemon.sh start datanode10. 觀察Web UI是否出現slave5 DataNode","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/categories/hadoop/"}],"tags":[]},{"title":"Hadoop Cluster相關環境建置筆記(持續修正中......)","slug":"[Install]BigData","date":"2017-01-08T10:00:00.000Z","updated":"2017-06-02T07:23:28.134Z","comments":true,"path":"2017/01/08/[Install]BigData/","link":"","permalink":"http://yoursite.com/2017/01/08/[Install]BigData/","excerpt":"","text":"安裝前準備停止iptables服務 1234## 先使用root登入Linux OSservice iptables stop #停止iptables服務vi /etc/rc.localservice iptables stop #停止iptables服務,存檔後離開 新增hadoop User,並賦予hadoop root的權限 123456adduser hadooppasswd hadoopvisudohadoop ALL=(ALL) ALL #加入這一行,存檔後離開(以上步驟每台電腦都要做) 進行SSH相關的設定,透過ssh做無密碼連線測試 123456789su - hadoopssh-keygenssh-copy-id hadoop@mna1 ##別忘記自己也要copyssh-copy-id hadoop@dna1 ##有幾台datanode就要做幾次ssh-copy-id hadoop@dna2 ##有幾台datanode就要做幾次##ssh連線測試ssh mna1ssh dna1 系統時間同步 1sudo date [MMddhhmm.SS];hwclocl -w 建立安裝目錄 123sudo mkdir -p /bgdt/install_srcsudo mkdir -p /bgdt/javasudo chown -R hadoop:hadoop /bgdt 將相關的安裝檔案上傳到 hadoop-master /bget/install_src目錄下 12345678910使用WinSCP上傳比較方便jdk-8u101-linux-x64.tar.gzhadoop-2.7.2.tar.gzspark-2.0.0-bin-hadoop2.7.tgzapache-hive-2.1.0-bin.tar.gzapache-hive-2.1.0-src.tar.gzAnaconda3-4.2.0-Linux-x86_64.shoozie-4.3.0-distro.tar.gzmysql-connector-java-5.1.39-bin.jarext-2.2.zip 安裝JDK 123456789101112step1. [CMD]tar -zxvf /bgdt/jdk-8u101-linux-x64.tar.gz -C /usr/localstep2. [CMD]vi ~/.bashrc加入以下兩行export JAVA_HOME=/usr/local/jdk1.8.0_101export PATH=$JAVA_HOME/bin:$PATHstpe3. [CMD]source ~/.bashrcstep4. [CMD]java -version(以上步驟在每台Datanode上也需要安裝喔!!) Hadoop Cluster安裝(2.7.2),每台電腦皆要安裝解壓縮並設定相關環境變數 123456789101112step1. [CMD]tar -zxvf /bgdt/hadoop-2.7.2.tar.gz -C /bgdtstep2. [CMD]vi ~/.bashrc加入以下變數export HADOOP_HOME=/bgdt/hadoop-2.7.2export HADOOP_CONF_DIR=/bgdt/hadoop-2.7.2/etc/hadoopexport CLASSPATH=$CLASSPATH:$HADOOP_HOME/lib/*:.export CLASSPATH=$CLASSPATH:$HADOOP_HOME/share/hadoop/common/*:.export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATHstep3. [CMD]source ~/.bashrcstep4. [CMD]java -version 修改設定檔修改設定檔(/bgdt/hadoop-2.7.2/etc/hadoop/core-site.xml) 123456789101112131415161718&lt;configuration&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mna1:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/bgdt/hadoop-2.7.2/tmp&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改設定檔(/bgdt/hadoop-2.7.2/etc/hadoop/hdfs-site.xml) 12345678910111213141516171819202122&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;mna1:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/bgdt/hadoop-2.7.2/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/bgdt/hadoop-2.7.2/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.block.size&lt;/name&gt; &lt;value&gt;64M&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改設定檔(/bgdt/hadoop-2.7.2/etc/hadoop/mapred-site.xml) 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;mna1:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;mna1:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改設定檔(/bgdt/hadoop-2.7.2/etc/hadoop/yarn-site.xml) 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;mna1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://mna1:19888/jobhistory/logs&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 修改設定檔(/bgdt/hadoop-2.7.2/etc/hadoop/slaves) 1234dna1dna2dna3dna4 修改設定檔(/bgdt/hadoop-2.7.2/etc/hadoop/capacity-scheduler.xml) 1234567僅修改以下屬性值(0.1--&gt;0.5).......&lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;/name&gt; &lt;value&gt;0.5&lt;/value&gt; &lt;/property&gt;...... 將檔案分送到其他Datanode 12345678910##將目錄壓縮tar -zcf /bgdt/hadoop.master.tar.gz /bgdt/hadoop-2.7.2##將壓縮後的檔案,透過scp指令傳送到其他主機(Datanode)scp /bgdt/hadoop.master.tar.gz dna1:/bgdt/install_srcscp /bgdt/hadoop.master.tar.gz dna2:/bgdt/install_srcscp /bgdt/hadoop.master.tar.gz dna3:/bgdt/install_srcscp /bgdt/hadoop.master.tar.gz dna4:/bgdt/install_src##各個Datanode將Hadoop解壓縮至/bgdt,解完後設定相關Haoop變數如第一大項 格式化HDFS系統 12hadoop namenode -formathadoop dfsadmin -report 啟動dfs與yarn 1234567##啟動服務start-dfs.shstart-yarn.sh##關閉服務stop-yarn.shstop-dfs.sh 單台Datanode啟動程式 123ssh dna1/bgdt/hadoop-2.7.2/sbin/hadoop-daemon.sh start datanode/bgdt/hadoop-2.7.2/sbin/yarn-daemon.sh start nodemanager HDFS WebUI (hadoop-master:50070) yarn WebUI (hadoop-master:8088) Spark安裝(2.0.0),僅安裝於NameNodeDownload spark 2.0.0安裝程式 下載後會產生一個 “spark-2.0.0-bin-hadoop2.7.tgz”檔案 或使用wget下載,下載前請先確定是否可以連網路囉!! 12wget -P \\/bgdt/install_src http://archive.apache.org/dist/spark/spark-2.0.0/spark-2.0.0-bin-hadoop2.7.tgz 解壓縮下載檔案 12tar -zxvf /bgdt/install_src/spark-2.0.0-bin-hadoop2.7.tgz -C /bgdtmv /bgdt/spark-2.0.0-bin-hadoop2.7 /bgdt/spark-2.0.0 ##目錄名稱太長了,所以把目錄名稱改短 修改~/.bashrc 1234export SPARK_HOME=/bgdt/spark-2.0.0將檔案儲存後,記得下source,讓設定生效source ~/.bashrc 測試(spark-shell)進入spark-shell CLI 12345##進入spark-shell/bgdt/spark-2.0.0/bin/sparl-shell............scala&gt; Spark Scala Shell Test 123[CTRL+l] &lt;--清除shell畫面scala&gt;scscala&gt;spark Spark Example for Scala-建立檔案內容並上傳至HDFS 12345678hadoop fs -mkdir -p /user/hadoop/cccecho &quot;1,\\&quot;test1\\&quot;,10000&quot; &gt;&gt; test1.csvecho &quot;2,\\&quot;test2\\&quot;,20000&quot; &gt;&gt; test1.csvecho &quot;3,\\&quot;test3\\&quot;,30000&quot; &gt;&gt; test1.csvecho &quot;4,\\&quot;test4\\&quot;,40000&quot; &gt;&gt; test1.csvecho &quot;5,\\&quot;test5\\&quot;,50000&quot; &gt;&gt; test1.csvcat test1.csvhadoop fs -put ~/test1.csv /user/hadoop/ccc Spark Example for Scala-spark for scala程式撰寫 123scala&gt;val distFile = sc.textFile(&quot;/user/hadoop/ccc/test1.csv&quot;)scala&gt;distFile.count()scala&gt;distFile.collect() 範例執行結果 測試(spark-submit)啟動Spark History Server(Port:18080) 12hadoop fs -mkdir -p /tmp/history$SPARK_HOME/sbin/start-history-server.sh hdfs://hadoop-master:8020/tmp/history 啟動Job History Server(Port:19888) 1mr-jobhistory-daemon.sh start historyserver MariaDB安裝(10.1.17),僅安裝於NameNode,目前須使用root權限安裝下載MariaDB安裝檔(目前最新版本為10.1.20)Download mariadb-10.1.17-linux-x86_64 關閉iptables service 123先以root權限登入系統並關閉iptables服務service iptables stopservice iptables status 建立mysql帳號 12groupadd mysqluseradd -g mysql mysql 安裝MariaDB 1234567891011121314151617cd /usr/localtar -zxvf /bgdt/install_src/mariadb-10.1.17-linux-x86_64.tar.gz -C /usr/localln -s mariadb-10.1.17-linux-x86_64 mysqlcd mysql./scripts/mysql_install_db --user=mysqlchown -R root .chown -R mysql datacd /usr/local/mysql./bin/mysqld --user=root &amp;cat &gt; /etc/rc.local &lt;&lt;EOF#!/bin/sh -ecd $&#123;mariadb_install_dist&#125;/mysql./bin/mysqld --user=root &amp;exit 0EOF&#125; 啟動MariaDB(修正中…) 1234nohup ./bin/mysqld --user=root &amp;./bin/mysqladmin -u root password &apos;!QAZxsw2&apos; --socket=/var/lib/mysql/mysql.sock ./bin/mysql -p --socket=/var/lib/mysql/mysql.sock輸入密碼:!QAZxsw2 設定權限 1234mariadb&gt;create database metastore_db;mariadb&gt;SELECT User, Host FROM mysql.user WHERE Host &lt;&gt; &apos;localhost&apos;; mariadb&gt;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;!QAZxsw2&apos; WITH GRANT OPTION;mariadb&gt;FLUSH PRIVILEGES; 確認”3306” Port是否有開啟？並且關閉確認關閉iptables Hive安裝(2.1.1),僅安裝於NameNode安裝/設定/啟動 Hive CLI下載 Hive安裝檔/原始碼Download Hive 安裝檔,apache-hive-2.1.1-binDownload Hive 原始碼,apache-hive-2.1.1-src 解壓縮檔案 12tar -zxvf /bgdt/install_src/apache-hive-2.1.1-bin.tar.gz -C /bgdtmv /bgdt/apache-hive-2.1.1-bin /bgdt/hive-2.1.1 修改~/.bashrc 12345export HIVE_HOME=/bgdt/hive-2.1.1export PATH= $HIVE_HOME\\bin:$PATH儲存檔案source ~/.bashrc 將mysql-connector-java-5.1.39-bin.jar copy to /bgdt/hive-2.1.1/lib目錄下建立Hive Warehouse 12hadoop fs -mkdir -p /user/hive/warehousehadoop fs -mkdir -p /tmp/hive Modify hive-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.XXX.XXX:3306/metastore_db&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;XXXXXXXXX&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt; &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: $&#123;hive.exec.scratchdir&#125;/&amp;lt;username&amp;gt; is created, with $&#123;hive.scratch.dir.permission&#125;.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/tmp&lt;/value&gt; &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/tmp&lt;/value&gt; &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.scratch.dir.permission&lt;/name&gt; &lt;value&gt;775&lt;/value&gt; &lt;description&gt;The permission for the user specific scratch directories that get created.&lt;/description&gt;&lt;/property&gt; create metastore_db 相關Table 12##此動作會在mariadb的metastore_db中建立Hive用到的系統表格(以下動作會將meta資料重建)schematool -dbType mysql -initSchema 在命令列中執行Hive CLI 1234hive............hive&gt; 在hive的CLI下建立Database及Table 12345hive&gt;create database test1;hive&gt;create table test1.staff(id int, name string, salary double) hive&gt;row format delimited fields terminated by &apos;,&apos;;hive&gt;LOAD DATA INPATH &apos;/user/hadoop/ccc/test1.csv&apos; overwrite into table test1.staff;hive&gt;select * from test1.staff; 以上命令執行結果 HWI安裝與設定(選裝)將已下載的Hive Souce code解壓縮至/bgdt 1234tar -zxvf /bgdt/install_src/apache-hive-2.1.0-src.tar.gz -C /bgdtcd /bgdt/apache-hive-2.1.0-src/hwijar cfM hive-hwi-2.1.0.war -C web .cp hive-hwi-2.1.0.war /bgdt/hive-2.1.0/lib/* 修改hive-site.xml 123456789101112131415&lt;property&gt; &lt;name&gt;hive.hwi.listen.host&lt;/name&gt; &lt;value&gt;hadoop-master&lt;/value&gt; &lt;description&gt;This is the host address the Hive Web Interface will listen on&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.hwi.listen.port&lt;/name&gt; &lt;value&gt;9999&lt;/value&gt; &lt;description&gt;This is the port the Hive Web Interface will listen on&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.hwi.war.file&lt;/name&gt; &lt;value&gt;lib/hive-hwi-2.1.0.war&lt;/value&gt; &lt;description&gt;This sets the path to the HWI war file, relative to $&#123;HIVE_HOME&#125;. &lt;/description&gt;&lt;/property&gt; Start HWI Service 123##HWI啟動Port(9999)nohup hive --service hwi &amp;netstat -tnl | grep &quot;9999&quot; HWI介面(請連到http://hadoop-master:9999/hwi) Hive for BeelineStart hiveserver2123##啟動Hive JDBC Port(port:10000)nohup hive --service hiveserver2 &amp;netstat -tnl | grep &quot;10000&quot; 使用beeline連線Hive(須連線HiveServer2)123beelinebeeline&gt; !connect jdbc:hive2://192.168.11.96:10000/default OOZIE安裝(4.3.0),僅安裝於NameNodeOOZIE安裝與設定從Github下載oozie原始碼,編譯oozie,並產生”oozie-4.3.0.tar.gz”檔案解壓縮oozie-4.3.0.tar.gz並變更目錄名稱 12sudo tar -zxvf /bgdt/install_src/oozie-4.3.0-distro.tar.gz -C /bgdtsudo chown -R hadoop:hadoop /bgdt/oozie-4.3.0 搬移相關lib至libext目錄 12345mkdir -p /bgdt/oozie-4.3.0/libextcp /bgdt/hadoop-2.7.2/share/hadoop/*/*.jar /bgdt/oozie-4.3.0/libext/cp /bgdt/hadoop-2.7.2/share/hadoop/*/lib/*.jar /bgdt/oozie-4.3.0/libext/rm -rf /bgdt/oozie-4.3.0/libext/jsp-api-2.1.jarcp /bgdt/install_src/ext-2.2.zip /bgdt/oozie-4.3.0/libext 修改oozie-site.xml 123456789101112131415161718192021cat &gt; /bgdt/oozie-4.3.0/conf/oozie-site.xml &lt;&lt;EOF&lt;?xml version=&quot;1.0&quot;?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;oozie.service.WorkflowAppService.system.libpath&lt;/name&gt; &lt;value&gt;/user/hadoop/share/lib&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;oozie.service.HadoopAccessorService.hadoop.configurations&lt;/name&gt; &lt;value&gt;*=/bgdt/hadoop-2.7.2/etc/hadoop&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;oozie.use.system.libpath&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;oozie.service.SparkConfigurationService.spark.configurations&lt;/name&gt; &lt;value&gt;*=/bgdt/spark-2.0.0/conf&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;EOF oozie4.3.0版需要再額外處理oozie.war檔中有hadoop-2.6相關jar檔衝突問題(在執行oozie時,會產生noSuchFile HAOOP_CLASSPATH的錯誤訊息) 12345#cd $&#123;install_dist&#125;/oozie-$&#123;OOZIE_VERSION&#125;/oozie#/bgdt/java/jdk1.8.0_101/bin/jar -xvf ../oozie.war#rm -rf ./WEB-INF/lib/hadoop-*-2.6.0.jar#/bgdt/java/jdk1.8.0_101/bin/jar cvf ../oozie.war *.* .#rm -rf $&#123;install_dist&#125;/oozie-$&#123;OOZIE_VERSION&#125;/oozie 安裝相關Server,createdb 12345cd /bgdt/oozie-4.3.0./bin/oozie-setup.sh prepare-war./bin/oozie-setup.sh db create -run./bin/oozied.sh start./bin/oozie admin -oozie http://localhost:11000/oozie -status OOZIE WEB UI: 1http://hadoop-master:11000/oozie 安裝sharelib複製相關jar檔 123456789cd /bgdt/oozie-4.3.0tar -zxvf oozie-sharelib-4.3.0-SNAPSHOT.tar.gzrm -rf /bgdt/oozie-4.3.0/share/lib/sparkmkdir -p /bgdt/oozie-4.3.0/share/lib/sparkcp /bgdt/spark/jars/* /bgdt/oozie-4.3.0/share/lib/sparkcp oozie-sharelib-oozie-4.3.0-SNAPSHOT.jar /bgdt/oozie-4.3.0/share/lib/sparkcp oozie-sharelib-spark-4.3.0-SNAPSHOT.jar /bgdt/oozie-4.3.0/share/lib/sparkcp /bgdt/hive-2.1.1/conf/hive-site.xml /bgdt/oozie-4.3.0/share/lib/sparkcp /bgdt/install_src/mysql-connector-java-5.1.39-bin.jar /bgdt/oozie-4.3.0/share/lib/spark 在HDFS上建立sharelib相關目錄 12hadoop fs -mkdir -p /user/hadoop/share/libhadoop fs -put /bgdt/oozie-4.3.0/share/lib/* /user/hadoop/share/lib 重啟oozie 123cd /bgdt/oozie-4.3.0./bin/oozied.sh stop./bin/oozied.sh start 執行oozie提供相關的測試範例:上傳oozie examples相關設定到 hdfs 123tar -zxvf oozie-examples.tar.gzhadoop fs -mkdir -p /user/hadoop/oozie/exampleshadoop fs -put -f /bgdt/oozie-4.3.0/examples/* /user/hadoop/oozie/examples 修改job.properties 1234567891011cd /bgdt/oozie-4.3.0/examples/apps/sparkvi ./job.properties##修改以下內容nameNode=hdfs://hadoop-master:8020jobTracker=hadoop-master:8032master=local[*]queueName=defaultexamplesRoot=oozie/examplesoozie.use.system.libpath=trueoozie.wf.application.path=/user/hadoop/oozie/examples/apps/spark 執行oozie example 123456cd /bgdt/oozie-4.3.0./bin/oozie job -oozie http://localhost:11000/oozie \\-config ./examples/apps/spark/job.properties \\-run 到WebUI觀察執行情形 1http://hadoop-master:8088 Anaconda3安裝(4.2.0),選裝,僅安裝於NameNodeDownload Anaconda3-4.2.0-Linux-x86.shhttps://repo.continuum.io/archive/https://www.continuum.io/downloads Upload Anaconda3-4.2.0-Linux-x86.sh to HostInstall bzip2(if not bzip2) 1sudo yum install bzip2 -y Install Anaconda3 12345bash /opt/Anaconda3-4.2.0-Linux-x86.sh[Enter]yes/opt/anaconda3yes Modify ~/.bashrc 12# added by Anaconda3 4.2.0 installerexport PATH=&quot;/opt/anaconda3/bin:$PATH&quot; Modify spark-env.sh 12345vi /usr/local/spark/conf/spark/spark-env.shexport PYSPARK_PYTHON=/opt/anaconda3/bin/python3export PYSPARK_DRIVER_PYTHON=jupyterexport PYSPARK_DRIVER_PYTHON_OPTS=&quot;notebook --NotebookApp.open_browser=False --NotebookApp.ip=&apos;*&apos; --NotebookApp.port=8880&quot; 重新進入Putty後,啟動Hadoop相關服務 123start-dfs.sh ##start Hadoopstart-yarn.sh ##start yarnnohup /usr/local/spark/bin/pyspark &amp; 啟動完成後,打開Web Browser即可看到Jupyter Web UI 以下編寫測試範例進入ipython Web UI 透過SparkContext取得文字檔 12rdd = sc.textFile(&quot;hdfs://XXX.XXX.XXX.XXX:8020/OOO/readme.txt&quot;)rdd.count() 操作DataFrame範例 123l = [(&apos;Alice&apos;, 1)]spark.createDataFrame(l).collect()spark.createDataFrame(l, [&apos;name&apos;, &apos;age&apos;]).collect() 使用Spark SQL取得資料 12ds = spark.sql(&quot;select * from students&quot;)ds.take(5) 相關參考參考文件(Link)Spark on Ubuntu 離線安裝(第一次就上手)Spark Standalone Cluster 練習 Big Data use default port Hadoop use port 1234568020 &lt;-- HDFS Portocol port(NameNode metadata service)(fs.defaultFS)50070 &lt;-- HDFS Web Browser HTTP Port,NameNode WebUI(dfs.http.address)(Web)50075 &lt;-- DataNode WebUI to access the status, logs etc(dfs.datanode.http.address)(Web)50090 &lt;-- Secondary NameNode Port(dfs.secondary.http.address)50020 &lt;-- The datanode ipc server address and port.(dfs.datanode.ipc.address)50010 &lt;-- The datanode server address and port for data transfer.(dfs.datanode.address) Yarn cluster use port 12345678088 &lt;-- Yarn-Cluster Web UI Port(yarn.resourcemanager.webapp.address)(web)8030 &lt;-- yarn.resourcemanager.scheduler.address8031 &lt;-- yarn.resourcemanager.resource-tracker.address8032 &lt;-- Yarn resourcemnager port(yarn.resourcemanager.address)(JobTracker port for Hadoop2.X)8033 &lt;-- yarn.resourcemanager.admin.address8040 &lt;-- Address where the localizer IPC is.(yarn.nodemanager.localizer.address)8042 &lt;-- NM Webapp address.(yarn.nodemanager.webapp.address) Hive use port 1210000 &lt;-- Hive Server2 Port(Hive JDBC Use)(ENV Variable HIVE_PORT)10002 &lt;-- Hive Server2 Web UI Oozie use port 1211000 &lt;-- OOZIE Web UI Port(OOZIE_HTTP_PORT in oozie_env.sh)(Web)11001 &lt;-- The admin port Oozie server runs(OOZIE_ADMIN_PORT in oozie_env.sh)(OOZIE admin Port) Spark use port 124040 &lt;-- Spark Web UI(spark)18080 &lt;-- Spark History port(Web) MR-Job History use port 12319888 &lt;-- MR Job History port(Web)10020 &lt;-- MapReduce JobHistory server address(mapreduce.jobhistory.address)10033 &lt;-- The address of the History server admin interface.(mapreduce.jobhistory.admin.address) Hadoop Cluster相關環境HEAP SIZE設定hadoop-env.sh 12export HADOOP_HEAPSIZE=1024 ##hadoop namenode&amp;datanode heap size(default 1000)export HADOOP_JOB_HISTORYSERVER_HEAPSIZE=1024 ##yarn job history heap size(default 1000) yarn-env.sh 12export YARN_RESOURCEMANAGER_HEAPSIZE=1024 ##Resource Manager heap size(default 1000)export YARN_NODEMANAGER_HEAPSIZE=512 ##Node Manager heap size(default 1000) Hadoop Cluster相關環境啟動與停止程序整理 1234567891011121314151617Hadoop Cluster相關服務啟動順序------------------------------start-dfs.shstart-yarn.shmr-jobhistory-daemon.sh start historyserver/bgdt/spark-2.0.0/sbin/start-history-server.shnohup hive --service hiveserver2 &amp; (使用&quot;netstat -tnl | grep &apos;10000&apos;&quot;驗證10000 port是否有啟動)/bgdt/oozie-4.3.0/bin/oozied.sh startHadoop Cluster相關服務停止順序------------------------------/bgdt/oozie-4.3.0/bin/oozied.sh stopmr-jobhistory-daemon.sh stop historyserver/bgdt/spark-2.0.0/sbin/stop-history-server.shstop-yarn.shstop-dfs.shhiveserver2需要用&quot;kill -9 PID&quot;方式停止,目前沒有停止指令 Hadoop Cluster相關環境啟動程序名稱","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/categories/hadoop/"}],"tags":[]},{"title":"常用Linux 命令筆記(for Oracle Linux)","slug":"[Linux]Linux","date":"2016-12-07T14:05:00.000Z","updated":"2017-03-10T02:41:02.821Z","comments":true,"path":"2016/12/07/[Linux]Linux/","link":"","permalink":"http://yoursite.com/2016/12/07/[Linux]Linux/","excerpt":"","text":"常用指令新增使用者及密碼修改 12345678[root@hadoop-master]$useradd hadoop[root@hadoop-master]$passwd hadoop[root@hadoop-master]$visudo #如果須要讓 &quot;hadoop&quot;使用者暫時具有管理者(root)權限的話,須使用此指令編輯檔案......root ALL=(ALL) ALL ##在此行後面加入......hadoop ALL=(ALL) ALL...... 系統關機或重啟命令(非”Root”使用者,須加入sudo) 1234567[hadoop@hadoop-master ~]$sudo shutdown -h now ##系統立刻關機 [hadoop@hadoop-master ~]$sudo shutdown -r now ##系統立刻重新開機 [hadoop@hadoop-master ~]$sudo shutdown -h 20:30 ##系統在今天的 20:30 分關機 [hadoop@hadoop-master ~]$sudo shutdown -h +10 ##系統在 10 分鐘後關機[hadoop@hadoop-master ~]$sudo sync;sync;sync;reboot ##重新開機指令,配合寫入緩衝資料的sync指令動作[hadoop@hadoop-master ~]$sudo init 0 ##關機[hadoop@hadoop-master ~]$sudo init 6 ##重新啟動","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[]}]}