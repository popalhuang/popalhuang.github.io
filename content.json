{"meta":{"title":"Popal's Blog","subtitle":"邊做邊學","description":"技術之路學習歷程","author":"popal","url":"http://yoursite.com"},"pages":[{"title":"about","date":"2018-01-02T06:33:56.000Z","updated":"2018-01-02T06:33:56.489Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""}],"posts":[{"title":"Solace Install for docker","slug":"[solace]SolaceforDocker_Install","date":"2020-07-21T16:00:00.000Z","updated":"2020-07-22T07:21:06.921Z","comments":true,"path":"2020/07/22/[solace]SolaceforDocker_Install/","link":"","permalink":"http://yoursite.com/2020/07/22/[solace]SolaceforDocker_Install/","excerpt":"","text":"Solace for Docker 環境安裝File list &amp; Work preparationVM Create &amp; Package InstallOS Install(for CentOS8.2)OS Network SettingOS start &amp; Package Installcifs package installDocker Install&amp;SettingDocker Package InstallDocker Enable/StartDocker macvlan Network SettingVM ExportGet a PubSub+ Software Event Broker Imagestart DockerPull the imageCreate the PubSub+ Software Event Broker ContainerManage the PubSub+ Software Event BrokerSolace PubSub+ Manager management accessSolace CLI management access使用 Solace CLI 進行基本設定操作System User Createcli userfile-transferClient User CreateConnect Max. Setting使用 Try Me! 進行Publish/Subscribe功能測試","categories":[{"name":"Solace","slug":"Solace","permalink":"http://yoursite.com/categories/Solace/"},{"name":"Solace Install","slug":"Solace/Solace-Install","permalink":"http://yoursite.com/categories/Solace/Solace-Install/"}],"tags":[]},{"title":"Solace Install for VitualBox","slug":"[Solace]SolaceforVirtualbox","date":"2020-07-21T16:00:00.000Z","updated":"2020-07-22T07:10:28.737Z","comments":true,"path":"2020/07/22/[Solace]SolaceforVirtualbox/","link":"","permalink":"http://yoursite.com/2020/07/22/[Solace]SolaceforVirtualbox/","excerpt":"","text":"Get a Software Event BrokerSolace PubSub+ Standard or Solace PubSub+ Enterprise Evaluation EditionSolace PubSub+ EnterpriseImport the Software Event BrokerIn Oracle VM VirtualBox Manager, select File &gt; Import Appliance.select the OVA file downloaded in the previous stepdefault appliance settings are sufficient to begin the importOracle VM VirtualBox Manager will import the event broker OVA.VM VirtualBox Manager,Settingsstart the VMLogin to the event broker,must “sysadmin”Configure the Host NameShut down Solace PubSub+Configure the hostnameRestart the Solace PubSub+Verify that the hostname has been set to the new oneAccess the Solace CLITo enter the Solace CLI from the console in the Linux host environmentcreate an admin user namedTo determine the IP address assigned to the event brokerTo remotely access the Solace CLI for the event broker, you can now ssh to port 2222Review Configuration Defaults","categories":[{"name":"Solace","slug":"Solace","permalink":"http://yoursite.com/categories/Solace/"},{"name":"Solace Install","slug":"Solace/Solace-Install","permalink":"http://yoursite.com/categories/Solace/Solace-Install/"}],"tags":[]},{"title":"Linux - systemctl Command Usage","slug":"[Linux]Linux-systemctl","date":"2020-07-21T16:00:00.000Z","updated":"2020-07-22T08:37:31.655Z","comments":true,"path":"2020/07/22/[Linux]Linux-systemctl/","link":"","permalink":"http://yoursite.com/2020/07/22/[Linux]Linux-systemctl/","excerpt":"","text":"systemctl command Usage透過systemctl管理單一服務(service unit)的啟動/開機啟動與觀察狀態 12345678910111213141516171819systemctl [command] [service]command 主要有：start ：立刻啟動後面接的servicestop ：立刻關閉後面接的servicerestart ：立刻關閉後啟動後面接的service，亦即執行 stop 再 start 的意思reload ：不關閉後面接的service的情況下，重新載入設定檔，讓設定生效enable ：設定下次開機時，後面接的service 會被啟動disable ：設定下次開機時，後面接的service 不會被啟動status ：目前後面接的這個service 的狀態，會列出有沒有正在執行、開機預設執行否、登錄等資訊等！is-active ：目前有沒有正在運作中is-enabled：開機時有沒有預設要啟用這個serviceExample:systemctl enable dockersystemctl start dockersystemctl status dockersystemctl restart dockersystemctl stop dockersystemctl disable docker 透過 systemctl 觀察系統上所有的服務 1234567891011systemctl [command] [--type=TYPE] [--all]command: list-units ：依據 unit 列出目前有啟動的 unit。若加上 --all 才會列出沒啟動的。 list-unit-files ：依據 /usr/lib/systemd/system/ 內的檔案，將所有檔案列表說明。--type=TYPE：unit type(service, socket, target...)Example:Example:systemctl list-units --type=service --allsystemctl list-units --type=service --all | grep cpusystemctl list-unit-files 透過 systemctl 管理不同的操作環境 (target unit) 12345678systemctl get-defaultsystemctl poweroff ##關機systemctl reboot ##重開機systemctl suspend ##暫停systemctl hibernate ##休眠systemctl rescue ##systemctl emergency ## 透過 systemctl 分析各服務之間的相依性 123systemctl list-dependenciessystemctl list-dependencies --reversesystemctl list-dependencies graphical.target","categories":[{"name":"OS","slug":"OS","permalink":"http://yoursite.com/categories/OS/"},{"name":"Linux","slug":"OS/Linux","permalink":"http://yoursite.com/categories/OS/Linux/"}],"tags":[]},{"title":"Solace Cache","slug":"[Solace]SolaceCache","date":"2020-07-21T16:00:00.000Z","updated":"2020-07-22T08:15:50.838Z","comments":true,"path":"2020/07/22/[Solace]SolaceCache/","link":"","permalink":"http://yoursite.com/2020/07/22/[Solace]SolaceCache/","excerpt":"","text":"Installing PubSub+ CacheProduct Key Feature LockingDownloading PubSub+ CacheInstalling PubSub+ Cache Instances Installing PubSub+ Cache InstancesInstalling on a Linux Server with systemdInstalling on a Lunux Server with chkconfigInstalling on a Linux Server without chkconfigUninstalling PubSub+ CacheUninstall a systemd serviceUninstall a chkconfig serviceUninstall from a Linux server without chkconfigEnvironment Variables Environment VariablesAdvanced Environment TuningManaging PubSub+ Cache Managing PubSub+ CacheOperating LimitsSteps to Configure PubSub+ CacheConfiguring Message VPN for CachingConfiguring Distributed CachesConfiguring Heartbeat IntervalsScheduling Message DeletesEnabling Distributed CachesConfiguring Cache ClustersEnabling Deliver-To-One-OverridesConfiguring Event ThresholdsConfiguring Global CachingConfiguring Max Memory for PubSub+ Cache InstancesConfiguring Max Number of Messages Per TopicConfiguring Max Number of TopicsConfiguring Message LifetimesEnabling New Topic AdvertisementsConfiguring Request Queue DepthsAssigning Topics to Cache ClustersEnabling Cache ClustersConfiguring PubSub+ Cache InstancesEnabling/Disabling Auto Start ModeConfiguring Stop On Lost Message BehaviorEnabling/Disabling PubSub+ Cache InstancesCustomizing PubSub+ Cache Instance Configuration FilesChanging PubSub+ Cache Configuration Files That Are In UseSample PubSub+ Cache ConfigurationConfiguration InformationDistributed PubSub+ Cache ConfigurationPerforming PubSub+ Cache Administrative Tasks Performing PubSub+ Cache Administrative TasksClearing PubSub+ Cache EventsDeleting Cached MessagesStarting PubSub+ Cache InstancesBacking Up/Restoring PubSub+ Cache Instance MessagesBacking up Cached MessagesRestoring Cached MessagesBacking Up Cached MessagesRestoring Cached MessagesMonitoring PubSub+ CacheSolCache EventsConfiguring Cache Cluster Event Thresholds Configuring Cache Cluster Event ThresholdsData Byte Rate ThresholdsData Message Rate ThresholdsMax Memory ThresholdsMax Topics ThresholdsRequest Queue Depth ThresholdsRequest Rate ThresholdsResponse Rate ThresholdsMonitoring SolCache Configuration and OperationsShow Product KeyShow Distributed CacheShow Cache ClusterShow Cache InstancePubSub+ Cache Instance States PubSub+ Cache Instance StatesClearing PubSub+ Cache Instance StatisticsOutput PubSub+ Cache Instance Debug InformationUsing SEMP to Monitor PubSub+ CacheSEMP Polling Frequency Guidelines","categories":[{"name":"Solace","slug":"Solace","permalink":"http://yoursite.com/categories/Solace/"},{"name":"Solace Cache","slug":"Solace/Solace-Cache","permalink":"http://yoursite.com/categories/Solace/Solace-Cache/"}],"tags":[]},{"title":"PySpark-RDD Memo_1","slug":"[spark]spark2.2.1_RDD_Memo_1","date":"2018-01-08T16:00:00.000Z","updated":"2020-07-22T08:36:44.362Z","comments":true,"path":"2018/01/09/[spark]spark2.2.1_RDD_Memo_1/","link":"","permalink":"http://yoursite.com/2018/01/09/[spark]spark2.2.1_RDD_Memo_1/","excerpt":"","text":"start pyspark 12345678910111213141516171819202122232425262728293031323334353637383940hadoop@hadoop-master:~$ pyspark###看到以下spark歡迎畫面表示pyspark啟動成功,pyspark預設會把SparkSession的Instance建構好Python 2.7.13 (default, Jan 19 2017, 14:48:08)[GCC 6.3.0 20170118] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.Setting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).2018-01-09 15:32:11,663 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable2018-01-09 15:32:21,550 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectExceptionWelcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ &apos;_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.2.1 /_/Using Python version 2.7.13 (default, Jan 19 2017 14:48:08)SparkSession available as &apos;spark&apos;.&gt;&gt;&gt;##開始可以操作pyspark相關語法## sc 是 SparkContext物件## spark 是 SparkSession物件&gt;&gt;&gt; sc&lt;SparkContext master=local[*] appName=PySparkShell&gt;&gt;&gt;&gt; spark&lt;pyspark.sql.session.SparkSession object at 0x7fa3a1ab4610&gt;&gt;&gt;&gt; from pyspark.conf import SparkConf&gt;&gt;&gt; conf = SparkConf()&gt;&gt;&gt; print (&quot;環境變數\\n&quot;)環境變數&gt;&gt;&gt; print (conf.toDebugString())spark.app.name=PySparkShellspark.master=local[*]spark.submit.deployMode=client pyspark: RDD Operate 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576## Apache Spark RDD - aggregate函數&gt;&gt;&gt; seqOp = (lambda x, y: (x[0] + y, x[1] + 1))&gt;&gt;&gt; combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).aggregate((0, 0), seqOp, combOp)(15, 5)&gt;&gt;&gt; sc.parallelize([]).aggregate((0, 0), seqOp, combOp)(0, 0)## Apache Spark RDD - cartesian&gt;&gt;&gt; rdd = sc.parallelize([1, 2])&gt;&gt;&gt; sorted(rdd.cartesian(rdd).collect())[(1, 1), (1, 2), (2, 1), (2, 2)]## Apache Spark RDD - glom,coalesce&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()[[1], [2, 3], [4, 5]]&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()[[1, 2, 3, 4, 5]]## Apache Spark RDD - collectAsMap&gt;&gt;&gt; m = sc.parallelize([(&quot;ted&quot;, 2), (&quot;kevin&quot;, 4)]).collectAsMap()&gt;&gt;&gt; m[&quot;ted&quot;]2&gt;&gt;&gt; m[&quot;kevin&quot;]4## Apache Spark RDD - combineByKey&gt;&gt;&gt; x = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 2)])&gt;&gt;&gt; def to_list(a):... return [a]...&gt;&gt;&gt; def append(a,b):... a.append(b)... return a...&gt;&gt;&gt; def extend(a,b):... a.extend(b)... return a...&gt;&gt;&gt; sorted(x.combineByKey(to_list, append, extend).collect())[(&apos;a&apos;, [1, 2]), (&apos;b&apos;, [1])]## Apache Spark RDD - count&gt;&gt;&gt; sc.parallelize([2, 3, 4]).count()3## Apache Spark RDD - countApproxDistinct&gt;&gt;&gt; n = sc.parallelize(range(1000)).map(str).countApproxDistinct()&gt;&gt;&gt; 900 &lt; n &lt; 1100True&gt;&gt;&gt; n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()&gt;&gt;&gt; 16 &lt; n &lt; 24True## Apache Spark RDD - countApprox&gt;&gt;&gt; rdd = sc.parallelize(range(1000), 10)&gt;&gt;&gt; rdd.countApprox(1000, 1.0)[Stage 20:===================================================&gt; (9 + 1) / 10]1000## Apache Spark RDD - countByKey&gt;&gt;&gt; rdd = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1)])&gt;&gt;&gt; sorted(rdd.countByKey().items())[(&apos;a&apos;, 2), (&apos;b&apos;, 1)]## Apache Spark RDD - countByValue&gt;&gt;&gt; sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())[(1, 2), (2, 3)]## Apache Spark RDD - distinct&gt;&gt;&gt; sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())[1, 2, 3]## Apache Spark RDD - filter&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4, 5])&gt;&gt;&gt; rdd.filter(lambda x: x % 2 == 0).collect()[2,4]","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Spark","slug":"BigData/Spark","permalink":"http://yoursite.com/categories/BigData/Spark/"}],"tags":[]},{"title":"Hadoop3.0.0 Document List","slug":"[hadoop]Hadoop3.0.0_doc","date":"2018-01-08T16:00:00.000Z","updated":"2020-07-22T08:28:51.203Z","comments":true,"path":"2018/01/09/[hadoop]Hadoop3.0.0_doc/","link":"","permalink":"http://yoursite.com/2018/01/09/[hadoop]Hadoop3.0.0_doc/","excerpt":"","text":"General OverviewHadoop: Setting up a Single Node Cluster.Hadoop Cluster SetupHadoop Commands GuideFileSystem ShellApache Hadoop CompatibilityApache Hadoop Downstream Developer’s GuideHadoop Interface Taxonomy: Audience and Stability ClassificationThe Hadoop FileSystem API Definition Common Hadoop: CLI MiniClusterNative Libraries GuideProxy user - Superusers Acting On Behalf Of Other UsersRack AwarenessHadoop in Secure ModeService Level Authorization GuideAuthentication for Hadoop HTTP web-consolesCredentialProvider API GuideHadoop Key Management Server (KMS) - Documentation SetsEnabling Dapper-like Tracing in HadoopUnix Shell Guide HDFS HDFS ArchitectureHDFS Users GuideHDFS Commands GuideHDFS High Availability Using the Quorum Journal ManagerHDFS High AvailabilityHDFS FederationViewFs GuideHDFS SnapshotsOffline Edits Viewer GuideOffline Image Viewer GuideHDFS Permissions GuideHDFS Quotas GuideC API libhdfsWebHDFS REST APIHadoop HDFS over HTTP - Documentation SetsHDFS Short-Circuit Local ReadsCentralized Cache Management in HDFSHDFS NFS GatewayHDFS Rolling UpgradeExtended Attributes in HDFSTransparent Encryption in HDFSHDFS Support for Multihomed NetworksArchival Storage, SSD &amp; MemoryMemory Storage Support in HDFSSynthetic Load Generator GuideHDFS Erasure CodingHDFS Disk BalancerHDFS Upgrade DomainHDFS DataNode Admin GuideUnix Shell GuideHDFS Router-based Federation MapReduce TutorialMapReduce Commands GuideApache Hadoop MapReduce - Migrating from Apache Hadoop 1.x to Apache Hadoop 2.xHadoop: Encrypted ShuffleHadoop: Pluggable Shuffle and Pluggable SortHadoop: Distributed Cache DeployMR Support for YARN Shared Cache MapReduce REST APIs MapReduce Application Master REST API’s.MapReduce History Server REST API’s. YARN Apache Hadoop YARNYARN CommandsHadoop: Capacity SchedulerHadoop: Fair SchedulerResourceManager RestartResourceManager High AvailabilityHadoop: YARN Resource ConfigurationYARN Node LabelsWeb Application ProxyThe YARN Timeline ServerThe YARN Timeline Service v.2Hadoop: Writing YARN ApplicationsYARN Application SecurityNodeManagerLaunching Applications Using Docker ContainersUsing CGroups with YARNYARN Secure ContainersYARN Service RegistryReservation SystemGraceful Decommission of YARN NodesOpportunistic ContainersHadoop: YARN FederationYARN Shared Cache YARN REST APIs Hadoop YARN - Introduction to the web services REST API’sResourceManager REST API’s.NodeManager REST API’sThe YARN Timeline ServerThe YARN Timeline Service v.2 Hadoop Compatible File Systems Hadoop-Aliyun module: Integration with Aliyun Web ServicesHadoop-AWS module: Integration with Amazon Web ServicesHadoop Azure Support: Azure Blob StorageHadoop Azure Data Lake SupportHadoop OpenStack Support: Swift Object Store Auth Hadoop Auth, Java HTTP SPNEGOHadoop Auth, Java HTTP SPNEGO - ExamplesHadoop Auth, Java HTTP SPNEGO - Server Side ConfigurationHadoop Auth, Java HTTP SPNEGO - Building It Tools Hadoop StreamingHadoop Archives GuideHadoop Archive Logs GuideDistCp GuideGridmixRumenResource Estimator ServiceYARN Scheduler Load Simulator (SLS)Hadoop Benchmarking Reference Changelog and Release NotesJava API docsUnix Shell APIMetrics Configuration core-default.xmlhdfs-default.xmlmapred-default.xmlyarn-default.xmlDeprecated Properties","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"Hadoop 3.0.0 Overview","slug":"[hadoop]Hadoop3.0.0_overview","date":"2018-01-08T16:00:00.000Z","updated":"2020-07-22T08:30:09.831Z","comments":true,"path":"2018/01/09/[hadoop]Hadoop3.0.0_overview/","link":"","permalink":"http://yoursite.com/2018/01/09/[hadoop]Hadoop3.0.0_overview/","excerpt":"","text":"Minimum required Java version increased from Java 7 to Java 8 Support for erasure coding in HDFS YARN Timeline Service v.2 Shell script rewrite Shaded client jars Support for Opportunistic Containers and Distributed Scheduling. MapReduce task-level native optimization Support for more than 2 NameNodes Default ports of multiple services have been changed. Support for Microsoft Azure Data Lake and Aliyun Object Storage System filesystem connectors Intra-datanode balancer Reworked daemon and task heap management S3Guard: Consistency and Metadata Caching for the S3A filesystem client HDFS Router-Based Federation API-based configuration of Capacity Scheduler queue configuration YARN Resource Types","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"Hadoop3.0.0 Quota","slug":"[hadoop]hadoop3.0.0_quota_cmd","date":"2018-01-08T16:00:00.000Z","updated":"2020-07-22T08:30:28.759Z","comments":true,"path":"2018/01/09/[hadoop]hadoop3.0.0_quota_cmd/","link":"","permalink":"http://yoursite.com/2018/01/09/[hadoop]hadoop3.0.0_quota_cmd/","excerpt":"","text":"Quota相關指令: 123456789Administrative Commands: hdfs dfsadmin [-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;] [-clrQuota &lt;dirname&gt;...&lt;dirname&gt;] [-setSpaceQuota &lt;quota&gt; [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;] [-clrSpaceQuota [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;] Reporting Commands: hadoop fs -count -q -v &lt;dirname&gt; Name Quotas 透過限制目錄或文件數量,當達到限制上限時,系統會發出錯誤訊息12345678910111213141516171819202122232425262728293031hadoop@hadoop-master:~$ hadoop fs -mkdir -p /user/hadoop/example/data2hadoop@hadoop-master:~$ hdfs dfsadmin -setQuota 3 /user/hadoop/example/data2## QUOTA --&gt;Name Quota(-setQuota設定數量)## REM_QUOTA(剩餘Quota) --&gt;REM_QUOTA = Quota-(DIR_COUNT+FILE_COUNT)hadoop@hadoop-master:~$ hadoop fs -count -q -v /user/hadoop/example/data2 QUOTA REM_QUOTA SPACE_QUOTA REM_SPACE_QUOTA DIR_COUNT FILE_COUNT CONTENT_SIZE PATHNAME 3 2 none inf 1 0 0 /user/hadoop/example/data2hadoop@hadoop-master:~$ hdfs dfs -put - /user/hadoop/example/data2/data1.txtTest1!!!!hadoop@hadoop-master:~$ hadoop fs -count -q -v /user/hadoop/example/data2 QUOTA REM_QUOTA SPACE_QUOTA REM_SPACE_QUOTA DIR_COUNT FILE_COUNT CONTENT_SIZE PATHNAME 3 1 none inf 1 1 10 /user/hadoop/example/data2hadoop@hadoop-master:~$ hdfs dfs -put - /user/hadoop/example/data2/data2.txtTest2!!!!hadoop@hadoop-master:~$ hadoop fs -count -q -v /user/hadoop/example/data2 QUOTA REM_QUOTA SPACE_QUOTA REM_SPACE_QUOTA DIR_COUNT FILE_COUNT CONTENT_SIZE PATHNAME 3 0 none inf 1 2 20 /user/hadoop/example/data2 ##無法再加入檔案,因為REM_QUOTA已經=0了 hadoop@hadoop-master:~$ hdfs dfs -put - /user/hadoop/example/data2/data3.txtput: The NameSpace quota (directories and files) of directory /user/hadoop/example/data2 is exceeded: quota=3 file count=4##清除Quota限制hadoop@hadoop-master:~$ hdfs dfsadmin -clrQuota /user/hadoop/example/data2hadoop@hadoop-master:~$ hdfs dfs -put - /user/hadoop/example/data2/data3.txtTEST3!!!!!!hadoop@hadoop-master:~$ hadoop fs -count -q -v /user/hadoop/example/data2 QUOTA REM_QUOTA SPACE_QUOTA REM_SPACE_QUOTA DIR_COUNT FILE_COUNT CONTENT_SIZE PATHNAME none inf none inf 1 3 32 /user/hadoop/example/data2 Space Quotas 透過限制目錄或檔案size大小,當達到限制上限時,系統會發出錯誤訊息(檔案大小和block size有關係)123456789101112131415161718192021hadoop@hadoop-master:~$ hadoop fs -mkdir -p /user/hadoop/example/data3hadoop@hadoop-master:~$ hdfs dfsadmin -setSpaceQuota 1M /user/hadoop/example/data3##Space Quota要考慮到Block size的問題,所以基本上會以block size的倍數來做為空間配額大小(block size*repl number)hadoop@hadoop-master:~$ hdfs dfsadmin -setSpaceQuota 64M /user/hadoop/example/data3hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/data3/data1.txtTest11111!!!hadoop@hadoop-master:~$ hadoop fs -count -q -v /user/hadoop/example/data3 QUOTA REM_QUOTA SPACE_QUOTA REM_SPACE_QUOTA DIR_COUNT FILE_COUNT CONTENT_SIZE PATHNAME none inf 67108864 67108851 1 1 13 /user/hadoop/example/data3hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/data3/data2.txtTEST2!!!!put: The DiskSpace quota of /user/hadoop/example/data3 is exceeded: quota = 67108864 B = 64 MB but diskspace consumed = 67108877 B = 64.00 MB## 清除SpaceQuota的限制hadoop@hadoop-master:~$ hdfs dfsadmin -clrSpaceQuota /user/hadoop/example/data3hadoop@hadoop-master:~$ hadoop fs -ls -R /user/hadoop/example/data3-rw-r--r-- 1 hadoop supergroup 13 2018-01-09 12:06 /user/hadoop/example/data3/data1.txthadoop@hadoop-master:~$ hadoop fs -count -q -v /user/hadoop/example/data3 QUOTA REM_QUOTA SPACE_QUOTA REM_SPACE_QUOTA DIR_COUNT FILE_COUNT CONTENT_SIZE PATHNAME none inf none inf 1 1 13 /user/hadoop/example/data3","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"hadoop3.0.0 相關服務 Port Number 設定參數","slug":"[hadoop]hadoop3_port_number_list","date":"2018-01-07T16:00:00.000Z","updated":"2020-07-22T08:30:51.651Z","comments":true,"path":"2018/01/08/[hadoop]hadoop3_port_number_list/","link":"","permalink":"http://yoursite.com/2018/01/08/[hadoop]hadoop3_port_number_list/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445dfs.balancer.address 0.0.0.0:0dfs.mover.address 0.0.0.0:0dfs.federation.router.http-address 0.0.0.0:50071dfs.federation.router.https-address 0.0.0.0:50072dfs.federation.router.admin-address 0.0.0.0:8111dfs.federation.router.rpc-address 0.0.0.0:8888dfs.namenode.backup.address 0.0.0.0:50100dfs.namenode.backup.http-address 0.0.0.0:50105dfs.journalnode.http-address 0.0.0.0:8480dfs.journalnode.https-address 0.0.0.0:8481dfs.journalnode.rpc-address 0.0.0.0:8485dfs.datanode.http.address 0.0.0.0:9864dfs.datanode.https.address 0.0.0.0:9865dfs.datanode.address 0.0.0.0:9866dfs.datanode.ipc.address 0.0.0.0:9867dfs.namenode.secondary.http-address 0.0.0.0:9868dfs.namenode.secondary.https-address 0.0.0.0:9869dfs.namenode.http-address 0.0.0.0:9870dfs.namenode.https-address 0.0.0.0:9871yarn.nodemanager.address $&#123;yarn.nodemanager.hostname&#125;:0yarn.nodemanager.localizer.address $&#123;yarn.nodemanager.hostname&#125;:8040yarn.nodemanager.webapp.address $&#123;yarn.nodemanager.hostname&#125;:8042yarn.nodemanager.collector-service.address $&#123;yarn.nodemanager.hostname&#125;:8048yarn.resourcemanager.scheduler.address $&#123;yarn.resourcemanager.hostname&#125;:8030yarn.resourcemanager.resource-tracker.address $&#123;yarn.resourcemanager.hostname&#125;:8031yarn.resourcemanager.address $&#123;yarn.resourcemanager.hostname&#125;:8032yarn.resourcemanager.admin.address $&#123;yarn.resourcemanager.hostname&#125;:8033yarn.resourcemanager.webapp.address $&#123;yarn.resourcemanager.hostname&#125;:8088yarn.resourcemanager.webapp.https.address $&#123;yarn.resourcemanager.hostname&#125;:8090yarn.timeline-service.address $&#123;yarn.timeline-service.hostname&#125;:10200yarn.timeline-service.webapp.address $&#123;yarn.timeline-service.hostname&#125;:8188yarn.timeline-service.webapp.https.address $&#123;yarn.timeline-service.hostname&#125;:8190mapreduce.jobhistory.address 0.0.0.0:10020 mapreduce.jobhistory.webapp.address 0.0.0.0:19888 mapreduce.jobhistory.webapp.https.address 0.0.0.0:19890mapreduce.jobhistory.admin.address 0.0.0.0:10033","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"hadoop3.0.0 daemon command","slug":"[hadoop]hadoop 3.0.0_daemon_cmd","date":"2018-01-07T16:00:00.000Z","updated":"2020-07-22T08:25:48.726Z","comments":true,"path":"2018/01/08/[hadoop]hadoop 3.0.0_daemon_cmd/","link":"","permalink":"http://yoursite.com/2018/01/08/[hadoop]hadoop 3.0.0_daemon_cmd/","excerpt":"","text":"1234567891011121314151617181920hdfs --daemon start balancer run a cluster balancing utilityhdfs --daemon start datanode run a DFS datanodehdfs --daemon start dfsrouter run the DFS routerhdfs --daemon start diskbalancer Distributes data evenly among disks on a given nodehdfs --daemon start journalnode run the DFS journalnodehdfs --daemon start mover run a utility to move block replicas across storage typeshdfs --daemon start namenode run the DFS namenodehdfs --daemon start nfs3 run an NFS version 3 gatewayhdfs --daemon start portmap run a portmap servicehdfs --daemon start secondarynamenode run the DFS secondary namenodehdfs --daemon start zkfc run the ZK Failover Controller daemonyarn --daemon start nodemanager run a nodemanager on each worker yarn --daemon start proxyserver run the web app proxy serveryarn --daemon start resourcemanager run the ResourceManageryarn --daemon start router run the Router daemonyarn --daemon start sharedcachemanager run the SharedCacheManager daemonyarn --daemon start timelineserver run the timeline servermapred --daemon start historyserver run job history servers as a standalone daemon","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"Hadoop3.0.0 NFS Gateway","slug":"[hadoop]hadoop3.0.0_nfs_gateway","date":"2018-01-07T16:00:00.000Z","updated":"2020-07-22T08:29:54.781Z","comments":true,"path":"2018/01/08/[hadoop]hadoop3.0.0_nfs_gateway/","link":"","permalink":"http://yoursite.com/2018/01/08/[hadoop]hadoop3.0.0_nfs_gateway/","excerpt":"","text":"Hadoop3.0.0 NFS Gateway setting &amp; start 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990hadoop@hadoop-master:~$ service nfs-kernel-server stophadoop@hadoop-master:~$ start-dfs.shhadoop@hadoop-master:~$ start-yarn.shhadoop@hadoop-master:~$ hdfs --daemon start nfs3hadoop@hadoop-master:~$ rpcinfo -p program vers proto port service 100000 4 tcp 111 portmapper 100000 3 tcp 111 portmapper 100000 2 tcp 111 portmapper 100000 4 udp 111 portmapper 100000 3 udp 111 portmapper 100000 2 udp 111 portmapper 100005 1 udp 46340 mountd 100005 1 tcp 33399 mountd 100005 2 udp 56322 mountd 100005 2 tcp 43065 mountd 100005 3 udp 52402 mountd 100005 3 tcp 49301 mountd 100003 2 tcp 2049 nfs 100003 3 tcp 2049 nfs 100003 4 tcp 2049 nfs 100227 2 tcp 2049 100227 3 tcp 2049 100003 2 udp 2049 nfs 100003 3 udp 2049 nfs 100003 4 udp 2049 nfs 100227 2 udp 2049 100227 3 udp 2049 100021 1 udp 50043 nlockmgr 100021 3 udp 50043 nlockmgr 100021 4 udp 50043 nlockmgr 100021 1 tcp 33247 nlockmgr 100021 3 tcp 33247 nlockmgr 100021 4 tcp 33247 nlockmgr hadoop@hadoop-master:~$ showmount -eExport list for hadoop-master:/ * hadoop@hadoop-master:~$ netstat -tnlActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 0 0.0.0.0:22 0.0.0.0:* LISTENtcp 0 0 192.168.51.4:8088 0.0.0.0:* LISTENtcp 0 0 192.168.51.4:8030 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:50079 0.0.0.0:* LISTENtcp 0 0 192.168.51.4:8031 0.0.0.0:* LISTENtcp 0 0 192.168.51.4:8032 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:2049 0.0.0.0:* LISTENtcp 0 0 192.168.51.4:8033 0.0.0.0:* LISTENtcp 0 0 192.168.51.4:50090 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:5355 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:9870 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:111 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:4242 0.0.0.0:* LISTENtcp 0 0 192.168.51.4:8020 0.0.0.0:* LISTENtcp6 0 0 :::22 :::* LISTENtcp6 0 0 :::5355 :::* LISTENtcp6 0 0 :::111 :::* LISTENhadoop@hadoop-master:~$ jps4116 Nfs32392 ResourceManager2108 SecondaryNameNode4157 Jps1837 NameNodehadoop@hadoop-master:~$ mkdir -p /opt/hdfshadoop@hadoop-master:~$ sudo mount -t nfs -o vers=3,proto=tcp,nolock,sync 192.168.51.4:/ /opt/hdfshadoop@hadoop-master:~$ ls -la /opt/hdfs/usertotal 4drwxr-xr-x 6 hadoop 2584148964 192 Jan 4 16:35 .drwxr-xr-x 4 hadoop 2584148964 128 Dec 26 15:25 ..drwxr-xr-x 41 hadoop 2584148964 1312 Jan 8 17:00 hadoopdrwxr-xr-x 3 hadoop 2584148964 96 Dec 26 18:44 hivedrwxr-xr-x 2 hadoop 2584148964 64 Jan 3 17:30 snapshotdrwxr-xr-x 2 hadoop 2584148964 64 Jan 4 16:35 vagranthadoop@hadoop-master:/home$ sudo umount /opt/hdfshadoop@hadoop-master:/home$ ls -la /opt/hdfstotal 8drwxrwxr-x 2 hadoop hadoop 4096 Jan 8 17:32 .drwxr-xr-x 4 hadoop hadoop 4096 Jan 8 17:32 .. Hadoop3.0.0 NFS Gateway modify mount point 12345678910111213141516171819202122232425262728293031vi /bgdt/hadoop-3.0.0/etc/hadoop/hdfs-site.xml##在設定檔中加入以下參數(nfs.export.point)&lt;property&gt; &lt;name&gt;nfs.export.point&lt;/name&gt; &lt;value&gt;/user&lt;/value&gt;&lt;/property&gt;##重新啟動HDFShadoop@hadoop-master:~$start-dfs.sh##重新啟動NFS3hadoop@hadoop-master:~$hdfs --daemon start nfs3hadoop@hadoop-master:~$showmount -eExport list for hadoop-master:/user *hadoop@hadoop-master:~$ sudo mount -t nfs -o vers=3,proto=tcp,nolock,sync 192.168.51.4:/user /opt/hdfshadoop@hadoop-master:~$ ls -la /opt/hdfstotal 8drwxr-xr-x 6 hadoop 2584148964 192 Jan 4 16:35 .drwxr-xr-x 4 hadoop hadoop 4096 Jan 8 17:32 ..drwxr-xr-x 41 hadoop 2584148964 1312 Jan 8 17:00 hadoopdrwxr-xr-x 3 hadoop 2584148964 96 Dec 26 18:44 hivedrwxr-xr-x 2 hadoop 2584148964 64 Jan 3 17:30 snapshotdrwxr-xr-x 2 hadoop 2584148964 64 Jan 4 16:35 vagranthadoop@hadoop-master:~$ sudo umount /opt/hdfs","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"hadoop archive command Memo","slug":"[hadoop]hadoop_archive_cmd","date":"2018-01-03T16:00:00.000Z","updated":"2020-07-22T08:26:27.142Z","comments":true,"path":"2018/01/04/[hadoop]hadoop_archive_cmd/","link":"","permalink":"http://yoursite.com/2018/01/04/[hadoop]hadoop_archive_cmd/","excerpt":"","text":"hadoop壓縮機制 12345usage: archive &lt;-archiveName &lt;NAME&gt;.har&gt; &lt;-p &lt;parent path&gt;&gt; [-r &lt;replication factor&gt;] &lt;src&gt;* &lt;dest&gt; -archiveName &lt;arg&gt; Name of the Archive. This is mandatory option -help Show the usage -p &lt;arg&gt; Parent path of sources. This is mandatory option -r &lt;arg&gt; Replication factor archive files hadoop archive操作方式 12345678910111213141516171819202122232425262728293031hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/test1.txthadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/test2.txthadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/test3.txt##壓縮/user/hadoop/example目錄下所有檔案和目錄hadoop@hadoop-master:~$ hadoop archive -archiveName example1.har -p /user/hadoop/example -r 3 /user/hadoop##壓縮/user/hadoop/example目錄下所有txt檔hadoop@hadoop-master:~$ hadoop archive -archiveName example2.har -p /user/hadoop/example/ -r 3 *.txt /user/hadoop## 查詢壓縮檔的內容hadoop@hadoop-master:~$ hadoop fs -ls -R har:///user/hadoop/example1.hardrwxr-xr-x - hadoop supergroup 0 2018-01-04 18:12 har:///user/hadoop/example1.har/lab_2drwxr-xr-x - hadoop supergroup 0 2018-01-04 18:13 har:///user/hadoop/example1.har/lab_2/example1.har-rw-r--r-- 3 hadoop supergroup 373 2018-01-03 15:41 har:///user/hadoop/example1.har/lab_2/test9.tar.gz-rw-r--r-- 3 hadoop supergroup 23 2018-01-04 18:09 har:///user/hadoop/example1.har/test1.txt-rw-r--r-- 3 hadoop supergroup 30 2018-01-04 18:10 har:///user/hadoop/example1.har/test2.txt-rw-r--r-- 3 hadoop supergroup 15 2018-01-04 18:10 har:///user/hadoop/example1.har/test3.txthadoop@hadoop-master:~$ hadoop fs -ls -R har:///user/hadoop/example2.har-rw-r--r-- 3 hadoop supergroup 23 2018-01-04 18:09 har:///user/hadoop/example2.har/test1.txt-rw-r--r-- 3 hadoop supergroup 30 2018-01-04 18:10 har:///user/hadoop/example2.har/test2.txt-rw-r--r-- 3 hadoop supergroup 15 2018-01-04 18:10 har:///user/hadoop/example2.har/test3.txt##解壓縮使用cp命令hadoop@hadoop-master:~$ hadoop fs -mkdir -p /user/hadoop/example/lab_3hadoop@hadoop-master:~$ hadoop fs -cp har:///user/hadoop/example2.har/* hdfs:/user/hadoop/example/lab_3hadoop@hadoop-master:~$ hadoop fs -ls -R /user/hadoop/example/lab_3-rw-r--r-- 1 hadoop supergroup 23 2018-01-04 18:34 /user/hadoop/example/lab_3/test1.txt-rw-r--r-- 1 hadoop supergroup 30 2018-01-04 18:34 /user/hadoop/example/lab_3/test2.txt-rw-r--r-- 1 hadoop supergroup 15 2018-01-04 18:34 /user/hadoop/example/lab_3/test3.txt","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"hdfs snapshot command Memo","slug":"[hadoop]hadoop_snapshot_cmd","date":"2018-01-03T16:00:00.000Z","updated":"2020-07-22T08:28:34.749Z","comments":true,"path":"2018/01/04/[hadoop]hadoop_snapshot_cmd/","link":"","permalink":"http://yoursite.com/2018/01/04/[hadoop]hadoop_snapshot_cmd/","excerpt":"","text":"Hadoop Snapshot機制用到的相關語法 12345678hdfs lsSnapshottableDirhdfs snapshotDiffhdfs dfsadmin -allowSnapshothdfs dfsadmin -disallowSnapshothdfs dfs -createSnapshothdfs dfs -deleteSnapshothdfs dfs -renameSnapshothdfs fsck -includeSnapshots Snapshot機制操作說明 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465hadoop@hadoop-master:~$ hadoop fs -mkdir /user/hadoop/snapshot##選定一個目錄做為Snapshot的起點hadoop@hadoop-master:~$ hdfs dfsadmin -allowSnapshot /user/hadoop/snapshotAllowing snaphot on /user/hadoop/snapshot succeeded##列出所有要做Snapshot目錄hadoop@hadoop-master:~$ hdfs lsSnapshottableDirdrwxr-xr-x 0 hadoop supergroup 0 2018-01-03 16:50 1 65536 /user/hadoop/snapshot##建立Snapshothadoop@hadoop-master:~$ hadoop fs -createSnapshot /user/hadoop/snapshot snapshotCreated snapshot /user/hadoop/snapshot/.snapshot/snapshot##在Snapshot目錄中新增一個檔案hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/snapshot/test1.txt111112222333334444455555##再做一次Snapshothadoop@hadoop-master:~$ hadoop fs -createSnapshot /user/hadoop/snapshot snapshot_201801031654Created snapshot /user/hadoop/snapshot/.snapshot/snapshot_201801031654##比較兩個Snapshot之間的差異hadoop@hadoop-master:~$ hdfs snapshotDiff /user/hadoop/snapshot snapshot snapshot_201801031654Difference between snapshot snapshot and snapshot snapshot_201801031654 under directory /user/hadoop/snapshot:M .+ ./test1.txt##以下為復原Snapshot方式hadoop@hadoop-master:~$hadoop fs -rm -r -skipTrash /user/hadoop/snapshot/*hadoop@hadoop-master:~$hadoop fs -cp /user/hadoop/snapshot/.snapshot/snapshot/* /user/hadoop/snapshot##刪除Snapshothadoop@hadoop-master:~$ hadoop fs -mkdir /tmp/important-datahadoop@hadoop-master:~$ echo &quot;important data&quot; | hdfs dfs -put - /tmp/important-dir/important-file.txthadoop@hadoop-master:~$ hdfs dfsadmin -allowSnapshot /tmp/important-dirhadoop@hadoop-master:~$ hdfs dfs -createSnapshot /tmp/important-dir first-snapshothadoop@hadoop-master:~$ hdfs lsSnapshottableDirdrwxr-xr-x 0 hadoop supergroup 0 2018-01-04 11:12 1 65536 /tmp/important-dirdrwxr-xr-x 0 hadoop supergroup 0 2018-01-04 11:59 3 65536 /user/hadoop/snapshot##此目錄下尚有其他Snapshot存在無法操作disallowSnapshothadoop@hadoop-master:~$ hdfs dfsadmin -disallowSnapshot /tmp/important-dirdisallowSnapshot: The directory /tmp/important-dir has snapshot(s). Please redo the operation after removing all the snapshots.##須將所有Snapshot刪除後,才能操作disallowSnapshothadoop@hadoop-master:~$ hdfs dfs -deleteSnapshot /tmp/important-dir first-snapshothadoop@hadoop-master:~$ hdfs dfsadmin -disallowSnapshot /tmp/important-dirDisallowing snaphot on /tmp/important-dir succeededhadoop@hadoop-master:~$ hdfs lsSnapshottableDirdrwxr-xr-x 0 hadoop supergroup 0 2018-01-04 11:59 3 65536 /user/hadoop/snapshot##更改Snapshot名稱,並顯示snapshottable下所有的Snapshothadoop@hadoop-master:~$ hdfs dfs -renameSnapshot /user/hadoop/snapshot snapshot snapshot_1hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/snapshot/.snapshotFound 3 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-03 17:01 /user/hadoop/snapshot/.snapshot/snapshot_1drwxr-xr-x - hadoop supergroup 0 2018-01-03 17:01 /user/hadoop/snapshot/.snapshot/snapshot_201801031654drwxr-xr-x - hadoop supergroup 0 2018-01-03 18:01 /user/hadoop/snapshot/.snapshot/snapshot_201801031737","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"hdfs client command Memo","slug":"[hadoop]hdfs_other_cmd","date":"2018-01-02T16:00:00.000Z","updated":"2020-07-22T08:32:27.248Z","comments":true,"path":"2018/01/03/[hadoop]hdfs_other_cmd/","link":"","permalink":"http://yoursite.com/2018/01/03/[hadoop]hdfs_other_cmd/","excerpt":"","text":"hdfs getconf相關用法 123456789101112131415161718192021222324252627282930Usage:hadoop getconf [-namenodes] gets list of namenodes in the cluster. [-secondaryNameNodes] gets list of secondary namenodes in the cluster. [-backupNodes] gets list of backup nodes in the cluster. [-includeFile] gets the include file path that defines the datanodes that can join the cluster. [-excludeFile] gets the exclude file path that defines the datanodes that need to decommissioned. [-nnRpcAddresses] gets the namenode rpc addresses [-confKey [key]] gets a specific key from the configuration---hadoop@hadoop-master:~$ hdfs getconf -namenodeshadoop-masterhadoop@hadoop-master:~$ hdfs getconf -secondaryNameNodeshadoop-masterhadoop@hadoop-master:~$ hdfs getconf -backupNodes0.0.0.0hadoop@hadoop-master:~$ hdfs getconf -nnRpcAddresseshadoop-master:8020hadoop@hadoop-master:~$ hdfs getconf -includeFileConfiguration dfs.hosts is missing.hadoop@hadoop-master:~$ hdfs getconf -excludeFileConfiguration dfs.hosts.exclude is missing.hadoop@hadoop-master:~$ hdfs getconf -confKey &quot;fs.trash.interval&quot;1440 hdfs envvars相關用法 123456789hadoop@hadoop-master:~$ hdfs envvarsJAVA_HOME=&apos;/bgdt/java/jdk1.8.0_101&apos;HADOOP_HDFS_HOME=&apos;/bgdt/hadoop-3.0.0&apos;HDFS_DIR=&apos;share/hadoop/hdfs&apos;HDFS_LIB_JARS_DIR=&apos;share/hadoop/hdfs/lib&apos;HADOOP_CONF_DIR=&apos;/bgdt/hadoop-3.0.0/etc/hadoop&apos;HADOOP_TOOLS_HOME=&apos;/bgdt/hadoop-3.0.0&apos;HADOOP_TOOLS_DIR=&apos;share/hadoop/tools&apos;HADOOP_TOOLS_LIB_JARS_DIR=&apos;share/hadoop/tools/lib&apos; hdfs classpath相關用法 123456789101112hadoop@hadoop-master:~$ hdfs classpath/bgdt/hadoop-3.0.0/etc/hadoop:/bgdt/hadoop-3.0.0/share/hadoop/common/lib/*:/bgdt/hadoop-3.0.0/share/hadoop/common/*:/bgdt/hadoop-3.0.0/share/hadoop/hdfs:/bgdt/hadoop-3.0.0/share/hadoop/hdfs/lib/*:/bgdt/hadoop-3.0.0/share/hadoop/hdfs/*:/bgdt/hadoop-3.0.0/share/hadoop/mapreduce/lib/*:/bgdt/hadoop-3.0.0/share/hadoop/mapreduce/*:/bgdt/hadoop-3.0.0/share/hadoop/yarn:/bgdt/hadoop-3.0.0/share/hadoop/yarn/lib/*:/bgdt/hadoop-3.0.0/share/hadoop/yarn/*:/bgdt/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0 hdfs version相關用法 1234567hadoop@hadoop-master:~$ hdfs versionHadoop 3.0.0Source code repository https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533Compiled by andrew on 2017-12-08T19:16ZCompiled with protoc 2.5.0From source with checksum 397832cb5529187dc8cd74ad54ff22This command was run using /bgdt/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar hdfs groups相關用法 12hadoop@hadoop-master:~$ hdfs groupshadoop : hadoop hadoop conftest 1234567891011hadoop@hadoop-master:~$ hadoop conftest/bgdt/hadoop-3.0.0/etc/hadoop/hadoop-policy.xml: valid/bgdt/hadoop-3.0.0/etc/hadoop/yarn-site.xml: valid/bgdt/hadoop-3.0.0/etc/hadoop/hdfs-site.xml: valid/bgdt/hadoop-3.0.0/etc/hadoop/core-site.xml: valid/bgdt/hadoop-3.0.0/etc/hadoop/mapred-site.xml: valid/bgdt/hadoop-3.0.0/etc/hadoop/capacity-scheduler.xml: valid/bgdt/hadoop-3.0.0/etc/hadoop/kms-site.xml: valid/bgdt/hadoop-3.0.0/etc/hadoop/httpfs-site.xml: valid/bgdt/hadoop-3.0.0/etc/hadoop/kms-acls.xml: validOK hadoop jnipath 12hadoop@hadoop-master:~$ hadoop jnipath/bgdt/hadoop-3.0.0/lib/native hadoop checknative 12345678910111213141516171819hadoop@hadoop-master:~$ hadoop checknative2018-01-04 17:18:04,659 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native2018-01-04 17:18:04,667 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library2018-01-04 17:18:04,682 ERROR snappy.SnappyCompressor: failed to load SnappyCompressorjava.lang.UnsatisfiedLinkError: Cannot load libsnappy.so.1 (libsnappy.so.1: cannot open shared object file: No such file or directory)! at org.apache.hadoop.io.compress.snappy.SnappyCompressor.initIDs(Native Method) at org.apache.hadoop.io.compress.snappy.SnappyCompressor.&lt;clinit&gt;(SnappyCompressor.java:57) at org.apache.hadoop.io.compress.SnappyCodec.isNativeCodeLoaded(SnappyCodec.java:82) at org.apache.hadoop.util.NativeLibraryChecker.main(NativeLibraryChecker.java:100)2018-01-04 17:18:04,691 WARN erasurecode.ErasureCodeNative: ISA-L support is not available in your platform... using builtin-java codec where applicableNative library checking:hadoop: true /bgdt/hadoop-3.0.0/lib/native/libhadoop.so.1.0.0zlib: true /lib/x86_64-linux-gnu/libz.so.1zstd : falsesnappy: falselz4: true revision:10301bzip2: true /lib/x86_64-linux-gnu/libbz2.so.1openssl: false Cannot load libcrypto.so (libcrypto.so: cannot open shared object file: No such file or directory)!ISA-L: false libhadoop was built without ISA-L support hadoop daemonlog 123456789101112131415161718192021##取得Class Log Level級別hadoop@hadoop-master:~$ hadoop daemonlog -getlevel hadoop-master:9870 org.apache.hadoop.yarn.server.nodemanager.NodeManagerConnecting to http://hadoop-master:9870/logLevel?log=org.apache.hadoop.yarn.server.nodemanager.NodeManagerSubmitted Class Name: org.apache.hadoop.yarn.server.nodemanager.NodeManagerLog Class: org.apache.commons.logging.impl.Log4JLoggerEffective Level: INFO##設定Class Log Level級別hadoop@hadoop-master:~$ hadoop daemonlog -setlevel hadoop-master:9870 org.apache.hadoop.yarn.server.nodemanager.NodeManager DEBUGConnecting to http://hadoop-master:9870/logLevel?log=org.apache.hadoop.yarn.server.nodemanager.NodeManager&amp;level=DEBUGSubmitted Class Name: org.apache.hadoop.yarn.server.nodemanager.NodeManagerLog Class: org.apache.commons.logging.impl.Log4JLoggerSubmitted Level: DEBUGSetting Level to DEBUG ...Effective Level: DEBUGhadoop@hadoop-master:~$ hadoop daemonlog -getlevel hadoop-master:9870 org.apache.hadoop.yarn.server.nodemanager.NodeManagerConnecting to http://hadoop-master:9870/logLevel?log=org.apache.hadoop.yarn.server.nodemanager.NodeManagerSubmitted Class Name: org.apache.hadoop.yarn.server.nodemanager.NodeManagerLog Class: org.apache.commons.logging.impl.Log4JLoggerEffective Level: DEBUG","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"hdfs ec command Memo","slug":"[hadoop]hdfs_ec_cmd","date":"2018-01-02T16:00:00.000Z","updated":"2020-07-22T08:32:10.444Z","comments":true,"path":"2018/01/03/[hadoop]hdfs_ec_cmd/","link":"","permalink":"http://yoursite.com/2018/01/03/[hadoop]hdfs_ec_cmd/","excerpt":"","text":"hdfs ec 使用方式(以XOR-2-1-1024k Policy做為測試) EC相關架構的介紹可以參考網路相關的文章123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174Usage: bin/hdfs ec [COMMAND] [-listPolicies] [-addPolicies -policyFile &lt;file&gt;] [-getPolicy -path &lt;path&gt;] [-removePolicy -policy &lt;policy&gt;] [-setPolicy -path &lt;path&gt; [-policy &lt;policy&gt;] [-replicate]] [-unsetPolicy -path &lt;path&gt;] [-listCodecs] [-enablePolicy -policy &lt;policy&gt;] [-disablePolicy -policy &lt;policy&gt;] [-help &lt;command-name&gt;]##列出所有Erasure Coding可用的相關Policieshadoop@hadoop-master:~$ hdfs ec -listPoliciesErasure Coding Policies:ErasureCodingPolicy=[Name=RS-10-4-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=10, numParityUnits=4]], CellSize=1048576, Id=5], State=DISABLEDErasureCodingPolicy=[Name=RS-3-2-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=3, numParityUnits=2]], CellSize=1048576, Id=2], State=DISABLEDErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1], State=DISABLEDErasureCodingPolicy=[Name=RS-LEGACY-6-3-1024k, Schema=[ECSchema=[Codec=rs-legacy, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=3], State=DISABLEDErasureCodingPolicy=[Name=XOR-2-1-1024k, Schema=[ECSchema=[Codec=xor, numDataUnits=2, numParityUnits=1]], CellSize=1048576, Id=4], State=DISABLED##列出所有Erasure Coding Codec列表hadoop@hadoop-master:~$ hdfs ec -listCodecsErasure Coding Codecs: Codec [Coder List] RS [RS_NATIVE, RS_JAVA] RS-LEGACY [RS-LEGACY_JAVA] XOR [XOR_NATIVE, XOR_JAVA]##Enable XOR-2-1-1024k的EC Policy hadoop@hadoop-master:~$ hdfs ec -enablePolicy -policy XOR-2-1-1024kErasure coding policy XOR-2-1-1024k is enabledhadoop@hadoop-master:~$ hdfs ec -listPoliciesErasure Coding Policies:ErasureCodingPolicy=[Name=RS-10-4-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=10, numParityUnits=4]], CellSize=1048576, Id=5], State=DISABLEDErasureCodingPolicy=[Name=RS-3-2-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=3, numParityUnits=2]], CellSize=1048576, Id=2], State=DISABLEDErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1], State=DISABLEDErasureCodingPolicy=[Name=RS-LEGACY-6-3-1024k, Schema=[ECSchema=[Codec=rs-legacy, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=3], State=DISABLEDErasureCodingPolicy=[Name=XOR-2-1-1024k, Schema=[ECSchema=[Codec=xor, numDataUnits=2, numParityUnits=1]], CellSize=1048576, Id=4], State=ENABLEDhadoop@hadoop-master:~$ hadoop fs -mkdir -p /user/hadoop/example/lab_2##將目錄設定為XOR-2-1-1024k的EC Policyhadoop@hadoop-master:~$ hdfs ec -setPolicy -path /user/hadoop/example/lab_2 -policy XOR-2-1-1024kSet erasure coding policy XOR-2-1-1024k on /user/hadoop/example/lab_2##上傳檔案hadoop@hadoop-master:~$ hadoop fs -put ~/test9.tar.gz /user/hadoop/example/lab_22018-01-03 15:41:12,678 WARN erasurecode.ErasureCodeNative: ISA-L support is not available in your platform... using builtin-java codec where applicable2018-01-03 15:41:12,741 WARN hdfs.DFSOutputStream: Cannot allocate parity block(index=2, policy=XOR-2-1-1024k). Not enough datanodes? Exclude nodes=[]2018-01-03 15:41:12,953 WARN hdfs.DFSOutputStream: Block group &lt;1&gt; has 1 corrupt blocks. It&apos;s at high risk of losing data.##觀察上傳檔案的複本數hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_2Found 1 items-rw-r--r-- 1 hadoop supergroup 373 2018-01-03 15:41 /user/hadoop/example/lab_2/test9.tar.gz##以下為使用fsck指令觀察該檔案Block相關訊息hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/example/lab_2/test9.tar.gz -files -blocks -locationsConnecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;locations=1&amp;path=%2Fuser%2Fhadoop%2Fexample%2Flab_2%2Ftest9.tar.gzFSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/example/lab_2/test9.tar.gz at Wed Jan 03 15:46:48 CST 2018/user/hadoop/example/lab_2/test9.tar.gz 373 bytes, erasure-coded: policy=XOR-2-1-1024k, 1 block(s): OK0. BP-1139418417-192.168.51.4-1514272982687:blk_-9223372036854775776_2346 len=373 Live_repl=2 [blk_-9223372036854775776:DatanodeInfoWithStorage[192.168.51.5:9866,DS-f59711c6-801d-4ebc-b55c-228f225117b8,DISK], blk_-9223372036854775774:DatanodeInfoWithStorage[192.168.51.6:9866,DS-482194d9-aa70-4ab8-8253-907739d5b1a1,DISK]]Status: HEALTHY Number of data-nodes: 2 Number of racks: 1 Total dirs: 0 Total symlinks: 0Replicated Blocks: Total size: 0 B Total files: 0 Total blocks (validated): 0 Minimally replicated blocks: 0 Over-replicated blocks: 0 Under-replicated blocks: 0 Mis-replicated blocks: 0 Default replication factor: 1 Average block replication: 0.0 Missing blocks: 0 Corrupt blocks: 0 Missing replicas: 0Erasure Coded Block Groups: Total size: 373 B Total files: 1 Total block groups (validated): 1 (avg. block group size 373 B) Minimally erasure-coded block groups: 1 (100.0 %) Over-erasure-coded block groups: 0 (0.0 %) Under-erasure-coded block groups: 0 (0.0 %) Unsatisfactory placement block groups: 0 (0.0 %) Average block group size: 2.0 Missing block groups: 0 Corrupt block groups: 0 Missing internal blocks: 0 (0.0 %)FSCK ended at Wed Jan 03 15:46:48 CST 2018 in 2 millisecondsThe filesystem under path &apos;/user/hadoop/example/lab_2/test9.tar.gz&apos; is HEALTHY##以blockId觀察複本狀況hadoop@hadoop-master:~$ hdfs fsck -blockId blk_-9223372036854775776Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;blockId=blk_-9223372036854775776+&amp;path=%2FFSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 at Wed Jan 03 15:49:12 CST 2018Block Id: blk_-9223372036854775776Block belongs to: /user/hadoop/example/lab_2/test9.tar.gzNo. of Expected Replica: 2No. of live Replica: 2No. of excess Replica: 0No. of stale Replica: 0No. of decommissioned Replica: 0No. of decommissioning Replica: 0No. of corrupted Replica: 0nullFsck on blockId &apos;blk_-9223372036854775776##以下為模擬shutdown 一台Datanode,資料是否還會存在??hadoop@hadoop-master:~$ hdfs dfsadmin -shutdownDatanode hadoop-slave2:9867Submitted a shutdown request to datanode hadoop-slave2:9867hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/example/lab_2/test9.tar.gz -files -blocks -locationsConnecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;locations=1&amp;path=%2Fuser%2Fhadoop%2Fexample%2Flab_2%2Ftest9.tar.gzFSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/example/lab_2/test9.tar.gz at Wed Jan 03 16:11:01 CST 2018/user/hadoop/example/lab_2/test9.tar.gz 373 bytes, erasure-coded: policy=XOR-2-1-1024k, 1 block(s): Under replicated BP-1139418417-192.168.51.4-1514272982687:blk_-9223372036854775776_2346. Target Replicas is 2 but found 1 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).0. BP-1139418417-192.168.51.4-1514272982687:blk_-9223372036854775776_2346 len=373 Live_repl=1 [blk_-9223372036854775776:DatanodeInfoWithStorage[192.168.51.5:9866,DS-f59711c6-801d-4ebc-b55c-228f225117b8,DISK]]Status: HEALTHY Number of data-nodes: 1 Number of racks: 1 Total dirs: 0 Total symlinks: 0Replicated Blocks: Total size: 0 B Total files: 0 Total blocks (validated): 0 Minimally replicated blocks: 0 Over-replicated blocks: 0 Under-replicated blocks: 0 Mis-replicated blocks: 0 Default replication factor: 1 Average block replication: 0.0 Missing blocks: 0 Corrupt blocks: 0 Missing replicas: 0Erasure Coded Block Groups: Total size: 373 B Total files: 1 Total block groups (validated): 1 (avg. block group size 373 B) Minimally erasure-coded block groups: 1 (100.0 %) Over-erasure-coded block groups: 0 (0.0 %) Under-erasure-coded block groups: 1 (100.0 %) Unsatisfactory placement block groups: 0 (0.0 %) Average block group size: 1.0 Missing block groups: 0 Corrupt block groups: 0 Missing internal blocks: 1 (50.0 %)FSCK ended at Wed Jan 03 16:11:01 CST 2018 in 3 millisecondsThe filesystem under path &apos;/user/hadoop/example/lab_2/test9.tar.gz&apos; is HEALTHY雖然複本數是設定為1,但因為使用的是EC策略,資料仍然是可下載而且資料內容也是正確的","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"hdfs fsck command Memo","slug":"[hadoop]hadoop_fsck_cmd","date":"2018-01-02T16:00:00.000Z","updated":"2020-07-22T08:27:53.217Z","comments":true,"path":"2018/01/03/[hadoop]hadoop_fsck_cmd/","link":"","permalink":"http://yoursite.com/2018/01/03/[hadoop]hadoop_fsck_cmd/","excerpt":"","text":"hadoop fsck使用方式 12345678910111213141516171819202122232425262728293031323334353637383940hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txtConnecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;path=%2Fuser%2Fhadoop%2Ftest1.txtFSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:15:13 CST 2018Status: HEALTHY Number of data-nodes: 2 Number of racks: 1 Total dirs: 0 Total symlinks: 0Replicated Blocks: Total size: 133 B Total files: 1 Total blocks (validated): 1 (avg. block size 133 B) Minimally replicated blocks: 1 (100.0 %) Over-replicated blocks: 0 (0.0 %) Under-replicated blocks: 0 (0.0 %) Mis-replicated blocks: 0 (0.0 %) Default replication factor: 2 Average block replication: 2.0 Missing blocks: 0 Corrupt blocks: 0 Missing replicas: 0 (0.0 %)Erasure Coded Block Groups: Total size: 0 B Total files: 0 Total block groups (validated): 0 Minimally erasure-coded block groups: 0 Over-erasure-coded block groups: 0 Under-erasure-coded block groups: 0 Unsatisfactory placement block groups: 0 Average block group size: 0.0 Missing block groups: 0 Corrupt block groups: 0 Missing internal blocks: 0FSCK ended at Wed Jan 03 13:15:13 CST 2018 in 1 millisecondsThe filesystem under path &apos;/user/hadoop/test1.txt&apos; is HEALTHY hdfs fsck 相關參數操作說明 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960##檢查並列出所有文件的狀態hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -filesConnecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txtFSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:17:02 CST 2018/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s): OK##列出所有Blocks資訊hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files -blocksConnecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txtFSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:18:46 CST 2018/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s): OK0. BP-1139418417-192.168.51.4-1514272982687:blk_1073743138_2323 len=133 Live_repl=2##列出所有Blocks的位置訊息hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files -blocks -locationsConnecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;locations=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txtFSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:19:53 CST 2018/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s): OK0. BP-1139418417-192.168.51.4-1514272982687:blk_1073743138_2323 len=133 Live_repl=2 [DatanodeInfoWithStorage[192.168.51.6:9866,DS-482194d9-aa70-4ab8-8253-907739d5b1a1,DISK], DatanodeInfoWithStorage[192.168.51.5:9866,DS-f59711c6-801d-4ebc-b55c-228f225117b8,DISK]]##列出檔案所有的訊息包含Rack位置hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files -blocks -locations -racksConnecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;locations=1&amp;racks=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txtFSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Thu Jan 04 18:43:12 CST 2018/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s): OK0. BP-1139418417-192.168.51.4-1514272982687:blk_1073743138_2323 len=133 Live_repl=2 [/default-rack/192.168.51.5:9866, /default-rack/192.168.51.6:9866]##列出所有完整的replication的位置訊息hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -files -blocks -replicaDetailsConnecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;replicadetails=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txtFSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 for path /user/hadoop/test1.txt at Wed Jan 03 13:21:56 CST 2018/user/hadoop/test1.txt 133 bytes, replicated: replication=2, 1 block(s): OK0. BP-1139418417-192.168.51.4-1514272982687:blk_1073743138_2323 len=133 Live_repl=2 [DatanodeInfoWithStorage[192.168.51.6:9866,DS-482194d9-aa70-4ab8-8253-907739d5b1a1,DISK](LIVE), DatanodeInfoWithStorage[192.168.51.5:9866,DS-f59711c6-801d-4ebc-b55c-228f225117b8,DISK](LIVE)]##查看文件中損壞Blocks的狀況hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -list-corruptfileblocksConnecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;listcorruptfileblocks=1&amp;path=%2Fuser%2Fhadoop%2Ftest1.txtThe filesystem under path &apos;/user/hadoop/test1.txt&apos; has 0 CORRUPT files##列出指定的block的詳細資訊hadoop@hadoop-master:~$ hdfs fsck /user/hadoop/test1.txt -blockId blk_1073743138Connecting to namenode via http://hadoop-master:9870/fsck?ugi=hadoop&amp;blockId=blk_1073743138+&amp;path=%2Fuser%2Fhadoop%2Ftest1.txtFSCK started by hadoop (auth:SIMPLE) from /192.168.51.4 at Wed Jan 03 13:34:40 CST 2018Block Id: blk_1073743138Block belongs to: /user/hadoop/test1.txtNo. of Expected Replica: 2No. of live Replica: 2No. of excess Replica: 0No. of stale Replica: 0No. of decommissioned Replica: 0No. of decommissioning Replica: 0No. of corrupted Replica: 0Block replica on datanode/rack: hadoop-slave1/default-rack is HEALTHYBlock replica on datanode/rack: hadoop-slave2/default-rack is HEALTHY","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"hdfs dfsadmin command Memo","slug":"[hadoop]hdfs_dfsadmin_cmd","date":"2018-01-02T16:00:00.000Z","updated":"2020-07-22T08:31:39.885Z","comments":true,"path":"2018/01/03/[hadoop]hdfs_dfsadmin_cmd/","link":"","permalink":"http://yoursite.com/2018/01/03/[hadoop]hdfs_dfsadmin_cmd/","excerpt":"","text":"hdfs dfsadmin相關語法 1234567891011121314151617181920212223242526272829303132333435Note: Administrative commands can only be run as the HDFS superuser. [-report [-live] [-dead] [-decommissioning] [-enteringmaintenance] [-inmaintenance]] [-safemode &lt;enter | leave | get | wait&gt;] [-saveNamespace [-beforeShutdown]] [-rollEdits] [-restoreFailedStorage true|false|check] [-refreshNodes] [-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;] [-clrQuota &lt;dirname&gt;...&lt;dirname&gt;] [-setSpaceQuota &lt;quota&gt; [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;] [-clrSpaceQuota [-storageType &lt;storagetype&gt;] &lt;dirname&gt;...&lt;dirname&gt;] [-finalizeUpgrade] [-rollingUpgrade [&lt;query|prepare|finalize&gt;]] [-refreshServiceAcl] [-refreshUserToGroupsMappings] [-refreshSuperUserGroupsConfiguration] [-refreshCallQueue] [-refresh &lt;host:ipc_port&gt; &lt;key&gt; [arg1..argn] [-reconfig &lt;namenode|datanode&gt; &lt;host:ipc_port&gt; &lt;start|status|properties&gt;] [-printTopology] [-refreshNamenodes datanode_host:ipc_port] [-getVolumeReport datanode_host:ipc_port] [-deleteBlockPool datanode_host:ipc_port blockpoolId [force]] [-setBalancerBandwidth &lt;bandwidth in bytes per second&gt;] [-getBalancerBandwidth &lt;datanode_host:ipc_port&gt;] [-fetchImage &lt;local directory&gt;] [-allowSnapshot &lt;snapshotDir&gt;] [-disallowSnapshot &lt;snapshotDir&gt;] [-shutdownDatanode &lt;datanode_host:ipc_port&gt; [upgrade]] [-evictWriters &lt;datanode_host:ipc_port&gt;] [-getDatanodeInfo &lt;datanode_host:ipc_port&gt;] [-metasave filename] [-triggerBlockReport [-incremental] &lt;datanode_host:ipc_port&gt;] [-listOpenFiles] [-help [cmd]] hdfs dfsadmin -report 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657hadoop@hadoop-master:~$ hdfs dfsadmin -reportConfigured Capacity: 42002972672 (39.12 GB)Present Capacity: 26379444224 (24.57 GB)DFS Remaining: 26331783168 (24.52 GB)DFS Used: 47661056 (45.45 MB)DFS Used%: 0.18%Replicated Blocks: Under replicated blocks: 967 Blocks with corrupt replicas: 0 Missing blocks: 0 Missing blocks (with replication factor 1): 0 Pending deletion blocks: 0Erasure Coded Block Groups: Low redundancy block groups: 0 Block groups with corrupt internal blocks: 0 Missing block groups: 0 Pending deletion blocks: 0-------------------------------------------------Live datanodes (2):Name: 192.168.51.5:9866 (hadoop-slave1)Hostname: hadoop-slave1Decommission Status : NormalConfigured Capacity: 21001486336 (19.56 GB)DFS Used: 23842816 (22.74 MB)Non DFS Used: 6787604480 (6.32 GB)DFS Remaining: 13099626496 (12.20 GB)DFS Used%: 0.11%DFS Remaining%: 62.37%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Wed Jan 03 17:52:32 CST 2018Last Block Report: Wed Jan 03 15:42:49 CST 2018Name: 192.168.51.6:9866 (hadoop-slave2)Hostname: hadoop-slave2Decommission Status : NormalConfigured Capacity: 21001486336 (19.56 GB)DFS Used: 23818240 (22.71 MB)Non DFS Used: 6655098880 (6.20 GB)DFS Remaining: 13232156672 (12.32 GB)DFS Used%: 0.11%DFS Remaining%: 63.01%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Wed Jan 03 17:52:33 CST 2018Last Block Report: Wed Jan 03 16:33:35 CST 2018 hdfs dfsadmin -printTopology 1234hadoop@hadoop-master:~$ hdfs dfsadmin -printTopologyRack: /default-rack 192.168.51.5:9866 (hadoop-slave1) 192.168.51.6:9866 (hadoop-slave2) hdfs dfsadmin -getDatanodeInfo 12345hadoop@hadoop-master:~$ hdfs dfsadmin -getDatanodeInfo 192.168.51.6:9867Uptime: 5100, Software version: 3.0.0, Config version: core-3.0.0,hdfs-1hadoop@hadoop-master:~$ hdfs dfsadmin -getDatanodeInfo 192.168.51.5:9867Uptime: 10709, Software version: 3.0.0, Config version: core-3.0.0,hdfs-1 hdfs dfsadmin -safemode 1234567891011121314151617181920212223242526272829303132333435hadoop@hadoop-master:~$ hdfs dfsadmin -safemodeUsage: hdfs dfsadmin [-safemode enter | leave | get | wait | forceExit]##取得safemode模式hadoop@hadoop-master:~$ hdfs dfsadmin -safemode getSafe mode is OFFhadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/snapshot/test3.txt33334444hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/snapshotFound 3 items-rw-r--r-- 1 hadoop supergroup 29 2018-01-03 16:53 /user/hadoop/snapshot/test1.txt-rw-r--r-- 1 hadoop supergroup 7 2018-01-03 17:36 /user/hadoop/snapshot/test2.txt-rw-r--r-- 1 hadoop supergroup 10 2018-01-03 18:04 /user/hadoop/snapshot/test3.txt##進入safemode模式hadoop@hadoop-master:~$ hdfs dfsadmin -safemode enterSafe mode is ONhadoop@hadoop-master:~$ hdfs dfsadmin -safemode getSafe mode is ONhadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/snapshot/test4.txtput: Cannot create file/user/hadoop/snapshot/test4.txt._COPYING_. Name node is in safe mode.##離開safemode模式hadoop@hadoop-master:~$ hdfs dfsadmin -safemode leaveSafe mode is OFFhadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/snapshot/test4.txt5555566666hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/snapshotFound 4 items-rw-r--r-- 1 hadoop supergroup 29 2018-01-03 16:53 /user/hadoop/snapshot/test1.txt-rw-r--r-- 1 hadoop supergroup 7 2018-01-03 17:36 /user/hadoop/snapshot/test2.txt-rw-r--r-- 1 hadoop supergroup 10 2018-01-03 18:04 /user/hadoop/snapshot/test3.txt-rw-r--r-- 1 hadoop supergroup 12 2018-01-03 18:05 /user/hadoop/snapshot/test4.txt hdfs dfsadmin -shutdownDatanode 1234567891011121314151617181920212223242526272829303132333435363738394041424344##停掉 Datanodehadoop@hadoop-master:~$ hdfs dfsadmin -shutdownDatanode hadoop-slave2:9867Submitted a shutdown request to datanode hadoop-slave2:9867##觀察 Datanode Statushadoop@hadoop-master:~$ hdfs dfsadmin -report -deadSafe mode is ONConfigured Capacity: 21001486336 (19.56 GB)Present Capacity: 13123469312 (12.22 GB)DFS Remaining: 13099626496 (12.20 GB)DFS Used: 23842816 (22.74 MB)DFS Used%: 0.18%Replicated Blocks: Under replicated blocks: 971 Blocks with corrupt replicas: 0 Missing blocks: 2 Missing blocks (with replication factor 1): 2 Pending deletion blocks: 0Erasure Coded Block Groups: Low redundancy block groups: 1 Block groups with corrupt internal blocks: 0 Missing block groups: 0 Pending deletion blocks: 0-------------------------------------------------Dead datanodes (1):Name: 192.168.51.6:9866 (hadoop-slave2)Hostname: hadoop-slave2Decommission Status : NormalConfigured Capacity: 21001486336 (19.56 GB)DFS Used: 23834624 (22.73 MB)Non DFS Used: 6655102976 (6.20 GB)DFS Remaining: 13232136192 (12.32 GB)DFS Used%: 0.11%DFS Remaining%: 63.01%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 0Last contact: Wed Jan 03 18:16:00 CST 2018Last Block Report: Wed Jan 03 16:33:35 CST 2018 hdfs dfsadmin -metasave 123## 產生所有metadata資訊,產生後會存放於hdfs系統的Log資料夾內hadoop@hadoop-master:~$ hdfs dfsadmin -metasave metasave.txtCreated metasave file metasave.txt in the log directory of namenode hdfs://hadoop-master:8020 hdfs dfsadmin -listOpenFiles 12hadoop@hadoop-master:~$ hdfs dfsadmin -listOpenFilesClient Host Client Name Open File Path hdfs dfsadmin -triggerBlockReport 12hadoop@hadoop-master:~$ hdfs dfsadmin -triggerBlockReport 192.168.51.5:9867Triggering a full block report on 192.168.51.5:9867. hdfs dfsadmin BalancerBandwidth 12345678910111213##取得Namenode與各Datanode之間的IPC頻寬(目前設定為10M bytes/s)hadoop@hadoop-master:~$ hdfs dfsadmin -getBalancerBandwidth hadoop-slave1:9867Balancer bandwidth is 10485760 bytes per second.hadoop@hadoop-master:~$ hdfs dfsadmin -getBalancerBandwidth hadoop-slave2:9867Balancer bandwidth is 10485760 bytes per second.##設定Namenode與Datanode之間的頻寬大小hadoop@hadoop-master:~$ hdfs dfsadmin -setBalancerBandwidth 15MBalancer bandwidth is set to 15728640hadoop@hadoop-master:~$ hdfs dfsadmin -getBalancerBandwidth hadoop-slave2:9867Balancer bandwidth is 15728640 bytes per second.hadoop@hadoop-master:~$ hdfs dfsadmin -getBalancerBandwidth hadoop-slave1:9867Balancer bandwidth is 15728640 bytes per second.","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"Hadoop權限操作說明","slug":"[hadoop]Hadoop_permission_memo","date":"2018-01-01T16:00:00.000Z","updated":"2020-07-22T08:28:09.538Z","comments":true,"path":"2018/01/02/[hadoop]Hadoop_permission_memo/","link":"","permalink":"http://yoursite.com/2018/01/02/[hadoop]Hadoop_permission_memo/","excerpt":"","text":"使用到的HDFS語法整理 1234567hadoop fs -ls -d /user/hadoophadoop fs -put ~/test1.txt /user/hadoophadoop fs -chmod -R 757 /user/hadoophadoop fs -ls /user/hadoophadoop fs -chown hadoop /user/hadoop/test1.txthadoop fs -appendToFile - /user/hadoop/test1.txthadoop fs -cat /user/hadoop/test1.txt 以下為操作驗證使用hadoop語法查看目錄權限 12hadoop@hadoop-master:~$hadoop fs -ls -d /user/hadoopdrwxr-xr-x - hadoop supergroup 0 2017-12-27 18:39 /user/hadoop 使用putty(以hadoop帳號)登入,hadoop-master主機建立一個新帳號 123hadoop@hadoop-master:~$sudo useradd -m popalhadoop@hadoop-master:~$ls -la /homehadoop@hadoop-master:~$sudo passwd popal 建立一個文字檔 12hadoop@hadoop-master:~$su - popalpopal@hadoop-master:~$vi ~/test1.txt 使用hadoop語法,上傳至HDFS時,出現錯誤訊息 12popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -put ~/test1.txt /user/hadoopError:&quot;put: Permission denied: user=popal, access=WRITE, inode=&quot;/user/hadoop&quot;:hadoop:supergroup:drwxr-xr-x&quot; 將/user/hadoop權限變更 1234popal@hadoop-master:~$su - hadoophadoop@hadoop-master:~$hadoop fs -chmod -R 757 /user/hadoophadoop@hadoop-master:~$hadoop fs -ls -d /user/hadoopdrwxr-xrwx - hadoop supergroup 0 2017-12-27 18:39 /user/hadoop 使用popal帳號再傳送一次檔案 123hadoop@hadoop-master:~$su - popalpopal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -put ~/test1.txt /user/hadoop(沒有任何錯誤訊息,並傳送成功) 觀察檔案是否有上傳至HDFS 12popal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -ls /user/hadoop-rw-r--r-- 3 popal supergroup 95 2018-01-02 10:43 /user/hadoop/test1.txt 將原本權限復原 123popal@hadoop-master:~$su - hadoophadoop@hadoop-master:~$hadoop fs -chmod -R 755 /user/hadoophadoop@hadoop-master:~$hadoop fs -ls /user/hadoop 將test1.txt的擁有者變更成hadoop 1hadoop@hadoop-master:~$hadoop fs -chown hadoop /user/hadoop/test1.txt 使用popal使用者變更test1.txt內容 123hadoop@hadoop-master:~$su - popalpopal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -appendToFile - /user/hadoop/test1.txtError&quot;appendToFile: Permission denied: user=popal, access=WRITE, inode=&quot;/user/hadoop/test1.txt&quot;:hadoop:supergroup:-rwxr-xr-x&quot; 將/user/hadoop/test1.txt的擁有者變更 12popal@hadoop-master:~$su - hadoophadoop@hadoop-master:~$hadoop fs -chown popal /user/hadoop/test1.txt 使用popal的帳號再變更/user/hadoop/test1.txt 123hadoop@hadoop-master:~$su - popalpopal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -appendToFile - /user/hadoop/test1.txtpopal@hadoop-master:~$/bgdt/hadoop-3.0.0/bin/hadoop fs -cat /user/hadoop/test1.txt","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"hadoop fs command Memo","slug":"[hadoop]hadoop_fs_cmd","date":"2018-01-01T16:00:00.000Z","updated":"2020-07-22T08:27:28.729Z","comments":true,"path":"2018/01/02/[hadoop]hadoop_fs_cmd/","link":"","permalink":"http://yoursite.com/2018/01/02/[hadoop]hadoop_fs_cmd/","excerpt":"","text":"建立與刪除目錄 1234567891011121314151617181920212223##建立目錄但不會將上層目錄一併建立完成hadoop@hadoop-master:~$hadoop fs -mkdir /user/hadoop/example/lab_1mkdir: `/user/hadoop/example/lab_1&apos;: No such file or directory##建立目錄,會連同上層目錄一併建立hadoop@hadoop-master:~$hadoop fs -mkdir -p /user/hadoop/example/lab_1hadoop@hadoop-master:~$hadoop fs ls -d /user/hadoop/example##刪除一個空目錄hadoop@hadoop-master:~$hadoop fs -rmdir /user/hadoop/example/lab_1##透過std in 建立一個test1.txt檔案hadoop@hadoop-master:~$hadoop fs -put -f - /user/hadoop/example/test1.txt##無法刪除,目錄下有檔案hadoop@hadoop-master:~$hadoop fs -rmdir /user/hadoop/example&quot;rmdir: `/user/hadoop/example&apos;: Directory is not empty&quot;##將所有檔案刪除完成後,刪除目錄hadoop@hadoop-master:~$hadoop fs -rm -r -f /user/hadoop/example ##建立目錄hadoop@hadoop-master:~$hadoop fs -mkdir -p /user/hadoop/example/lab_1 檔案建立/複製/刪除/搬移put/get操作檔案 12345678910111213141516171819202122232425262728293031323334353637hadoop@hadoop-master:~$rm -rf ~/get_hdfs_test2.txthadoop@hadoop-master:~$dd if=/dev/urandom of=test2.txt bs=1K count=11+0 records in1+0 records out1024 bytes (1.0 kB, 1.0 KiB) copied, 0.00234966 s, 436 kB/s##使用PUT上傳檔案到HDFShadoop@hadoop-master:~$hadoop fs -put ~/test2.txt /user/hadoop/example/hadoop@hadoop-master:~$hadoop fs -ls /user/hadoop/exampleFound 2 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-02 15:40 /user/hadoop/example/lab_1-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:40 /user/hadoop/example/test2.txt##使用GET下載檔案到Localhadoop@hadoop-master:~$ hadoop fs -get /user/hadoop/example/test2.txt ~/get_hdfs_test2.txthadoop@hadoop-master:~$ ls -la ~/get*-rw-r--r-- 1 hadoop hadoop 1024 Jan 2 15:44 /home/hadoop/get_hdfs_test2.txt##使用getmerge結合數個檔案內容,並下載至Localhadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/lab_1/test1.txt1,&quot;test_001&quot;,100002,&quot;test_002&quot;,20000hadoop@hadoop-master:~$ hadoop fs -put - /user/hadoop/example/lab_1/test2.txt3,&quot;test_003&quot;,300004,&quot;test_004&quot;,40000hadoop@hadoop-master:~$ rm -rf ~/merge_file.txthadoop@hadoop-master:~$ hadoop fs -getmerge -nl /user/hadoop/example/lab_1/* ~/merge_file.txthadoop@hadoop-master:~$ ls -la ~/merge_file.txt-rw-r--r-- 1 hadoop hadoop 78 Jan 2 15:53 /home/hadoop/merge_file.txthadoop@hadoop-master:~$ cat ~/merge_file.txt1,&quot;test_001&quot;,100002,&quot;test_002&quot;,200003,&quot;test_003&quot;,300004,&quot;test_004&quot;,40000 copy/move操作檔案 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121## copyfromlocal/copytolocal操作hadoop@hadoop-master:~$ dd if=/dev/urandom of=test3.txt bs=1K count=11+0 records in1+0 records out1024 bytes (1.0 kB, 1.0 KiB) copied, 0.00109903 s, 932 kB/shadoop@hadoop-master:~$ rm -rf ~/cp_hdfs_tolocal_test3.txthadoop@hadoop-master:~$ hadoop fs -copyFromLocal ~/test3.txt /user/hadoop/example/lab_1hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1Found 3 items-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txthadoop@hadoop-master:~$ hadoop fs -copyToLocal /user/hadoop/example/lab_1/test3.txt ~/cp_hdfs_tolocal_test3.txthadoop@hadoop-master:~$ ls -la ~/cp_hdfs_tolocal_test3.txt-rw-r--r-- 1 hadoop hadoop 1024 Jan 2 16:01 /home/hadoop/cp_hdfs_tolocal_test3.txthadoop@hadoop-master:~$## cp操作(同一個HDFS中資料複製)hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1Found 3 items-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txthadoop@hadoop-master:~$ hadoop fs -cp /user/hadoop/example/test2.txt /user/hadoop/example/lab_1/test4.txthadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1Found 4 items-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txthadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/exampleFound 2 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-02 16:14 /user/hadoop/example/lab_1-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:40 /user/hadoop/example/test2.txt## cp操作(不同HDFS中資料複製)hadoop@hadoop-master:~$ hadoop fs -cp hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test5.txthadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1Found 5 items-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txthadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test5.txtid,end_date,start_date,location1,2015-10-14 00:00:00,2015-09-14 00:00:00,CA-SF2,2015-10-15 01:00:20,2015-08-14 00:00:00,CA-SD3,2015-10-16 02:30:00,2015-01-14 00:00:00,NY-NY4,2015-10-17 03:00:20,2015-02-14 00:00:00,NY-NY5,2015-10-18 04:30:00,2014-04-14 00:00:00,CA-SDhadoop@hadoop-master:~$ hadoop fs -ls hdfs://172.20.22.95:8020/user/hadoop/tmpFound 1 items-rw-r--r-- 2 hadoop supergroup 272 2017-11-22 17:09 hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv## moveFromLocal/moveToLocalhadoop@hadoop-master:~$ dd if=/dev/urandom of=test6.txt bs=1K count=11+0 records in1+0 records out1024 bytes (1.0 kB, 1.0 KiB) copied, 0.00116886 s, 876 kB/shadoop@hadoop-master:~$ hadoop fs -moveFromLocal ~/test6.txt /user/hadoop/example/lab_1hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1Found 6 items-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txthadoop@hadoop-master:~$ hadoop fs -moveToLocal /user/hadoop/example/test6.txt ~/mv_hdfs_tolocal_test6.txtmoveToLocal: Option &apos;-moveToLocal&apos; is not implemented yet.## mv操作(同一個HDFS中資料複製),被搬移之後檔案會不見hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1Found 6 items-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txthadoop@hadoop-master:~$ hadoop fs -mv /user/hadoop/example/test2.txt /user/hadoop/example/lab_1/test7.txthadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1Found 7 items-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txthadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/exampleFound 1 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-02 16:21 /user/hadoop/example/lab_1## mv操作(不同一個HDFS中資料複製),如果hadoop版本不一致無法使用mv搬移檔案hadoop@hadoop-master:~$ hadoop fs -ls hdfs://172.20.22.95:8020/user/hadoop/tmpFound 1 items-rw-r--r-- 2 hadoop supergroup 272 2017-11-22 17:09 hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csvhadoop@hadoop-master:~$ hadoop fs -ls hdfs://192.168.51.4:8020/user/hadoop/example/lab_1Found 7 items-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:50 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:51 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:59 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test3.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:07 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test4.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:11 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test5.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:14 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test6.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:40 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test7.txthadoop@hadoop-master:~$ hadoop fs -mv hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test8.txtmv: `hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv&apos;: Does not match target filesystemhadoop@hadoop-master:~$ hadoop fs -cp hdfs://172.20.22.95:8020/user/hadoop/tmp/test2.csv hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test8.txthadoop@hadoop-master:~$ hadoop fs -ls hdfs://192.168.51.4:8020/user/hadoop/example/lab_1Found 8 items-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:50 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:51 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:59 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test3.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:07 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test4.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:11 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test5.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:14 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test6.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:40 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test7.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:31 hdfs://192.168.51.4:8020/user/hadoop/example/lab_1/test8.txt 查看檔案內容檢視壓縮檔案內容(.tar.gz) 12345678910111213141516171819202122232425262728293031323334353637383940hadoop@hadoop-master:~$ tar -zcvf ~/test9.tar.gz &quot;test1.csv&quot; &quot;test2.csv&quot; &quot;test3.csv&quot;test1.csvtest2.csvtest3.csvhadoop@hadoop-master:~$ tar -ztvf ~/test9.tar.gz-rw-rw-r-- hadoop/hadoop 272 2017-11-08 15:49 test1.csv-rw-rw-r-- hadoop/hadoop 300 2017-11-06 19:14 test2.csv-rw-rw-r-- hadoop/hadoop 422 2017-08-21 16:30 test3.csvhadoop@hadoop-master:~$ hadoop fs -put ~/test9.tar.gz /user/hadoop/example/lab_1/hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1Found 9 items-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:50 /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt-rw-r--r-- 2 hadoop supergroup 373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz##可以使用hadoop fs -text來查看壓縮檔案內容hadoop@hadoop-master:~$ hadoop fs -text /user/hadoop/example/lab_1/test9.tar.gztest1.csv0000664000175100017510000000042013200533434012136 0ustar hadoophadoop100,1,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.1&quot;200,2,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.2&quot;150,3,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.3&quot;300,4,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.4&quot;..........5,&quot;http://www.google.com&quot;;&quot;192.168.1.5&quot;;&quot;201&quot;,&quot;005&quot;;&quot;test1_005&quot;;&quot;500&quot;6,&quot;http://www.google.com&quot;;&quot;192.168.1.6&quot;;&quot;403&quot;,&quot;006&quot;;&quot;test1_006&quot;;&quot;1600&quot;##hadoop fs -cat查看壓縮檔時,會出現亂碼,需要配合zcat指令來查看hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test9.tar.gz | zcattest1.csv0000664000175100017510000000042013200533434012136 0ustar hadoophadoop100,1,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.1&quot;200,2,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.2&quot;150,3,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.3&quot;300,4,&quot;http://www.google.com&quot;,&quot;http://www.google.com&quot;,&quot;192.168.1.4&quot;..........5,&quot;http://www.google.com&quot;;&quot;192.168.1.5&quot;;&quot;201&quot;,&quot;005&quot;;&quot;test1_005&quot;;&quot;500&quot;6,&quot;http://www.google.com&quot;;&quot;192.168.1.6&quot;;&quot;403&quot;,&quot;006&quot;;&quot;test1_006&quot;;&quot;1600&quot; 檢視檔案內容,且資料append到檔案時,會馬上呈現 1234##hadoop fs -tail指令的功能(與cat/text相同可以觀察檔案內容)hadoop@hadoop-master:~$ hadoop fs -tail /user/hadoop/example/lab_1/test1.txt1,&quot;test_001&quot;,100002,&quot;test_002&quot;,20000 以下使用圖片說明 hadoop fs -tail時,可以查看檔案完整內容hadoop fs -tail -f 時,持續呈現檔案Append的相關內容 建立空檔案vs.清空檔案內容 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647##使用 -touchz建立一個空的檔案,常用於建立時戳或者flag檔案hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1Found 9 items-rw-r--r-- 2 hadoop supergroup 76 2018-01-02 17:28 /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt-rw-r--r-- 2 hadoop supergroup 373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gzhadoop@hadoop-master:~$ hadoop fs -touchz /user/hadoop/example/lab_1/test10.txthadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1Found 10 items-rw-r--r-- 2 hadoop supergroup 76 2018-01-02 17:28 /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt-rw-r--r-- 2 hadoop supergroup 373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gzhadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test1.txt1,&quot;test_001&quot;,100002,&quot;test_002&quot;,200003,&quot;test_003&quot;,300004,&quot;test_004&quot;,40000## -truncate -w 10,只留10 bytes資料hadoop@hadoop-master:~$ hadoop fs -truncate -w 10 /user/hadoop/example/lab_1/test1.txtWaiting for /user/hadoop/example/lab_1/test1.txt ...Truncated /user/hadoop/example/lab_1/test1.txt to length: 10hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test1.txt1,&quot;test_00hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 10 2018-01-02 17:42 /user/hadoop/example/lab_1/test1.txt## -truncate -w 0,資料全部清空hadoop@hadoop-master:~$ hadoop fs -truncate -w 0 /user/hadoop/example/lab_1/test1.txtTruncated /user/hadoop/example/lab_1/test1.txt to length: 0hadoop@hadoop-master:~$ hadoop fs -cat /user/hadoop/example/lab_1/test1.txthadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt 使用hadoop fs -find找檔案 此部分可以參考Linux find指令1234567891011121314151617181920212223242526說明:-iname pattern 所要查找的文檔名，不區分大小寫-name pattern 所要查找的文檔名，區分大小寫-print 換行列印-print0 連續列印##-name後,可接欲找尋的檔案名稱,&quot;*&quot;代表任何值hadoop@hadoop-master:~$ hadoop fs -find / -name &quot;test*.txt&quot; -print/user/hadoop/example/lab_1/test1.txt/user/hadoop/example/lab_1/test10.txt/user/hadoop/example/lab_1/test2.txt/user/hadoop/example/lab_1/test3.txt/user/hadoop/example/lab_1/test4.txt/user/hadoop/example/lab_1/test5.txt/user/hadoop/example/lab_1/test6.txt/user/hadoop/example/lab_1/test7.txt/user/hadoop/example/lab_1/test8.txt/user/hadoop/test1.txt##-name後,可接正規式的相關參數hadoop@hadoop-master:~$ hadoop fs -find / -name &quot;[A-Z,a-z]*.db&quot; -print/user/hive/warehouse/test1.db/user/hive/warehouse/test2.db/user/hive/warehouse/test3.db/user/hive/warehouse/test4.db/user/hive/warehouse/test5.db/user/hive/warehouse/test6.db 使用hadoop fs -df/-du查看檔案目錄的大小 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849##後面如果都沒接任何參數,看各個檔案的大小(以byte為單位)及檔名hadoop@hadoop-master:~$ hadoop fs -du /user/hadoop/example/lab_10 0 /user/hadoop/example/lab_1/test1.txt0 0 /user/hadoop/example/lab_1/test10.txt38 76 /user/hadoop/example/lab_1/test2.txt1024 2048 /user/hadoop/example/lab_1/test3.txt1024 2048 /user/hadoop/example/lab_1/test4.txt272 544 /user/hadoop/example/lab_1/test5.txt1024 2048 /user/hadoop/example/lab_1/test6.txt1024 2048 /user/hadoop/example/lab_1/test7.txt272 544 /user/hadoop/example/lab_1/test8.txt373 746 /user/hadoop/example/lab_1/test9.tar.gz##加入&quot;-h&quot;,在顯示檔案大小時,會將單位標示出來hadoop@hadoop-master:~$ hadoop fs -du -h /user/hadoop/example/lab_10 0 /user/hadoop/example/lab_1/test1.txt0 0 /user/hadoop/example/lab_1/test10.txt38 76 /user/hadoop/example/lab_1/test2.txt1 K 2 K /user/hadoop/example/lab_1/test3.txt1 K 2 K /user/hadoop/example/lab_1/test4.txt272 544 /user/hadoop/example/lab_1/test5.txt1 K 2 K /user/hadoop/example/lab_1/test6.txt1 K 2 K /user/hadoop/example/lab_1/test7.txt272 544 /user/hadoop/example/lab_1/test8.txt373 746 /user/hadoop/example/lab_1/test9.tar.gz##加入&quot;-s&quot;,最主要是用來顯示該目錄下所有檔案的summary後的大小hadoop@hadoop-master:~$ hadoop fs -du -h -s /user/hadoop/example/lab_14.9 K 9.9 K /user/hadoop/example/lab_1##hadoop fs -du 加入&quot;-v&quot;,可以看到每個欄位的標題hadoop@hadoop-master:~$ hadoop fs -du -h -v /user/hadoop/example/lab_1SIZE DISK_SPACE_CONSUMED_WITH_ALL_REPLICAS FULL_PATH_NAME0 0 /user/hadoop/example/lab_1/test1.txt0 0 /user/hadoop/example/lab_1/test10.txt38 76 /user/hadoop/example/lab_1/test2.txt1 K 2 K /user/hadoop/example/lab_1/test3.txt1 K 2 K /user/hadoop/example/lab_1/test4.txt272 544 /user/hadoop/example/lab_1/test5.txt1 K 2 K /user/hadoop/example/lab_1/test6.txt1 K 2 K /user/hadoop/example/lab_1/test7.txt272 544 /user/hadoop/example/lab_1/test8.txt373 746 /user/hadoop/example/lab_1/test9.tar.gz## hadoop fs -df 顯示HDFS系統(多個)剩餘空間hadoop@hadoop-master:~$ hadoop fs -df hdfs://172.20.22.95:8020 hdfs://192.168.51.4:8020Filesystem Size Used Available Use%hdfs://172.20.22.95:8020 2232839094272 29245636608 1911548899328 1%hdfs://192.168.51.4:8020 42002972672 47734784 26333745152 0% 使用hadoop fs -ls查看檔案目錄相關資訊 1234567891011121314151617181920212223242526272829303132333435363738394041424344##hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/exampleFound 2 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-02 17:38 /user/hadoop/example/lab_1-rw-r--r-- 2 hadoop supergroup 36 2018-01-02 18:29 /user/hadoop/example/test.txt##-r Reverse the order of the sorthadoop@hadoop-master:~$ hadoop fs -ls -r /user/hadoop/exampleFound 2 items-rw-r--r-- 2 hadoop supergroup 36 2018-01-02 18:29 /user/hadoop/example/test.txtdrwxr-xr-x - hadoop supergroup 0 2018-01-02 17:38 /user/hadoop/example/lab_1## 檢視&quot;-R&quot;遞回目錄下所有檔案及目錄hadoop@hadoop-master:~$ hadoop fs -ls -R /user/hadoop/exampledrwxr-xr-x - hadoop supergroup 0 2018-01-02 17:38 /user/hadoop/example/lab_1-rw-r--r-- 2 hadoop supergroup 0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt-rw-r--r-- 2 hadoop supergroup 373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz-rw-r--r-- 2 hadoop supergroup 36 2018-01-02 18:29 /user/hadoop/example/test.txt## &quot;-d&quot;只顯示目前只定的檔案或目錄hadoop fs -ls -d /user/hadoop/exampledrwxr-xr-x - hadoop supergroup 0 2018-01-02 18:29 /user/hadoop/example## &quot;-e&quot; Display the erasure coding policy of files and directories.hadoop@hadoop-master:~$ hadoop fs -ls -e /user/hadoop/example/lab_1Found 10 items-rw-r--r-- 2 hadoop supergroup Replicated 0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup Replicated 0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt-rw-r--r-- 2 hadoop supergroup Replicated 38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup Replicated 1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt-rw-r--r-- 2 hadoop supergroup Replicated 1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt-rw-r--r-- 2 hadoop supergroup Replicated 272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt-rw-r--r-- 2 hadoop supergroup Replicated 1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt-rw-r--r-- 2 hadoop supergroup Replicated 1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt-rw-r--r-- 2 hadoop supergroup Replicated 272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt-rw-r--r-- 2 hadoop supergroup Replicated 373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz 使用hadoop fs -count查看檔案目錄相關資訊 1234567891011121314151617181920##計算指定的目錄或檔案的數量大小hadoop@hadoop-master:~$ hadoop fs -count &quot;/user/hadoop/example/*&quot; 1 10 5051 /user/hadoop/example/lab_1 0 1 36 /user/hadoop/example/test.txt## &quot;-q&quot;選項會多出以下欄位## QUOTA,REM_QUOTA,SPACE_QUOTA,REM_SPACE_QUOTA,DIR_COUNT,FILE_COUNT,CONTENT_SIZE,PATHNAME hadoop@hadoop-master:~$ hadoop fs -count -q &quot;/user/hadoop/example/*&quot; none inf none inf 1 10 5051 /user/hadoop/example/lab_1 none inf none inf 0 1 36 /user/hadoop/example/test.txt## &quot;-u&quot; option shows the quota and the usage against the quota without the detailed content summaryhadoop@hadoop-master:~$ hadoop fs -count -u &quot;/user/hadoop/example/*&quot; none inf none inf /user/hadoop/example/lab_1 none inf none inf /user/hadoop/example/test.txt## &quot;-e&quot; option shows the erasure coding policy.hadoop@hadoop-master:~$ hadoop fs -count -e &quot;/user/hadoop/example/*&quot; 1 10 5051 EC: /user/hadoop/example/lab_1 0 1 36 Replicated /user/hadoop/example/test.txt 使用hadoop fs -stat查看檔案目錄相關資訊 12345678910111213141516171819202122232425262728##相關-stat Format說明%a: octal%A: symbolic%b: filesize in bytes%F: type%g: group name of owner%n: name%o: block size%r: replication%u: user name of owner%x,%X :access date%y,%Y :modification datehadoop@hadoop-master:~$ hadoop fs -stat /user/*2018-01-02 09:05:222017-12-26 10:44:41hadoop@hadoop-master:~$ hadoop fs -stat &quot;%A,perm:%a,type:%F,%u:%g,size:%b,block size:%o,%r,mtime:%y atime:%x name:%n&quot; &quot;/user/hadoop/example/lab_1/*&quot;rw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:0,block size:67108864,2,mtime:2018-01-02 09:45:22 atime:2018-01-02 09:18:30 name:test1.txtrw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:0,block size:67108864,2,mtime:2018-01-02 09:38:21 atime:2018-01-02 09:38:21 name:test10.txtrw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:38,block size:67108864,2,mtime:2018-01-02 07:51:41 atime:2018-01-02 07:51:16 name:test2.txtrw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:1024,block size:67108864,2,mtime:2018-01-02 07:59:28 atime:2018-01-02 07:59:28 name:test3.txtrw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:1024,block size:67108864,2,mtime:2018-01-02 08:07:12 atime:2018-01-02 08:07:12 name:test4.txtrw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:272,block size:67108864,2,mtime:2018-01-02 08:11:21 atime:2018-01-02 08:11:21 name:test5.txtrw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:1024,block size:67108864,2,mtime:2018-01-02 08:14:24 atime:2018-01-02 08:14:23 name:test6.txtrw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:1024,block size:67108864,2,mtime:2018-01-02 07:40:46 atime:2018-01-02 07:40:46 name:test7.txtrw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:272,block size:67108864,2,mtime:2018-01-02 08:31:28 atime:2018-01-02 08:31:28 name:test8.txtrw-r--r--,perm:644,type:regular file,hadoop:supergroup,size:373,block size:67108864,2,mtime:2018-01-02 09:05:38 atime:2018-01-02 09:05:37 name:test9.tar.gz 使用hadoop fs -test檔案測試 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101##相關參數說明:-d return 0 if &lt;path&gt; is a directory.-e return 0 if &lt;path&gt; exists.-f return 0 if &lt;path&gt; is a file.-s return 0 if file &lt;path&gt; is greater than zero bytes in size.-w return 0 if file &lt;path&gt; exists and write permission is granted.-r return 0 if file &lt;path&gt; exists and read permission is granted.-z return 0 if file &lt;path&gt; is zero bytes in size, else return 1.---## &quot;-e&quot; 判斷指定的路徑是否存在(0:是,1:否)hadoop@hadoop-master:~$ hadoop fs -test -e /user/hadoophadoop@hadoop-master:~$ echo $?0hadoop@hadoop-master:~$ hadoop fs -test -e /user/vagranthadoop@hadoop-master:~$ echo $?1## &quot;-d&quot; 判斷是否為目錄(0:是,1:否)hadoop@hadoop-master:~$ hadoop fs -test -d /user/hadoop/example/lab_1hadoop@hadoop-master:~$ echo $?0hadoop@hadoop-master:~$ hadoop fs -test -d /user/hadoop/example/test.txthadoop@hadoop-master:~$ echo $?1## &quot;-f&quot; 判斷是否為目錄(0:是,1:否)hadoop@hadoop-master:~$ hadoop fs -test -f /user/hadoop/example/lab_1hadoop@hadoop-master:~$ echo $?1hadoop@hadoop-master:~$ hadoop fs -test -f /user/hadoop/example/test.txthadoop@hadoop-master:~$ echo $?0## &quot;-w&quot;判斷檔案是否有寫的權限hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/test.txt-rwxr-xr-x 2 hadoop supergroup 36 2018-01-02 18:29 /user/hadoop/example/test.txthadoop@hadoop-master:~$ hadoop fs -test -w /user/hadoop/example/test.txthadoop@hadoop-master:~$ echo $?0hadoop@hadoop-master:~$ su - popalPassword:popal@hadoop-master:~$ /bgdt/hadoop-3.0.0/bin/hadoop fs -test -w /user/hadoop/example/test.txtpopal@hadoop-master:~$ echo $?1hadoop@hadoop-master:~$ hadoop fs -chmod 766 /user/hadoop/example/test.txthadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/test.txt-rwxrw-rw- 2 hadoop supergroup 36 2018-01-02 18:29 /user/hadoop/example/test.txthadoop@hadoop-master:~$ su - popalPassword:popal@hadoop-master:~$ /bgdt/hadoop-3.0.0/bin/hadoop fs -test -w /user/hadoop/example/test.txtpopal@hadoop-master:~$ echo $?0## &quot;-r&quot;判斷檔案是否有讀的權限hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/test.txt-rwxrw-rw- 2 hadoop supergroup 36 2018-01-02 18:29 /user/hadoop/example/test.txthadoop@hadoop-master:~$ su - popalPassword:popal@hadoop-master:~$ /bgdt/hadoop-3.0.0/bin/hadoop fs -test -r /user/hadoop/example/test.txtpopal@hadoop-master:~$ echo $?0popal@hadoop-master:~$ exitlogouthadoop@hadoop-master:~$ hadoop fs -chmod 700 /user/hadoop/example/test.txthadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/test.txt-rwx------ 2 hadoop supergroup 36 2018-01-02 18:29 /user/hadoop/example/test.txthadoop@hadoop-master:~$ su - popalPassword:popal@hadoop-master:~$ /bgdt/hadoop-3.0.0/bin/hadoop fs -test -r /user/hadoop/example/test.txtpopal@hadoop-master:~$ echo $?1## &quot;-z&quot;判斷檔案是否為0 byteshadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1Found 10 items-rw-r--r-- 2 hadoop supergroup 0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt-rw-r--r-- 2 hadoop supergroup 373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gzhadoop@hadoop-master:~$ hadoop fs -test -z /user/hadoop/example/lab_1/test1.txthadoop@hadoop-master:~$ echo $?0hadoop@hadoop-master:~$ hadoop fs -test -z /user/hadoop/example/lab_1/test2.txthadoop@hadoop-master:~$ echo $?1## &quot;-s&quot;判斷檔案是否不為0 byteshadoop@hadoop-master:~$ hadoop fs -test -s /user/hadoop/example/lab_1/test1.txthadoop@hadoop-master:~$ echo $?1hadoop@hadoop-master:~$ hadoop fs -test -s /user/hadoop/example/lab_1/test2.txthadoop@hadoop-master:~$ echo $?0 使用hadoop fs -setrep 設定檔案複本數 12345678910111213141516171819202122232425262728hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1Found 10 items-rw-r--r-- 2 hadoop supergroup 0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt-rw-r--r-- 2 hadoop supergroup 373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gzhadoop@hadoop-master:~$ hadoop fs -setrep -w 1 /user/hadoop/example/lab_1/test1.txtReplication 1 set: /user/hadoop/example/lab_1/test1.txtWaiting for /user/hadoop/example/lab_1/test1.txt ... donehadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1Found 10 items-rw-r--r-- 1 hadoop supergroup 0 2018-01-02 17:45 /user/hadoop/example/lab_1/test1.txt-rw-r--r-- 2 hadoop supergroup 0 2018-01-02 17:38 /user/hadoop/example/lab_1/test10.txt-rw-r--r-- 2 hadoop supergroup 38 2018-01-02 15:51 /user/hadoop/example/lab_1/test2.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:59 /user/hadoop/example/lab_1/test3.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:07 /user/hadoop/example/lab_1/test4.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:11 /user/hadoop/example/lab_1/test5.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 16:14 /user/hadoop/example/lab_1/test6.txt-rw-r--r-- 2 hadoop supergroup 1024 2018-01-02 15:40 /user/hadoop/example/lab_1/test7.txt-rw-r--r-- 2 hadoop supergroup 272 2018-01-02 16:31 /user/hadoop/example/lab_1/test8.txt-rw-r--r-- 2 hadoop supergroup 373 2018-01-02 17:05 /user/hadoop/example/lab_1/test9.tar.gz 使用hadoop fs -rm刪除檔案 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556## hadoop fs -rm指令,只能刪檔案hadoop@hadoop-master:~$ hadoop fs -rm /user/hadoop/example/lab_1/*Deleted /user/hadoop/example/lab_1/test1.txtDeleted /user/hadoop/example/lab_1/test10.txtDeleted /user/hadoop/example/lab_1/test2.txtDeleted /user/hadoop/example/lab_1/test3.txtDeleted /user/hadoop/example/lab_1/test4.txtDeleted /user/hadoop/example/lab_1/test5.txtDeleted /user/hadoop/example/lab_1/test6.txtDeleted /user/hadoop/example/lab_1/test7.txtDeleted /user/hadoop/example/lab_1/test8.txtDeleted /user/hadoop/example/lab_1/test9.tar.gz## &quot;-rm&quot;無法刪目錄,如要刪除目錄需要用&quot;-rmdir or -rm -r&quot;hadoop fs -rm /user/hadoop/examplerm: `/user/hadoop/example&apos;: Is a directoryhadoop@hadoop-master:~$ hadoop fs -rmdir /user/hadoop/examplermdir: `/user/hadoop/example&apos;: Directory is not empty## &quot;-rmdir&quot;只能刪除空目錄hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1hadoop@hadoop-master:~$ hadoop fs -rmdir /user/hadoop/example/lab_1hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/example/lab_1ls: `/user/hadoop/example/lab_1&apos;: No such file or directory## &quot;-rm -r&quot;,可以刪除所有檔案hadoop@hadoop-master:~$ hadoop fs -mkdir /user/hadoop/example/lab_1hadoop@hadoop-master:~$ hadoop fs -touchz /user/hadoop/example/lab_1/test1.txthadoop@hadoop-master:~$ hadoop fs -rmdir /user/hadoop/example/lab_1rmdir: `/user/hadoop/example/lab_1&apos;: Directory is not emptyhadoop@hadoop-master:~$ hadoop fs -rm /user/hadoop/example/lab_1rm: `/user/hadoop/example/lab_1&apos;: Is a directory## &quot;-rm -r&quot;,會將刪除的檔案或目錄先丟到垃圾桶hadoop@hadoop-master:~$ hadoop fs -rm -r /user/hadoop/example/lab_12018-01-03 11:38:37,066 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://hadoop-master:8020/user/hadoop/example/lab_1&apos; to trash at: hdfs://hadoop-master:8020/user/hadoop/.Trash/Current/user/hadoop/example/lab_1hadoop@hadoop-master:~$ hadoop fs -ls /user/hadoop/exampleFound 1 items-rwx------ 2 hadoop supergroup 36 2018-01-02 18:29 /user/hadoop/example/test.txt##檢查垃圾桶hadoop@hadoop-master:~$ hadoop fs -ls -R /user/hadoop/.Trashdrwx------ - hadoop supergroup 0 2018-01-03 11:38 /user/hadoop/.Trash/Currentdrwx------ - hadoop supergroup 0 2018-01-03 11:38 /user/hadoop/.Trash/Current/userdrwx------ - hadoop supergroup 0 2018-01-03 11:38 /user/hadoop/.Trash/Current/user/hadoopdrwx------ - hadoop supergroup 0 2018-01-03 11:38 /user/hadoop/.Trash/Current/user/hadoop/exampledrwxr-xr-x - hadoop supergroup 0 2018-01-03 11:37 /user/hadoop/.Trash/Current/user/hadoop/example/lab_1-rw-r--r-- 2 hadoop supergroup 0 2018-01-03 11:37 /user/hadoop/.Trash/Current/user/hadoop/example/lab_1/test1.txt## &quot;-rm -r -skipTrash&quot;,會直接刪除目錄或檔案,並且不會進垃圾桶hadoop@hadoop-master:~$ hadoop fs -rm -r -skipTrash /user/hadoop/example/lab_1Deleted /user/hadoop/example/lab_1","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"Spark2.2.1環境安裝說明","slug":"[spark]spark2.2.1_Install","date":"2017-12-28T16:00:00.000Z","updated":"2020-07-22T08:36:20.890Z","comments":true,"path":"2017/12/29/[spark]spark2.2.1_Install/","link":"","permalink":"http://yoursite.com/2017/12/29/[spark]spark2.2.1_Install/","excerpt":"","text":"相關環境說明 IP Address HostName 角色 192.168.51.4 hadoop-master NameNode(NN),SecondaryNameNode, HiveServer2 192.168.51.5 hadoop-slave1 DataNode(DN1) 192.168.51.6 hadoop-slave2 DataNode(DN2) 環境準備Install Linux OS(略)Install Linux JDK8(略)Install Linux Hadoop3.0.0(略)Install MariaDB(略)Install Hive2.3(略)Download Spark2.2.1 tagz file 12wget -P /bgdt http://apache.stu.edu.tw/spark/spark-2.2.1/spark-2.2.1-bin-hadoop2.7.tgzwget -P /bgdt http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.39/mysql-connector-java-5.1.39.jar 解壓縮下載的spark-2.2.1-bin-hadoop2.7.tgz file,並更改目錄名稱(過長) 12tar -zxvf /bgdt/spark-2.2.1-bin-hadoop2.7.tgz -C /bgdtmv /bgdt/spark-2.2.1-bin-hadoop2.7 /bgdt/spark-2.2.1 加入SPARK_HOME相關的環境變數 12345678910vi ~/.bashrc##Spark Segementexport SPARK_HOME=/bgdt/spark-2.2.1export PATH=$PATH:$SPARK_HOME/sbin:$SPARK_HOME/binsource ~/.bashrccp /bgdt/mysql-connector-java-5.1.39.jar /bgdt/spark-2.2.1/jarsln -s /bgdt/hive-2.3.2/conf/hive-site.xml /bgdt/spark-2.2.1/conf/hive-site.xml 執行 spark shell(for python) 123456789101112131415161718192021hadoop@hadoop-master:~$ pysparkPython 2.7.13 (default, Jan 19 2017, 14:48:08)[GCC 6.3.0 20170118] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.Setting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).2017-12-29 11:34:53,515 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable2017-12-29 11:35:06,762 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectExceptionWelcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ &apos;_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.2.1 /_/Using Python version 2.7.13 (default, Jan 19 2017 14:48:08)SparkSession available as &apos;spark&apos;.&gt;&gt;&gt; spark&lt;pyspark.sql.session.SparkSession object at 0x7f62b8e37610&gt;&gt;&gt;&gt; sc&lt;SparkContext master=local[*] appName=PySparkShell&gt; 執行 spark shell(for scala) 12345678910111213141516171819202122232425hadoop@hadoop-master:~$ spark-shellSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).2017-12-29 11:44:58,352 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSpark context Web UI available at http://192.168.51.4:4040Spark context available as &apos;sc&apos; (master = local[*], app id = local-1514519099800).Spark session available as &apos;spark&apos;.Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ &apos;_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.2.1 /_/Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_101)Type in expressions to have them evaluated.Type :help for more information.scala&gt; sparkres0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7afb9c93scala&gt; scres1: org.apache.spark.SparkContext = org.apache.spark.SparkContext@14292d71scala&gt; 執行spark-sql Shell 1234567891011121314151617181920212223hadoop@hadoop-master:~$ spark-sql2017-12-29 18:04:01,165 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable2017-12-29 18:04:01,307 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore2017-12-29 18:04:01,344 INFO metastore.ObjectStore: ObjectStore, initialize called2017-12-29 18:04:01,642 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored2017-12-29 18:04:01,642 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored2017-12-29 18:04:05,450 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=&quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order&quot;2017-12-29 18:04:07,495 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.2017-12-29 18:04:07,497 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table...................2017-12-29 18:04:20,967 INFO metastore.HiveMetaStore: 0: get_database: global_temp2017-12-29 18:04:20,967 INFO HiveMetaStore.audit: ugi=hadoop ip=unknown-ip-addr cmd=get_database: global_temp2017-12-29 18:04:20,970 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException2017-12-29 18:04:21,293 INFO session.SessionState: Created local directory: /tmp/d8b65bb8-f4f6-43f2-b544-39331b237673_resources2017-12-29 18:04:21,298 INFO session.SessionState: Created HDFS directory: /tmp/hive/hadoop/d8b65bb8-f4f6-43f2-b544-39331b2376732017-12-29 18:04:21,306 INFO session.SessionState: Created local directory: /tmp/hadoop/d8b65bb8-f4f6-43f2-b544-39331b2376732017-12-29 18:04:21,310 INFO session.SessionState: Created HDFS directory: /tmp/hive/hadoop/d8b65bb8-f4f6-43f2-b544-39331b237673/_tmp_space.db2017-12-29 18:04:21,311 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse2017-12-29 18:04:21,406 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpointspark-sql&gt; Spark2.2.1 &amp; Hive2.3.2整合測試 1234567891011121314151617181920212223242526272829303132333435363738394041424344###開啟第1個putty連線hadoop@hadoop-master:~$spark-sqlspark-sql&gt;create database test1;..... INFO execution.SparkSqlParser: Parsing command: create database test1..... INFO metastore.HiveMetaStore: 0: create_database: Database(name:test1, description:, locationUri:hdfs://hadoop-master:8020/user/hive/warehouse/test1.db, parameters:&#123;&#125;)..... INFO HiveMetaStore.audit: ugi=hadoop ip=unknown-ip-addr cmd=create_database: Database(name:test1, description:, locationUri:hdfs://hadoop-master:8020/user/hive/warehouse/test1.db, parameters:&#123;&#125;)..... WARN metastore.ObjectStore: Failed to get database test1, returning NoSuchObjectException..... INFO common.FileUtils: Creating directory if it doesn&apos;t exist: hdfs://hadoop-master:8020/user/hive/warehouse/test1.dbTime taken: 0.326 seconds..... INFO CliDriver: Time taken: 0.326 secondsspark-sql&gt;show databases;INFO execution.SparkSqlParser: Parsing command: show databasesINFO metastore.HiveMetaStore: 0: get_databases: *HiveMetaStore.audit: ugi=hadoop ip=unknown-ip-addr cmd=get_databases: *INFO codegen.CodeGenerator: Code generated in 544.163987 msdefaulttest1###開啟第2個putty連線hadoop@hadoop-master:~$hivehive&gt;show databases;OKdefaulttest1......hive&gt;create database test2;OKTime taken: 0.341 secondshive&gt;show databases;OKdefaulttest1test2......spark-sql&gt;show databases;defaulttest1test2###以上驗證由兩種不同方式所建立的資料庫是可以同步的","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Spark","slug":"BigData/Spark","permalink":"http://yoursite.com/categories/BigData/Spark/"}],"tags":[]},{"title":"Hive2.3+Hadooop3.0.0環境安裝說明","slug":"[hive]Hive2.3_for_Hadoop3_Install","date":"2017-12-27T16:00:00.000Z","updated":"2020-07-22T08:33:56.143Z","comments":true,"path":"2017/12/28/[hive]Hive2.3_for_Hadoop3_Install/","link":"","permalink":"http://yoursite.com/2017/12/28/[hive]Hive2.3_for_Hadoop3_Install/","excerpt":"","text":"相關環境說明 IP Address HostName 角色 192.168.51.4 hadoop-master NameNode(NN),SecondaryNameNode, HiveServer2 192.168.51.5 hadoop-slave1 DataNode(DN1) 192.168.51.6 hadoop-slave2 DataNode(DN2) 環境準備Install Linux OS(略)Install Linux JDK8(略)Install Linux Hadoop3.0.0(略)Install MariaDB(略)Hive2.3 Install File Download 12wget -P /bgdt https://archive.apache.org/dist/hive/hive-2.3.2/apache-hive-2.3.2-bin.tar.gz wget -P /bgdt http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.39/mysql-connector-java-5.1.39.jar 解壓縮 1tar -zxvf /bgdt/apache-hive-2.3.2-bin.tar.gz -C /bgdt 使用Mariadb建立Hive2.3所使用的MetaStore DataBase 1234567891011121314step1:sudo -S /usr/local/mysql/bin/mysqld --user=root &amp; ##啟動mariadbhadoop@hadoop-master:~$ netstat -tnl | grep &quot;3306&quot;tcp6 0 0 :::3306 :::* LISTENstep2:/usr/local/mysql/bin/mysql -u root --password=\\!QAZxsw2 ##進入Mariadb CLIMariaDB [(none)]&gt;step3:MariaDB [(none)]&gt;grant all privileges on *.* to &apos;root&apos;@&apos;hadoop-master&apos; identified by &apos;!QAZxsw2&apos; with grant option;step4:MariaDB [(none)]&gt; create database metastore3_db; ##這邊建立的database就是之後ConnectionURL中,使用的DB名稱 在HDFS中建立hive儲存目錄 1hadoop fs -mkdir -p /user/hive/warehouse 使用SchemaTool建立Hive2.3 Meta Data Table Schema 12schematool -dbType mysql -initSchema ##此語法會重新建立新的Table,如果需要upgrade就不行用這個語法schematool -dbType mysql -upgradeSchemaFrom 0.7.0 -dryRun ##更新新的schema,但不會影響到既有的Table 相關配置檔設定 hive-site.xml檔案設定內容1234567891011121314151617181920212223242526272829303132&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop-master:3306/metastore3_db&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;!QAZxsw2&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 修改~/.bashrc 12345678910vi ~/.bashrc##Hive Segementexport HIVE_HOME=/bgdt/hive-2.3.2export PATH=$PATH:$HIVE_HOME/binsource ~/.bashrccp /bgdt/mysql-connector-java-5.1.39.jar /bgdt/hive-2.3.2/lib 啟動Hive 12hive ##進入Hive CLInohup /bgdt/hive-2.3.2/bin/hive --service hiveserver2 &amp; ##啟動Hive2.3 for JDBC Service","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hive","slug":"BigData/Hive","permalink":"http://yoursite.com/categories/BigData/Hive/"}],"tags":[]},{"title":"Hadoop3.0.0 Cluster環境安裝說明","slug":"[hadoop]hadoop_3_install","date":"2017-12-26T16:00:00.000Z","updated":"2020-07-22T08:26:09.698Z","comments":true,"path":"2017/12/27/[hadoop]hadoop_3_install/","link":"","permalink":"http://yoursite.com/2017/12/27/[hadoop]hadoop_3_install/","excerpt":"","text":"相關環境說明 IP Address HostName 角色 192.168.51.4 hadoop-master NameNode(NN),SecondaryNameNode 192.168.51.5 hadoop-slave1 DataNode(DN1) 192.168.51.6 hadoop-slave2 DataNode(DN2) 環境準備Install Linux OS(略)Install JDK8(略)下載hadoop-3.0.0.tar.gz 1wget -P /bgdt http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-3.0.0/hadoop-3.0.0.tar.gz 檔案解壓縮 123456789101112131415161718tar -zxvf /bgdt/hadoop-3.0.0.tar.gz -C /bgdt---解完壓縮檔之後進入hadoop-3.0.0目錄查看目錄結構[hadoop@hadoop-master:~$]ls -la /bgdt/hadoop-3.0.0total 220drwxr-xr-x 12 hadoop hadoop 4096 Dec 26 15:23 .drwxr-xr-x 10 hadoop hadoop 4096 Dec 27 16:54 ..drwxr-xr-x 2 hadoop hadoop 4096 Dec 9 03:42 bindrwxr-xr-x 3 hadoop hadoop 4096 Dec 9 03:17 etcdrwxr-xr-x 2 hadoop hadoop 4096 Dec 9 03:42 includedrwxr-xr-x 3 hadoop hadoop 4096 Dec 9 03:42 libdrwxr-xr-x 4 hadoop hadoop 4096 Dec 9 03:42 libexec-rw-r--r-- 1 hadoop hadoop 147066 Nov 15 03:19 LICENSE.txtdrwxrwxr-x 3 hadoop hadoop 4096 Dec 27 10:58 logs-rw-r--r-- 1 hadoop hadoop 20891 Nov 15 03:19 NOTICE.txt-rw-r--r-- 1 hadoop hadoop 1366 Jul 9 2016 README.txtdrwxr-xr-x 3 hadoop hadoop 4096 Dec 9 03:17 sbindrwxr-xr-x 4 hadoop hadoop 4096 Dec 9 03:53 share 設定檔編輯修改~/.bashrc 123456789vi ~/.bashrcexport JAVA_HOME=/bgdt/java/jdk1.8.0_101export HADOOP_HOME=/bgdt/hadoop-3.0.0export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin ---修改完成後:wq存檔[hadoop@hadoop-master:~$]source ~/.bashrc core-site.xml 123456789101112131415161718192021&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop-master:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/bgdt/hadoop-3.0.0/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop-master:50090&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/bgdt/hadoop-3.0.0/tmp/dfs/name&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/bgdt/hadoop-3.0.0/tmp/dfs/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.block.size&lt;/name&gt; &lt;value&gt;64M&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; maperd-site.xml 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;mna1:10020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;mna1:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.application.classpath&lt;/name&gt; &lt;value&gt; $HADOOP_HOME/share/hadoop/common/*, $HADOOP_HOME/share/hadoop/hdfs/*, $HADOOP_HOME/share/hadoop/mapreduce/*, $HADOOP_HOME/share/hadoop/yarn/* &lt;/value&gt;&lt;/property&gt; &lt;/configuration&gt; yarn-site.xml 12345678910111213141516&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop-master&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop-master:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; worker 12hadoop-slave1hadoop-slave2 壓縮檔案後scp至DataNode 123456789101112將NameNode hadoop目錄壓縮後copy至其他DataNode,進行解壓縮---[hadoop@hadoop-master:~$]cd /bgdt[hadoop@hadoop-master:~$]tar -zcvf ~/hadoop-3.0.0.tar.gz hadoop-3.0.0[hadoop@hadoop-master:~$]scp ~/hadoop-3.0.0.tar.gz hadoop-slave1:/bgdt[hadoop@hadoop-master:~$]scp ~/hadoop-3.0.0.tar.gz hadoop-slave2:/bgdt---ssh至DataNode並將hadoop-3.0.0.tar.gz解壓縮ssh hadoop-slave1[hadoop@hadoop-slave1:~$]tar -zxvf /bgdt/hadoop-3.0.0.tar.gz -C /bgdtssh hadoop-slave2[hadoop@hadoop-slave2:~$]tar -zxvf /bgdt/hadoop-3.0.0.tar.gz -C /bgdt HDFS格式化 1hadoop namenode -format Hadoop 3.0.0 Port Number List Service Name Hadoop 2.X Port Number Hadoop 3.X Port Number Hadoop HDFS NameNode 8020 9820 Hadoop HDFS NameNode HTTP UI 50070 9870 Hadoop HDFS NameNode HTTPS UI 50470 9840 Hadoop HDFS SecondaryNameNode HTTP UI 50091 9869 Hadoop HDFS SecondaryNameNode HTTPS UI 50090 9868 Hadoop HDFS 8020 9820 Hadoop HDFS DataNode IPC Port 50020 9867 Hadoop HDFS DataNode 50010 9866 Hadoop HDFS DataNode HTTP UI 50075 9864 Hadoop HDFS DataNode HTTPS UI 50475 9865 Hadoop服務啟動命令 123456789101112131415161718192021222324252627282930啟動方式1:使用start-*.sh-----[hadoop@hadoop-master:~$]start-dfs.sh[hadoop@hadoop-master:~$]start-yarn.sh[hadoop@hadoop-master:~$]jps2340 ResourceManager1834 NameNode2107 SecondaryNameNode[hadoop@hadoop-master:~$]ssh hadoop-slave1[hadoop@hadoop-slave1:~$]jps1568 DataNode1738 NodeManager[hadoop@hadoop-master:~$]ssh hadoop-slave2[hadoop@hadoop-slave2:~$]jps1715 NodeManager1545 DataNode啟動方式2:-----[hadoop@hadoop-master:~$]hdfs --daemon start namenode[hadoop@hadoop-master:~$]yarn --daemon start resourcemanager[hadoop@hadoop-master:~$]hdfs --daemon start proxyserver[hadoop@hadoop-master:~$]hdfs --daemon start httpfs[hadoop@hadoop-master:~$]mapred --daemon start historyserver-[hadoop@hadoop-slave1:~$]hdfs --daemon start datanode[hadoop@hadoop-slave1:~$]yarn --daemon start nodemanager-[hadoop@hadoop-slave2:~$]hdfs --daemon start datanode[hadoop@hadoop-slave2:~$]yarn --daemon start nodemanager Hadoop服務關閉 1234567891011121314151617181920212223停止方式1:使用stop-*.sh-----[hadoop@hadoop-master:~$]stop-dfs.sh[hadoop@hadoop-master:~$]stop-yarn.sh停止方式2:-----[hadoop@hadoop-master:~$]hdfs --daemon stop namenode[hadoop@hadoop-master:~$]yarn --daemon stop resourcemanager[hadoop@hadoop-master:~$]hdfs --daemon stop proxyserver[hadoop@hadoop-master:~$]hdfs --daemon stop httpfs[hadoop@hadoop-master:~$]mapred --daemon stop historyserver-[hadoop@hadoop-slave1:~$]hdfs --daemon stop datanode[hadoop@hadoop-slave1:~$]yarn --daemon stop nodemanager-[hadoop@hadoop-slave2:~$]hdfs --daemon stop datanode[hadoop@hadoop-slave2:~$]yarn --daemon stop nodemanager停止DataNode方式3:-----[hadoop@hadoop-master:~$]hdfs dfsadmin -shutdownDatanode 192.168.51.5:9867[hadoop@hadoop-master:~$]hdfs dfsadmin -shutdownDatanode 192.168.51.6:9867 Hadoop MapReduce Example 12hadoop jar /bgdt/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar pi 4 100hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar grep input output &apos;dfs[a-z.]+&apos; Hadoop Web UIweb-1 Hadoop WebUI-1 Port:9870,此頁面可以觀察Namenode相關資訊 web-2 Hadoop WebUI-2 此頁面可以觀察DataNode狀態 web-3 Hadoop WebUI-3 此頁面可以觀察HDFS檔案結構,新版的Browser可以直接由介面上上傳檔案,但先決條件為沒有設定相關權限 web-4 Hadoop WebUI-4,Port:8088,此頁面為ResourceManager的管理介面,系統會列出所有Application的運行狀態 UI-5 此圖為Run Hadoop MapReduce時,產生的錯誤訊息(此部分為Hadoop3.0.0的問題,Exception可以暫時略過不管) HDFS-10429 DataStreamer interrupted warning always appears when using CLI upload file UI-6 程式Run完後,在此畫面可以觀察Application最後的執行狀態,如果有啟動JobHistory時可以觀察到相關的Log","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"Excel之下拉式選單設計技巧-1","slug":"[excel]Excel_skill","date":"2017-12-07T16:00:00.000Z","updated":"2020-07-22T08:24:19.047Z","comments":true,"path":"2017/12/08/[excel]Excel_skill/","link":"","permalink":"http://yoursite.com/2017/12/08/[excel]Excel_skill/","excerpt":"","text":"Excel操作的小小筆記 開啟Excel,之後先建立一個名叫部門代碼表的Sheet 建立完工作表之後,需在”公式”-&gt;”名稱管理員”-&gt;”新增”中定義相關的名稱其中有需要注意以下幾個地方:1.名稱:部門代碼表2.範圍:整個活頁簿3.參照到:需要在要定義的工作表中選出定義的範圍(=部門代碼表!$A:$B)“=部門代碼表!”–&gt;意思是部門代碼表這個工作表(Sheet)“$A:$B”–&gt;表示工作表中的A欄和B欄全部選取的內容是A和B兩個欄位全部,因為我們會想要隨時增加部門進來所以不限定列數 我們再新增一個”部門名稱”的定義給名稱管理員,相關欄位同上圖,只是這次我們只選擇A欄這個部份的定義是要給下拉式選單當成選擇的項目 最後我們查看名稱管理員會出現兩個名稱定義(部門代碼表,部門名稱) 再來我們開啟另一個工作表,隨便選一個cell做為下拉式選單測試接著點選”資料”-&gt;”資料驗證”-&gt;”設定”然後”儲存格內允許”–&gt;清單“來源”–&gt;填入”=部門名稱”“儲存格內的下拉式選單”記得勾選 此時點選按鈕就會出現一個下拉式選單 最後一個步驟,選擇一個要呈現結果的cell,填入以下公式=VLOOKUP(A1,部門代碼表,2,FALSE)以下說明VLOOKUP(自動查找函數)相關參數查閱欄位：填入欄位編號查閱範圍：定義的範圍名稱欄位編號：取出欄位編號，在定義的範圍名稱欄位代號(由左至右1,2,….)是否完全符合：搜尋時是否要完全符合查詢欄位的值，TRUE-部分符合，FALSE-完全符合。","categories":[{"name":"Other","slug":"Other","permalink":"http://yoursite.com/categories/Other/"},{"name":"Excel","slug":"Other/Excel","permalink":"http://yoursite.com/categories/Other/Excel/"}],"tags":[]},{"title":"Hive相關語法整理-3(Partition Table)","slug":"[hive]hive_sql_3","date":"2017-11-06T16:00:00.000Z","updated":"2020-07-22T08:33:20.487Z","comments":true,"path":"2017/11/07/[hive]hive_sql_3/","link":"","permalink":"http://yoursite.com/2017/11/07/[hive]hive_sql_3/","excerpt":"","text":"Create Partition Table CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name [(col_name data_type [COMMENT col_comment], … [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], …)] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY ‘storage.handler.class.name’ [WITH SERDEPROPERTIES (…)] ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, …)] [AS select_statement];1234567891011Example:create table page_view_7( viewTime int, userid bigint, page_url string, referrer_url string, ip string)partitioned by(dt string, country string)row format delimited fields terminated by &apos;,&apos;stored as textfile; Alter PartitionAdd Partition ALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION ‘location’][, PARTITION partition_spec [LOCATION ‘location’], …];123Example:ALTER TABLE page_view_7 ADD PARTITION (dt=&apos;2008-08-08&apos;, country=&apos;us&apos;);ALTER TABLE page_view_7 ADD PARTITION (dt=&apos;2017-11-01&apos;, country=&apos;us&apos;) location &quot;/user/hadoop/data/page_view_7/partitions&quot;; Rename Partition ALTER TABLE table_name PARTITION partition_spec RENAME TO PARTITION partition_spec;1234567891011121314Example:alter table page_view_7 partition (dt=&quot;2017-11-07&quot;,country=&quot;taiwan&quot;) rename to partition (dt=&quot;test_2017-11-07&quot;,country=&quot;taiwan&quot;);hive&gt; show partitions page_view_7;OKdt=2008-08-08/country=Taiwandt=2008-08-08/country=usdt=2008-08-09/country=usdt=2008-08-10/country=usdt=2017-08-08/country=taiwandt=2017-08-08/country=usdt=2017-11-01/country=usdt=test_2017-11-07/country=taiwanTime taken: 0.054 seconds, Fetched: 8 row(s) Exchange Partition ALTER TABLE table_name_2 EXCHANGE PARTITION (partition_spec) WITH TABLE table_name_1;123456789101112131415161718192021222324Example:alter table pv_7 exchange partition (dt=&quot;test_2017-11-07&quot;,country=&quot;taiwan&quot;) with table page_view_7;Demo:hive&gt; create table pv_7 like page_view_7;OKTime taken: 0.59 secondshive&gt; alter table pv_7 exchange partition (dt=&quot;test_2017-11-07&quot;,country=&quot;taiwan&quot;) with table page_view_7;OKTime taken: 0.297 secondshive&gt; show partitions page_view_7;OKdt=2008-08-08/country=Taiwandt=2008-08-08/country=usdt=2008-08-09/country=usdt=2008-08-10/country=usdt=2017-08-08/country=taiwandt=2017-08-08/country=usdt=2017-11-01/country=usTime taken: 0.058 seconds, Fetched: 7 row(s)hive&gt; show partitions pv_7;OKdt=test_2017-11-07/country=taiwanTime taken: 0.055 seconds, Fetched: 1 row(s) Drop Partition ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, …] [IGNORE PROTECTION] [PURGE];123456789101112131415161718192021222324252627Example:alter table page_view_7 drop if exists partition (dt=&quot;2017-11-01&quot;,country=&quot;us&quot;);Demo:hive&gt; show partitions page_view_7;OKdt=2008-08-08/country=Taiwandt=2008-08-08/country=usdt=2008-08-09/country=usdt=2008-08-10/country=usdt=2017-08-08/country=taiwandt=2017-08-08/country=usdt=2017-11-01/country=usTime taken: 0.054 seconds, Fetched: 7 row(s)hive&gt; alter table page_view_7 drop if exists partition (dt=&quot;2017-11-01&quot;,country=&quot;us&quot;);Dropped the partition dt=2017-11-01/country=usOKTime taken: 0.36 secondshive&gt; show partitions page_view_7;OKdt=2008-08-08/country=Taiwandt=2008-08-08/country=usdt=2008-08-09/country=usdt=2008-08-10/country=usdt=2017-08-08/country=taiwandt=2017-08-08/country=usTime taken: 0.052 seconds, Fetched: 6 row(s) Show Partition","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hive","slug":"BigData/Hive","permalink":"http://yoursite.com/categories/BigData/Hive/"}],"tags":[]},{"title":"Hive相關語法整理-1","slug":"[hive]hive_sql_1","date":"2017-11-05T16:00:00.000Z","updated":"2020-07-22T08:32:41.486Z","comments":true,"path":"2017/11/06/[hive]hive_sql_1/","link":"","permalink":"http://yoursite.com/2017/11/06/[hive]hive_sql_1/","excerpt":"","text":"Create Database create (database|schema) [if not exists] database_name [comment database_comment] [location hdfs_path] [with dbproperties (property_name=property_value, …)];12345678910Example:create database test1;create database if not exists test1;create database if not exists mydatabasewith dbproperties(&apos;Experiment Name&apos; = &apos;Correlation age/sentiment&apos;,&apos;date&apos; = &apos;2013-07-11&apos;,&apos;Lead Developer&apos; = &apos;John Foo&apos;,&apos;Lead Developer Email&apos; = &apos;jfoo@somewhere.com&apos;); Alter Database ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, …); ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path;123Example: alter database mydatabase set dbproperties(&quot;date&quot;=&quot;2017-11-06&quot;); alter database mydatabase set dbproperties(&quot;test&quot;=&quot;2017-11-06&quot;); Show Database 123Example:show databases;show databases like &quot;test*&quot;; Desc Database describe database [extended] db_name;123Example:desc database test_db;desc database extended test_db; Use Database use db_name12Example:use test1; Drop Database drop (database|schema) [if exists] database_name [restrict|cascade];123Example:drop database if exists test_db; ##Database內有Table的話無法drop,必須是要空的Databasedrop database test_db cascade; ##Database刪除時會Database內所有的Table資料一併刪除","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hive","slug":"BigData/Hive","permalink":"http://yoursite.com/categories/BigData/Hive/"}],"tags":[]},{"title":"Hive相關語法整理-2(Table)","slug":"[hive]hive_sql_2","date":"2017-11-05T16:00:00.000Z","updated":"2020-07-22T08:33:01.189Z","comments":true,"path":"2017/11/06/[hive]hive_sql_2/","link":"","permalink":"http://yoursite.com/2017/11/06/[hive]hive_sql_2/","excerpt":"","text":"Create Table CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name [(col_name data_type [COMMENT col_comment], … [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], …)] [CLUSTERED BY (col_name, col_name, …) [SORTED BY (col_name [ASC|DESC], …)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, …) – (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, …), (col_value, col_value, …), …) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY ‘storage.handler.class.name’ [WITH SERDEPROPERTIES (…)] – (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, …)] – (Note: Available in Hive 0.6.0 and later) [AS select_statement]; – (Note: Available in Hive 0.5.0 and later; not supported for external tables) CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path]; 12345678Example1:以下範例為Table建立各種型態欄位create table test1(a1 tinyint,a2 smallint,a3 int,a4 bigint);create table test2(a1 float,a2 double);create table test3(a1 string,a2 varchar(2000),a3 char(4));create table test4(a1 boolean,a2 date,a3 timestamp);create table test5(a1 map&lt;string,string&gt;,a2 array&lt;string&gt;,a3 struct&lt;c1:string,c2:int,c3:date&gt;);create table test5_1(a1 map&lt;string,string&gt;,a2 array&lt;string&gt;,a3 struct&lt;c1:string,c2:int,c3:date&gt;,a4 uniontype&lt;string,int,date&gt;); 1234Example2:建立Partition Tablecreate table test6_1(a1 string,a2 int)partitioned by(a3 string); 12345Example3:建立Bluck Tablecreate table test6_2(a1 string,a2 int)clustered by(a2) into 5 buckets; 123Example4:CTAS 用法範例create table test7 as select &quot;ttt&quot;, concat(&quot;part_&quot;, &quot;aaa&quot;); 123Example5:建立外部表格,表格刪除時Table檔案並不會跟著一起被刪除create external table if not exists test8(a1 string,a2 int); 123Example6:建立暫存表格create temporary table if not exists test9(a1 string,a2 int); 123Example7:建立與某一個表格相同欄位的表格 (copy 某個Table的欄位)create table test10 like test1; --copy columns from table; 123456789101112131415161718Example8:create table taiwan_area( cityname string COMMENT &quot;行政區名稱&quot;, type string COMMENT &quot;行政區類別&quot;, area float COMMENT &quot;區域面積&quot;, village_num smallint COMMENT &quot;村里數&quot;, neighborhood_num smallint COMMENT &quot;鄰數&quot;, people_num int COMMENT &quot;人口數&quot;, density int COMMENT &quot;人口密度&quot;, zipcode smallint COMMENT &quot;郵遞區號&quot;)row format serde &apos;org.apache.hadoop.hive.serde2.OpenCSVSerde&apos;with SERDEPROPERTIES(&quot;separatorChar&quot; = &quot;,&quot;,&quot;quoteChar&quot; = &quot;\\&quot;&quot;,&quot;escapeChar&quot; = &quot;\\\\&quot;)STORED AS TEXTFILE; 123456789101112Result:hive&gt; desc taiwan_area;OKcityname string from deserializertype string from deserializerarea string from deserializervillage_num string from deserializerneighborhood_num string from deserializerpeople_num string from deserializerdensity string from deserializerzipcode string from deserializerTime taken: 0.254 seconds, Fetched: 8 row(s) 1234567891011121314Example9:create table page_view( viewTime int, userid bigint, page_url string, referrer_url string, ip string comment &apos;IP Address of the User&apos;)comment &apos;This is the page view table&apos;partitioned by(dt string, country string)row format delimited fields terminated by &apos;\\001&apos; collection items terminated by &apos;\\002&apos; map keys terminated by &apos;\\003&apos;stored as textfile; 1234567891011121314151617Result:hive&gt; desc page_view;OKviewtime intuserid bigintpage_url stringreferrer_url stringip string IP Address of the Userdt stringcountry string# Partition Information# col_name data_type commentdt stringcountry stringTime taken: 0.193 seconds, Fetched: 13 row(s) Example10:12345678Prepare CSV file:CSV檔內容如下:test1.csvecho 100,1,\\&quot;http://www.google.com\\&quot;,\\&quot;http://www.google.com\\&quot;,\\&quot;192.168.1.1\\&quot; &gt; ~/test1.csvecho 200,2,\\&quot;http://www.google.com\\&quot;,\\&quot;http://www.google.com\\&quot;,\\&quot;192.168.1.2\\&quot; &gt;&gt; ~/test1.csvecho 150,3,\\&quot;http://www.google.com\\&quot;,\\&quot;http://www.google.com\\&quot;,\\&quot;192.168.1.3\\&quot; &gt;&gt; ~/test1.csvecho 300,4,\\&quot;http://www.google.com\\&quot;,\\&quot;http://www.google.com\\&quot;,\\&quot;192.168.1.4\\&quot; &gt;&gt; ~/test1.csv直接在Linux系統上使用指令建立以上檔案 1234567891011create table page_view_1( viewTime int, userid bigint, page_url string, referrer_url string, ip string comment &apos;IP Address of the User&apos;)comment &apos;This is the page view table&apos;partitioned by(dt string, country string)row format delimited fields terminated by &apos;,&apos;stored as textfile; 12使用Load將資料insert至Tableload data local inpath &quot;/home/hadoop/test1.csv&quot; overwrite into table page_view_1 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;); 12345678Result:hive&gt; select * from page_view_1;OK100 1 &quot;http://www.google.com&quot; &quot;http://www.google.com&quot; &quot;192.168.1.1&quot; taipei taiwan200 2 &quot;http://www.google.com&quot; &quot;http://www.google.com&quot; &quot;192.168.1.2&quot; taipei taiwan150 3 &quot;http://www.google.com&quot; &quot;http://www.google.com&quot; &quot;192.168.1.3&quot; taipei taiwan300 4 &quot;http://www.google.com&quot; &quot;http://www.google.com&quot; &quot;192.168.1.4&quot; taipei taiwanTime taken: 0.335 seconds, Fetched: 4 row(s) 12345678910Example11Step1:建立CSV檔test2.csvecho 1,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;404\\&quot;,100 &gt;~/test2.csvecho 2,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;200\\&quot;,100 &gt;&gt;~/test2.csvecho 3,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;500\\&quot;,100 &gt;&gt;~/test2.csvecho 4,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;503\\&quot;,100 &gt;&gt;~/test2.csvecho 5,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;201\\&quot;,100 &gt;&gt;~/test2.csvecho 6,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;403\\&quot;,100 &gt;&gt;~/test2.csv 1234567891011Step2:create table page_view_2( userid bigint, data struct&lt;page_url:string,ip:string,status:string&gt;, view_count int)comment &apos;This is the page view table&apos;partitioned by(dt string, country string)row format delimited fields terminated by &apos;,&apos; collection items terminated by &apos;\\;&apos;stored as textfile; 12Step3:load data local inpath &quot;/home/hadoop/test2.csv&quot; overwrite into table page_view_2 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;); 12345678910Step4:hive&gt; select * from page_view_2;OK1 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;404\\&quot;&quot;&#125; 100 taipei taiwan2 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;200\\&quot;&quot;&#125; 100 taipei taiwan3 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;500\\&quot;&quot;&#125; 100 taipei taiwan4 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;503\\&quot;&quot;&#125; 100 taipei taiwan5 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;201\\&quot;&quot;&#125; 100 taipei taiwan6 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;403\\&quot;&quot;&#125; 100 taipei taiwanTime taken: 2.183 seconds, Fetched: 6 row(s) 1234567891011121314151617Example12:Step1:create table page_view_3( viewTime int, userid bigint, page_url string, referrer_url string, ip string comment &apos;IP Address of the User&apos;)comment &apos;This is the page view table&apos;partitioned by(dt string, country string)row format serde &apos;org.apache.hadoop.hive.serde2.OpenCSVSerde&apos;with serdeproperties(&quot;separatorChar&quot; = &quot;,&quot;,&quot;quoteChar&quot; = &quot;\\&quot;&quot;,&quot;escapeChar&quot; = &quot;\\\\&quot;) stored as textfile; 12Step2:load data local inpath &quot;/home/hadoop/test1.csv&quot; overwrite into table page_view_3 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;); 12345678Step3:hive&gt; select * from page_view_3;OK100 1 http://www.google.com http://www.google.com 192.168.1.1 taipei taiwan200 2 http://www.google.com http://www.google.com 192.168.1.2 taipei taiwan150 3 http://www.google.com http://www.google.com 192.168.1.3 taipei taiwan300 4 http://www.google.com http://www.google.com 192.168.1.4 taipei taiwanTime taken: 1.003 seconds, Fetched: 4 row(s) 12345678910Example13使用delimited切割欄位,以分號來做為集合區分Step1:echo 1,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;100\\&quot;,\\&quot;001\\&quot;\\;\\&quot;test1_001\\&quot;\\;100 &gt;~/test4.csvecho 2,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.2\\&quot;\\;\\&quot;200\\&quot;,\\&quot;002\\&quot;\\;\\&quot;test1_002\\&quot;\\;200 &gt;&gt;~/test4.csvecho 3,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.3\\&quot;\\;\\&quot;300\\&quot;,\\&quot;003\\&quot;\\;\\&quot;test1_003\\&quot;\\;300 &gt;&gt;~/test4.csvecho 4,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.4\\&quot;\\;\\&quot;400\\&quot;,\\&quot;004\\&quot;\\;\\&quot;test1_004\\&quot;\\;400 &gt;&gt;~/test4.csvecho 5,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.5\\&quot;\\;\\&quot;500\\&quot;,\\&quot;005\\&quot;\\;\\&quot;test1_005\\&quot;\\;500 &gt;&gt;~/test4.csvecho 6,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.6\\&quot;\\;\\&quot;600\\&quot;,\\&quot;006\\&quot;\\;\\&quot;test1_006\\&quot;\\;600 &gt;&gt;~/test4.csvecho 7,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.7\\&quot;\\;\\&quot;700\\&quot;,\\&quot;007\\&quot;\\;\\&quot;test1_007\\&quot;\\;700 &gt;&gt;~/test4.csv 12345678910111213Step2:建立Tablecreate table page_view_4( id bigint, data struct&lt;page_url:string,ip:string,status:string&gt;, userdata struct&lt;id:string,name:string,viewcnt:int&gt;)comment &apos;This is the page view table&apos;partitioned by(dt string, country string)row format delimited fields terminated by &apos;,&apos; collection items terminated by &quot;\\;&quot;stored as textfile; 123Step3:load data to page_view_4load data local inpath &quot;/home/hadoop/test4.csv&quot; overwrite into table page_view_4 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;); 1234567891011Step4:hive&gt; select * from page_view_4;OK1 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;100\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;001\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_001\\&quot;&quot;,&quot;viewcnt&quot;:100&#125; taipei taiwan2 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.2\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;200\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;002\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_002\\&quot;&quot;,&quot;viewcnt&quot;:200&#125; taipei taiwan3 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.3\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;300\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;003\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_003\\&quot;&quot;,&quot;viewcnt&quot;:300&#125; taipei taiwan4 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.4\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;400\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;004\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_004\\&quot;&quot;,&quot;viewcnt&quot;:400&#125; taipei taiwan5 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.5\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;500\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;005\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_005\\&quot;&quot;,&quot;viewcnt&quot;:500&#125; taipei taiwan6 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.6\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;600\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;006\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_006\\&quot;&quot;,&quot;viewcnt&quot;:600&#125; taipei taiwan7 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.7\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;700\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;007\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_007\\&quot;&quot;,&quot;viewcnt&quot;:700&#125; taipei taiwanTime taken: 0.13 seconds, Fetched: 7 row(s) 123456789Example14:使用冒號(:)來做為Map的key,value的區分範例Step1:echo 1,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;401\\&quot;,id:001\\;name:test1_001\\;count:100 &gt; ~/test5.csvecho 2,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.2\\&quot;\\;\\&quot;401\\&quot;,id:002\\;name:test1_002\\;count:200 &gt;&gt; ~/test5.csvecho 3,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.3\\&quot;\\;\\&quot;401\\&quot;,id:003\\;name:test1_003\\;count:300 &gt;&gt; ~/test5.csvecho 4,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.4\\&quot;\\;\\&quot;401\\&quot;,id:004\\;name:test1_004\\;count:400 &gt;&gt; ~/test5.csvecho 5,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.5\\&quot;\\;\\&quot;401\\&quot;,id:005\\;name:test1_005\\;count:500 &gt;&gt; ~/test5.csvecho 6,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.6\\&quot;\\;\\&quot;401\\&quot;,id:006\\;name:test1_006\\;count:600 &gt;&gt; ~/test5.csv 12345678910111213Step2:create table page_view_5( id bigint, data struct&lt;page_url:string,ip:string,status:string&gt;, userdata map&lt;string,string&gt;)comment &apos;This is the page view table&apos;partitioned by(dt string, country string)row format delimited fields terminated by &apos;,&apos; collection items terminated by &quot;\\;&quot; map keys terminated by &quot;\\:&quot;stored as textfile; 12Step3:load data local inpath &quot;/home/hadoop/test5.csv&quot; overwrite into table page_view_5 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;); 12345678910Step4:hive&gt; select * from page_view_5;OK1 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;001&quot;,&quot;name&quot;:&quot;test1_001&quot;,&quot;count&quot;:&quot;100&quot;&#125; taipei taiwan2 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.2\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;002&quot;,&quot;name&quot;:&quot;test1_002&quot;,&quot;count&quot;:&quot;200&quot;&#125; taipei taiwan3 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.3\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;003&quot;,&quot;name&quot;:&quot;test1_003&quot;,&quot;count&quot;:&quot;300&quot;&#125; taipei taiwan4 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.4\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;004&quot;,&quot;name&quot;:&quot;test1_004&quot;,&quot;count&quot;:&quot;400&quot;&#125; taipei taiwan5 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.5\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;005&quot;,&quot;name&quot;:&quot;test1_005&quot;,&quot;count&quot;:&quot;500&quot;&#125; taipei taiwan6 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.6\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;006&quot;,&quot;name&quot;:&quot;test1_006&quot;,&quot;count&quot;:&quot;600&quot;&#125; taipei taiwanTime taken: 1.217 seconds, Fetched: 6 row(s) 1234567Example15:Step1:echo 1,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.1\\&quot;\\;\\&quot;401\\&quot;,id:001\\;name:test1_001\\;gender:F\\;count:100,300\\;400\\;500 &gt; ~/test6.csvecho 2,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.2\\&quot;\\;\\&quot;401\\&quot;,id:002\\;name:test1_002\\;gender:M\\;count:200,600\\;700\\;800 &gt;&gt; ~/test6.csvecho 3,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.3\\&quot;\\;\\&quot;401\\&quot;,id:003\\;name:test1_003\\;gender:F\\;count:300,100\\;200\\;300 &gt;&gt; ~/test6.csvecho 4,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.4\\&quot;\\;\\&quot;401\\&quot;,id:004\\;name:test1_004\\;gender:M\\;count:400,400\\;500\\;600 &gt;&gt; ~/test6.csvecho 5,\\&quot;http://www.google.com\\&quot;\\;\\&quot;192.168.1.5\\&quot;\\;\\&quot;401\\&quot;,id:005\\;name:test1_005\\;gender:F\\;count:500,250\\;100\\;900 &gt;&gt; ~/test6.csv 1234567891011121314Step2:create table page_view_6( id bigint, data struct&lt;page_url:string,ip:string,status:string&gt;, userdata map&lt;string,string&gt;, countlist array&lt;int&gt;)comment &apos;This is the page view table&apos;partitioned by(city string, country string)row format delimited fields terminated by &apos;,&apos; collection items terminated by &quot;\\;&quot; map keys terminated by &quot;\\:&quot;stored as textfile; 12Step3:load data local inpath &quot;/home/hadoop/test6.csv&quot; overwrite into table page_view_6 partition(city=&quot;taipei&quot;,country=&quot;taiwan&quot;); 123456789Step4:hive&gt; select * from page_view_6;OK1 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;001&quot;,&quot;name&quot;:&quot;test1_001&quot;,&quot;gender&quot;:&quot;F&quot;,&quot;count&quot;:&quot;100&quot;&#125; [300,400,500] taipei taiwan2 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.2\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;002&quot;,&quot;name&quot;:&quot;test1_002&quot;,&quot;gender&quot;:&quot;M&quot;,&quot;count&quot;:&quot;200&quot;&#125; [600,700,800] taipei taiwan3 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.3\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;003&quot;,&quot;name&quot;:&quot;test1_003&quot;,&quot;gender&quot;:&quot;F&quot;,&quot;count&quot;:&quot;300&quot;&#125; [100,200,300] taipei taiwan4 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.4\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;004&quot;,&quot;name&quot;:&quot;test1_004&quot;,&quot;gender&quot;:&quot;M&quot;,&quot;count&quot;:&quot;400&quot;&#125; [400,500,600] taipei taiwan5 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.5\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;401\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;005&quot;,&quot;name&quot;:&quot;test1_005&quot;,&quot;gender&quot;:&quot;F&quot;,&quot;count&quot;:&quot;500&quot;&#125; [250,100,900] taipei taiwanTime taken: 1.126 seconds, Fetched: 5 row(s) Alter TableRename Table Name alter table table_name rename to new_table_name;1234Example1:將Table更改名稱或搬移至其他資料庫alter table page_view_3 rename to pv_3;alter table pv_3 rename to test.pv_3; Alter Table Properties alter table table_name set tblproperties table_properties;123Example1:修改Table的屬性alter table test.pv_3 set tblproperties (&quot;comment&quot; = &quot;page_view_3 rename to pv_3&quot;); Add SerDe Properties ALTER TABLE table_name [PARTITION partition_spec] SET SERDE serde_class_name [WITH SERDEPROPERTIES serde_properties]; ALTER TABLE table_name [PARTITION partition_spec] SET SERDEPROPERTIES serde_properties; serde_properties: : (property_name = property_value, property_name = property_value, … )1alter table test.pv_3 set serdeproperties(&apos;field.delim&apos;=&apos;\\u0001&apos;); Alter Either Table or Partition ALTER TABLE table_name [PARTITION partition_spec] SET FILEFORMAT file_format;file_format: :sequence,textfile,rcfile,orc12Example1alter table test.pv_3 set fileformat textfile; ALTER TABLE table_name [PARTITIONpartitionSpec] SET LOCATION “newlocation”修改表格存放路徑12Example:alter table test.pv_3 set location &quot;/user/hadoop/data&quot; Import Data to Tableload data LOAD DATA [local] INPATH ‘hdfs_file_or_directory_path’ [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 …)]1234567891011121314151617181920212223Example1:load data local inpath &quot;/home/hadoop/test1.csv&quot; overwrite into table page_view_3 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;);Result:hive&gt; select * from page_view_3;OKTime taken: 0.106 secondshive&gt; load data local inpath &quot;/home/hadoop/test1.csv&quot; overwrite into table page_view_3 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;);Loading data to table default.page_view_3 partition (dt=taipei, country=taiwan)OKTime taken: 0.658 secondshive&gt; select * from page_view_3;OK100 1 http://www.google.com http://www.google.com 192.168.1.1 taipei taiwan200 2 http://www.google.com http://www.google.com 192.168.1.2 taipei taiwan150 3 http://www.google.com http://www.google.com 192.168.1.3 taipei taiwan300 4 http://www.google.com http://www.google.com 192.168.1.4 taipei taiwanhive&gt; truncate table page_view_3;OKTime taken: 0.345 secondshive&gt; select * from page_view_3;OKTime taken: 0.143 seconds 123456789101112131415161718Example2:load data inpath &quot;/user/hadoop/data/test1.csv&quot; overwrite into table page_view_3 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;);Result:hive&gt; select * from page_view_3;OKTime taken: 0.129 secondshive&gt; load data inpath &quot;/user/hadoop/data/test1.csv&quot; overwrite into table page_view_3 partition(dt=&quot;taipei&quot;,country=&quot;taiwan&quot;);Loading data to table default.page_view_3 partition (dt=taipei, country=taiwan)OKTime taken: 0.647 secondshive&gt; select * from page_view_3;OK100 1 http://www.google.com http://www.google.com 192.168.1.1 taipei taiwan200 2 http://www.google.com http://www.google.com 192.168.1.2 taipei taiwan150 3 http://www.google.com http://www.google.com 192.168.1.3 taipei taiwan300 4 http://www.google.com http://www.google.com 192.168.1.4 taipei taiwanTime taken: 0.109 seconds, Fetched: 4 row(s) create table ….as select create table table_name as select_statement1234567891011121314Example:複製一個Table到新Table中(包含資料)create table test.pv_4 as select * from page_view_4;hive&gt; select * from test.pv_4;OK1 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;100\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;001\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_001\\&quot;&quot;,&quot;viewcnt&quot;:100&#125; taipei taiwan2 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.2\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;200\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;002\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_002\\&quot;&quot;,&quot;viewcnt&quot;:200&#125; taipei taiwan3 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.3\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;300\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;003\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_003\\&quot;&quot;,&quot;viewcnt&quot;:300&#125; taipei taiwan4 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.4\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;400\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;004\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_004\\&quot;&quot;,&quot;viewcnt&quot;:400&#125; taipei taiwan5 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.5\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;500\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;005\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_005\\&quot;&quot;,&quot;viewcnt&quot;:500&#125; taipei taiwan6 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.6\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;600\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;006\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_006\\&quot;&quot;,&quot;viewcnt&quot;:600&#125; taipei taiwan7 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.7\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;700\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;007\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_007\\&quot;&quot;,&quot;viewcnt&quot;:700&#125; taipei taiwanTime taken: 0.087 seconds, Fetched: 7 row(s) create table …like CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path];12345678Example:複製一個Table到新Table中(僅 columns)create table test.pv_4_like like page_view_4;create table if not exists test.pv_4_new like page_view_4;hive&gt; select * from test.pv_4_new;OKTime taken: 0.099 seconds from …insert select FROM from_statementINSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 …) [IF NOT EXISTS]] select_statement1[INSERT OVERWRITE TABLE tablename2 [PARTITION … [IF NOT EXISTS]] select_statement2][INSERT INTO TABLE tablename2 [PARTITION …] select_statement2] …;1234Example:from page_view_3 insert overwrite table page_view_3_1 partition(dt=&quot;taipei&quot;,country=&quot;test1&quot;) if not exists select page_url,viewTime,userid,viewTime,ip; Show TableShow Tables SHOW TABLES [IN database_name] [‘identifier_with_wildcards’];123456789101112131415161718192021222324252627282930313233343536373839Demo:hive&gt; show tables from test;OKlist_bucket_singlepv_3pv_4pv_4_likepv_4_newtest_atest_btest_ctest_dtest_etest_fTime taken: 0.024 seconds, Fetched: 11 row(s)hive&gt; show tables in test;OKlist_bucket_singlepv_3pv_4pv_4_likepv_4_newtest_atest_btest_ctest_dtest_etest_fTime taken: 0.018 seconds, Fetched: 11 row(s)hive&gt; show tables in test like &quot;test*&quot;;OKtest_atest_btest_ctest_dtest_etest_fTime taken: 0.019 seconds, Fetched: 6 row(s)hive&gt; Show Table SHOW TABLE EXTENDED [IN|FROM database_name] LIKE ‘identifier_with_wildcards’ [PARTITION(partition_spec)];12345678910111213141516171819202122Example:show table extended from test like &quot;pv_3&quot;Demo:hive&gt; show table extended from test like &quot;pv_3&quot;;OKtableName:pv_3owner:hadooplocation:hdfs://mna1:8020/user/hadoop/datainputformat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormatoutputformat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormatcolumns:struct columns &#123; i32 viewtime, i64 userid, string page_url, string referrer_url, string ip&#125;partitioned:truepartitionColumns:struct partition_columns &#123; string dt, string country&#125;totalNumberFiles:1totalFileSize:272maxFileSize:272minFileSize:272lastAccessTime:1510037040061lastUpdateTime:1510043296966Time taken: 0.037 seconds, Fetched: 15 row(s) Show Table Properties 12345678910Example:hive&gt; SHOW TBLPROPERTIES page_view_7;OKtransient_lastDdlTime 1510042890Time taken: 0.029 seconds, Fetched: 1 row(s)hive&gt; SHOW TBLPROPERTIES page_view_7(&quot;transient_lastDdlTime&quot;);OK1510042890Time taken: 0.029 seconds, Fetched: 1 row(s) Show Create Table SHOW CREATE TABLE ([db_name.]table_name|view_name);1234567891011121314151617181920212223242526272829Example:show create table page_view_7;Demo:hive&gt; show create table page_view_7;OKCREATE TABLE `page_view_7`( `viewtime` int, `userid` bigint, `page_url` string, `referrer_url` string, `ip` string)PARTITIONED BY ( `dt` string, `country` string)ROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&apos;WITH SERDEPROPERTIES ( &apos;field.delim&apos;=&apos;,&apos;, &apos;serialization.format&apos;=&apos;,&apos;)STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos;OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;LOCATION &apos;hdfs://mna1:8020/user/hive/warehouse/page_view_7&apos;TBLPROPERTIES ( &apos;transient_lastDdlTime&apos;=&apos;1510042890&apos;)Time taken: 0.126 seconds, Fetched: 22 row(s) Describe Tabledesc formatted DESCRIBE [EXTENDED|FORMATTED] [db_name.]table_name[.col_name ( [.field_name] | [.’$elem$’] | [.’$key$’] | [.’$value$’] )* ];1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950Example:desc formatted page_view_7;demo:desc formatted page_view_7;OK# col_name data_type commentviewtime intuserid bigintpage_url stringreferrer_url stringip string# Partition Information# col_name data_type commentdt stringcountry string# Detailed Table InformationDatabase: defaultOwner: hadoopCreateTime: Tue Nov 07 16:21:30 CST 2017LastAccessTime: UNKNOWNRetention: 0Location: hdfs://mna1:8020/user/hive/warehouse/page_view_7Table Type: MANAGED_TABLETable Parameters: transient_lastDdlTime 1510042890# Storage InformationSerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDeInputFormat: org.apache.hadoop.mapred.TextInputFormatOutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormatCompressed: NoNum Buckets: -1Bucket Columns: []Sort Columns: []Storage Desc Params: field.delim , serialization.format ,Time taken: 0.105 seconds, Fetched: 36 row(s)hive&gt; desc formatted page_view_7 page_url;OK# col_name data_type min max num_nulls distinct_count avg_col_len max_col_len num_trues num_falses commentpage_url string from deserializerTime taken: 0.043 seconds, Fetched: 3 row(s) desc extended desc [extended] [db_name.]table_name[.col_name];123456789101112131415161718192021222324252627282930Example:hive&gt; desc extended page_view_5 userdata;OKuserdata map&lt;string,string&gt; from deserializerTime taken: 0.03 seconds, Fetched: 1 row(s)hive&gt; desc extended page_view_5 userdata.$key$;OK$key$ string from deserializerTime taken: 0.029 seconds, Fetched: 1 row(s)hive&gt; desc extended page_view_5 userdata.$value$;OK$value$ string from deserializerTime taken: 0.03 seconds, Fetched: 1 row(s)hive&gt; desc formatted page_view_6;OK# col_name data_type commentid bigintdata struct&lt;page_url:string,ip:string,status:string&gt;userdata map&lt;string,string&gt;countlist array&lt;int&gt;hive&gt; desc extended page_view_6 countlist.$elem$;OK$elem$ int from deserializerTime taken: 0.031 seconds, Fetched: 1 row(s)hive&gt; desc extended page_view_6 data.ip;OKip string from deserializerTime taken: 0.064 seconds, Fetched: 1 row(s) Drop Tabledrop table drop table [db_name.]table_name;1234567891011121314151617Example:hive&gt; show tables from test like &quot;pv*&quot;;OKpv_3pv_4pv_4_likepv_4_newTime taken: 0.017 seconds, Fetched: 4 row(s)hive&gt; drop table test.pv_3;OKTime taken: 1.598 secondshive&gt; show tables from test like &quot;pv*&quot;;OKpv_4pv_4_likepv_4_newTime taken: 0.018 seconds, Fetched: 3 row(s) truncate table truncate table [db_name.]table_name;1234567891011121314151617181920212223Example:hive&gt; select * from test.pv_4;OK1 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.1\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;100\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;001\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_001\\&quot;&quot;,&quot;viewcnt&quot;:100&#125; taipei taiwan2 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.2\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;200\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;002\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_002\\&quot;&quot;,&quot;viewcnt&quot;:200&#125; taipei taiwan3 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.3\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;300\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;003\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_003\\&quot;&quot;,&quot;viewcnt&quot;:300&#125; taipei taiwan4 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.4\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;400\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;004\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_004\\&quot;&quot;,&quot;viewcnt&quot;:400&#125; taipei taiwan5 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.5\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;500\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;005\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_005\\&quot;&quot;,&quot;viewcnt&quot;:500&#125; taipei taiwan6 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.6\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;600\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;006\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_006\\&quot;&quot;,&quot;viewcnt&quot;:600&#125; taipei taiwan7 &#123;&quot;page_url&quot;:&quot;\\&quot;http://www.google.com\\&quot;&quot;,&quot;ip&quot;:&quot;\\&quot;192.168.1.7\\&quot;&quot;,&quot;status&quot;:&quot;\\&quot;700\\&quot;&quot;&#125; &#123;&quot;id&quot;:&quot;\\&quot;007\\&quot;&quot;,&quot;name&quot;:&quot;\\&quot;test1_007\\&quot;&quot;,&quot;viewcnt&quot;:700&#125; taipei taiwanTime taken: 0.071 seconds, Fetched: 7 row(s)hive&gt; truncate table test.pv_4;OKTime taken: 0.124 secondshive&gt; select * from test.pv_4;OKTime taken: 0.063 secondshive&gt; show tables from test like &quot;pv*&quot;;OKpv_4pv_4_likepv_4_newTime taken: 0.018 seconds, Fetched: 3 row(s) multi drop table 123456789101112Example:hive&gt; show tables from test like &quot;pv*&quot;;OKpv_4pv_4_likepv_4_newTime taken: 0.017 seconds, Fetched: 3 row(s)[hadoop@mna1 ~]$ hive -e &quot;show tables from test like &apos;pv*&apos;&quot; |xargs -I &apos;&#123;&#125;&apos; hive -e &quot;drop table test.&#123;&#125;&quot;hive&gt; show tables from test like &quot;pv*&quot;;OKTime taken: 1.548 seconds","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hive","slug":"BigData/Hive","permalink":"http://yoursite.com/categories/BigData/Hive/"}],"tags":[]},{"title":"透過vagrant打包一個VM Box","slug":"[vagrant]打造一個自己開發境境","date":"2017-07-18T16:00:00.000Z","updated":"2017-07-18T07:15:28.059Z","comments":true,"path":"2017/07/19/[vagrant]打造一個自己開發境境/","link":"","permalink":"http://yoursite.com/2017/07/19/[vagrant]打造一個自己開發境境/","excerpt":"","text":"Download &amp; Install Virtual BoxDownload Virtual Box Install EXE file Install VirtualBox Modify Virtual Box Preferences點選”File”–&gt;”Preferences”修改預設機器資料夾位置 DownLoad &amp; Install VagrantDownload vagrant_1.9.1.msi Install vagrant Setting VAGRANT_HOME Create VM DownLoad Ubuntu iso &amp; Install Ubuntu LinuxDownload ubuntu-17.04-server-amd64.iso Install Linux Server restart Linux ServerSetting Ubuntu LinuxModify LocalesModify visudoCreate SSH KeyInstall VBoxGuestAdditionvagrant package &amp; addCreate vagrantfile(vagrant init)vagrant up","categories":[{"name":"vagrant","slug":"vagrant","permalink":"http://yoursite.com/categories/vagrant/"}],"tags":[]},{"title":"Spark程式開發系列_for Java1.8_001","slug":"[spark]spark_001","date":"2017-05-21T16:00:00.000Z","updated":"2020-07-22T08:36:02.769Z","comments":true,"path":"2017/05/22/[spark]spark_001/","link":"","permalink":"http://yoursite.com/2017/05/22/[spark]spark_001/","excerpt":"","text":"SparkSession Example 1 使用vi,或先用記事本建立以下json檔案(我的是建立在/home/vagrant/src/job/person5_data.json) 12&#123;&quot;id&quot;:&quot;0000000001&quot;,&quot;name&quot;:&quot;Test001&quot;,&quot;gender&quot;:&quot;M&quot;,&quot;age&quot;:&quot;40&quot;,&quot;telphone&quot;:&quot;123456789&quot;,&quot;address&quot;:&#123;&quot;country&quot;:&quot;Taiwan&quot;,&quot;city&quot;:&quot;Taipei&quot;,&quot;state&quot;:&quot;Sanxia&quot;,&quot;street&quot;:&quot;test.&quot;&#125;,&quot;child&quot;:[&#123;&quot;age&quot;:&quot;5&quot;,&quot;name&quot;:&quot;sub_001&quot;,&quot;gender&quot;:&quot;M&quot;&#125;,&#123;&quot;age&quot;:&quot;6&quot;,&quot;name&quot;:&quot;sub_002&quot;,&quot;gender&quot;:&quot;M&quot;&#125;]&#125;,&#123;&quot;id&quot;:&quot;0000000002&quot;,&quot;name&quot;:&quot;Test002&quot;,&quot;gender&quot;:&quot;F&quot;,&quot;age&quot;:&quot;29&quot;,&quot;telphone&quot;:&quot;223456670&quot;,&quot;address&quot;:&#123;&quot;country&quot;:&quot;Taiwan&quot;,&quot;city&quot;:&quot;Taipei&quot;,&quot;state&quot;:&quot;Sanzhi&quot;,&quot;street&quot;:&quot;test.&quot;&#125;,&quot;child&quot;:[&#123;&quot;age&quot;:&quot;7&quot;,&quot;name&quot;:&quot;sub_003&quot;,&quot;gender&quot;:&quot;M&quot;&#125;,&#123;&quot;age&quot;:&quot;8&quot;,&quot;name&quot;:&quot;sub_004&quot;,&quot;gender&quot;:&quot;F&quot;&#125;]&#125; 將person5_data.json檔案放到HDFS目錄上 1hadoop fs -put -f /home/vagrant/src/job/person5_data.json /user/hadoop/data Create SparkSession 123456789101112131415161718192021222324252627282930import org.apache.spark.sql.AnalysisException;import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;import org.apache.spark.sql.SaveMode;import org.apache.spark.sql.SparkSession;import org.apache.spark.sql.catalog.Database;public class SparkExample0 &#123; public static void main(String[] args) throws AnalysisException &#123; //1. Create SparkSession SparkSession spark = SparkSession.builder() .master(&quot;local&quot;) .config(&quot;spark.executor.memory&quot;, &quot;2g&quot;) .appName(&quot;SparkExample0&quot;) .enableHiveSupport() .getOrCreate(); //2. set new runtime options spark.conf().set(&quot;spark.sql.shuffle.partitions&quot;, 6); //3. SparkSession-透過read方式來讀取各種不同來源的資料 Dataset&lt;Row&gt; ds1 = spark.read().format(&quot;json&quot;).load(&quot;/user/hadoop/data/person5_data.json&quot;); ds1.printSchema(); //4. 將檔案存成Hive資料表 ds1.repartition(1).write().mode(SaveMode.Overwrite).saveAsTable(&quot;example.person_data5&quot;); &#125;&#125; 使用spark-submit執行Java Spark程式 1spark-submit --class &quot;SparkExample0&quot; /home/vagrant/src/job/Spark2Example.jar","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Spark","slug":"BigData/Spark","permalink":"http://yoursite.com/categories/BigData/Spark/"}],"tags":[]},{"title":"Vagrant file 配置檔案說明","slug":"[vagrant]Vagrantfile配置檔說明","date":"2017-02-19T16:00:00.000Z","updated":"2017-02-20T06:02:47.965Z","comments":true,"path":"2017/02/20/[vagrant]Vagrantfile配置檔說明/","link":"","permalink":"http://yoursite.com/2017/02/20/[vagrant]Vagrantfile配置檔說明/","excerpt":"","text":"vagrant file配置檔案說明 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# -*- mode: ruby -*-# vi: set ft=ruby :MASTER_IP = &apos;192.168.51.4&apos;DN1_IP = &apos;192.168.51.5&apos;DN2_IP = &apos;192.168.51.6&apos;Vagrant.configure(&quot;2&quot;) do |config| config.ssh.insert_key =false #define data1 server config.vm.define &quot;slave1&quot; do |slave1| slave1.vm.hostname = &quot;hadoop-slave1&quot; ##設定VM主機名稱 slave1.vm.network &quot;private_network&quot;, ip: DN1_IP ##設定主機 ip slave1.vm.box = &quot;ubuntu/yakkety64&quot; ##設定VM使用的OS Box slave1.vm.synced_folder &quot;.&quot;, &quot;/home/vagrant/src&quot;, mount_options: [&quot;dmode=775,fmode=664&quot;] ##設定公用分享目錄 slave1.vm.provider &quot;virtualbox&quot; do |v| v.name = &quot;slave1&quot; v.cpus = 1 v.memory = 2500 end slave1.vm.provision &quot;shell&quot;, path: &quot;bootstrap-slave.sh&quot; ##設定VM第一次被啟動時,所需執行的Shell,且使用root帳號執行 end #define data2 server config.vm.define &quot;slave2&quot; do |slave2| slave2.vm.hostname = &quot;hadoop-slave2&quot; ##設定VM主機名稱 slave2.vm.network &quot;private_network&quot;, ip: DN2_IP ##設定主機 ip slave2.vm.box = &quot;ubuntu/yakkety64&quot; ##設定VM使用的OS Box slave2.vm.synced_folder &quot;.&quot;, &quot;/home/vagrant/src&quot;, mount_options: [&quot;dmode=775,fmode=664&quot;] ##設定公用分享目錄 slave2.vm.provider &quot;virtualbox&quot; do |v| v.name = &quot;slave2&quot; ##設定VM名稱 v.cpus = 1 ##設定VM所使用的CPU core數量 v.memory = 2500 ##設定VM記憶體大小 end slave2.vm.provision &quot;shell&quot;, path: &quot;bootstrap-slave.sh&quot; ##設定VM第一次被啟動時,所需執行的Shell,且使用root帳號執行 end #define Master server config.vm.define &quot;master&quot; do |master| master.vm.hostname = &quot;hadoop-master&quot; ##設定VM主機名稱 master.vm.network &quot;private_network&quot;, ip: MASTER_IP ##設定主機 ip master.vm.box = &quot;ubuntu/yakkety64&quot; ##設定VM使用的OS Box master.vm.synced_folder &quot;.&quot;, &quot;/home/vagrant/src&quot;, mount_options: [&quot;dmode=775,fmode=664&quot;] ##設定公用分享目錄 master.vm.provider &quot;virtualbox&quot; do |v| v.name = &quot;master&quot; ##設定VM名稱 v.cpus = 1 ##設定VM所使用的CPU core數量 v.memory = 3500 ##設定VM記憶體大小 end master.vm.provision &quot;shell&quot;, path: &quot;bootstrap-master.sh&quot; ##設定VM第一次被啟動時,所需執行的Shell,且使用root帳號執行 #master.vm.provision &quot;shell&quot;, path: &quot;bootstrap-complete.sh&quot;, run: &quot;always&quot; ##每次啟動VM時,都必須執行的Shell,且使用root帳號執行 #master.vm.provision &quot;shell&quot;, path: &quot;bootstrap-complete.sh&quot;, run: &quot;always&quot;,privileged: false ##設定VM第一次被啟動時,所需執行的Shell,且使非root帳號執行 end end","categories":[{"name":"vagrant","slug":"vagrant","permalink":"http://yoursite.com/categories/vagrant/"}],"tags":[]},{"title":"Vagrant Command Line說明筆記","slug":"[vagrant]VagrantMemo","date":"2017-02-19T16:00:00.000Z","updated":"2017-07-18T05:24:19.802Z","comments":true,"path":"2017/02/20/[vagrant]VagrantMemo/","link":"","permalink":"http://yoursite.com/2017/02/20/[vagrant]VagrantMemo/","excerpt":"","text":"vagrant package 1vagrant package --base &lt;vm_name&gt; --output &lt;box_name&gt;.box vagrant box 1234vagrant box add ubuntu/precise64 ##https://atlas.hashicorp.com/vagrant box add precise64 http://files.vagrantup.com/precise64.boxvagrant box listvagrant box remove precise64 vagrant init 1vagrant init precise64 ##會在執行語法的目錄下產生一個Vagrantfile的配置檔 vagrant 啟動/停止/重啟/移除 全部VM 1234567891011121314151617##以下為啟動/停止/重啟/移除全部VM的用法vagrant up ##啟動全部VMvagrant halt ##停止全部VMvagrant reload ##重啟全部VMvagrant destroy ##移除全部VM##以下為啟動/停止/重啟/移除單一VM的用法vagrant up mastervagrant halt mastervagrant reload mastervagrant destroy master##以下為啟動/停止/重啟/移除多個VM的用法(VM名稱中間空格,以做為多個VM名稱區隔)vagrant up master slave1vagrant halt master slave1vagrant reload master slave1vagrant destroy master slave1 other command 1234vagrant versionvagrant statusvagrant ssh-config &lt;vm_name&gt;vagrant ssh","categories":[{"name":"vagrant","slug":"vagrant","permalink":"http://yoursite.com/categories/vagrant/"}],"tags":[]},{"title":"Hadoop常用命令說明","slug":"[hadoop]Hadoop_CMD","date":"2017-01-09T16:00:00.000Z","updated":"2020-07-22T08:26:58.494Z","comments":true,"path":"2017/01/10/[hadoop]Hadoop_CMD/","link":"","permalink":"http://yoursite.com/2017/01/10/[hadoop]Hadoop_CMD/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647hadoop fs -mkdir /user/hadoop/data hadoop fs -mkdir -p /user/hadoop/data ##連Partants Directories建立hadoop fs -ls -h[-d] /user/hadoophadoop fs -lsr /user/hadoophadoop fs -put /opt/staff.csv /user/hadoop/data ##上傳檔案至HDFS,不會overwritehadoop fs -put -f /opt/staff.csv /user/hadoop/data ##上傳檔案至HDFS,overwrite當前檔案hadoop fs -df -h /user/hadoophadoop fs -du -h /user/hadoophadoop fs -dus /user/hadoophadoop fs -Ddfs.replication=4 -put /opt/staff.csv /user/hadoop/datahadoop fs -setrep 10 /user/hadoop/data/staff.csvhadoop fs -mkdir -p /user/hadoop/ccchadoop fs -touchz /user/hadoop/ccc/test.xml ##建立一個空檔案hadoop fs -cat /user/hadoop/ccc/test.xmlhadoop fs -rm /user/hadoop/ccc/test.xml ##刪除HDFS上的檔案hadoop fs -rmdir /user/hadoop/ccc ##刪除HDFS上的空目錄hadoop fs -rmr /user/hadoop/ccc ##刪除指定目錄下所有的檔案及目錄hadoop fs -copyFromLocal /opt/staff.csv /user/hadoop/ccc ##檔案上傳到HDFShadoop fs -copyToLocal /user/hadoop/ccc/staff.csv /opt/staff_download.csv ##檔案下載到/opthadoop fs -moveFromLocalhadoop fs -moveToLocalhadoop fsck /user/hadoop/data/staff.csv -files -blocks -locationshadoop fsck -blockId blk_&lt;id&gt;hadoop dfsadmin -safemode enter ##進入安全模式hadoop dfsadmin -safemode leave ##離開安全模式hadoop dfsadmin -safemode get ##取得目前狀態hadoop dfsadmin -safemode wait hadoop dfsadmin -reporthadoop dfsadmin -refreshNodeshadoop dfsadmin -printTopologyhadoop namenode -format ##格式化namenode,在HDFS系統上的資料會全部刪除","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"Hadoop Cluster-動態新增一個節點","slug":"[hadoop]CreatNewDataNode","date":"2017-01-09T16:00:00.000Z","updated":"2020-07-22T08:24:54.640Z","comments":true,"path":"2017/01/10/[hadoop]CreatNewDataNode/","link":"","permalink":"http://yoursite.com/2017/01/10/[hadoop]CreatNewDataNode/","excerpt":"","text":"hadoop cluster Run了一段時間後,想要增加運算速度,因此想動態新增一個DataNode1.動態新增一個DataNode 123456789101112131415161718##新主機請安裝以下1. 安裝Linux OS2. 在/etc/hosts加入所有Hadoop cluster節點的hostname3. 進入NameNode下執行 ssh-copy-id hadoop@hadoop-slave5 scp hadoop-bin-2.7.2.tar.gz hadoop@hadoop-slave5:/bgdt/install_src4. 進入新增的DataNode5. tar -zxvf jdk-8u101-linux-x64.tar.gz -C /bgdt/java6. tar -zxvf hadoop-bin-2.7.2.tar.gz -C /bgdt7. Modify ~/.bashrc8. scp 相關設定檔 scp /bgdt/hadoop-2.7.2/etc/hadoop/hdfs-site.xml hadoop@hadoop-slave5:/bgdt/hadoop-2.7.2/etc/hadoop/hdfs-site.xml scp /bgdt/hadoop-2.7.2/etc/hadoop/core-site.xml hadoop@hadoop-slave5:/bgdt/hadoop-2.7.2/etc/hadoop/core-site.xml scp /bgdt/hadoop-2.7.2/etc/hadoop/mapred-site.xml hadoop@hadoop-slave5:/bgdt/hadoop-2.7.2/etc/hadoop/mapred-site.xml scp /bgdt/hadoop-2.7.2/etc/hadoop/yarn-site.xml hadoop@hadoop-slave5:/bgdt/hadoop-2.7.2/etc/hadoop/yarn-site.xml scp /bgdt/hadoop-2.7.2/etc/hadoop/slaves hadoop@hadoop-slave5:/bgdt/hadoop-2.7.2/etc/hadoop/slaves9. hadoop-daemon.sh start datanode10. 觀察Web UI是否出現slave5 DataNode","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"Hadoop資料儲存於多個目錄中的模擬測試","slug":"[hadoop]hadoopMultiDirectoryStore","date":"2017-01-09T16:00:00.000Z","updated":"2020-07-22T08:31:24.027Z","comments":true,"path":"2017/01/10/[hadoop]hadoopMultiDirectoryStore/","link":"","permalink":"http://yoursite.com/2017/01/10/[hadoop]hadoopMultiDirectoryStore/","excerpt":"","text":"環境說明 12namenode *1(for Linux OS)datanode *4(for Linux OS) 相關設定說明 1234567891011mkdir -p /bgdt/data/data2##修改hdfs-site.xml&lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/bgdt/hadoop-2.7.2/tmp/dfs/data,/bgdt/data/data2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.failed.volumes.tolerated&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 進入劇本之前 123456##重啟dfs與yarnstart-dfs.shstart-yarn.sh##並下達以下指令:hadoop balancer 劇本1 系統Run了一段時間後,發現空間不夠了,想增加一個空間給目前的Hadoop cluster環境,因此有了以下的劇本與想法1.用新加一個目錄,來取代模擬新分割區2.copy 一個檔案至HDFS系統後,透過介面查詢相關block id,找出相對實體檔案的儲存位置 12345678910111213hadoop fs -mkdir -P /user/hadoop/datahadoop fs -put /opt/staff.csv /user/hadoop/data##打開Web UI觀察 /user/hadoop/data/staff.csv block Id=XXXXXX在檔案下載畫面中會有檔案相關資訊block Id: Avaailabledna1dna3##確認block Id,與存放的Datanode位置後,直接查找存放的實體位置ssh dna1 &quot;find /bgdt -name &quot;blk_&lt;ID&gt;&quot;&quot;/bgdt/data/data2/current/BP-1008347755-192.168.11.96-1474254342729/current/finalized/subdir0/subdir0/blk_XXXXXX 劇本2 將某個Datanode的某個目錄權限及owner設定成(chown root:root chmod 000),模擬硬碟損壞的狀況 1.測試當某個正在運行的hadoop cluster節點的儲存目錄被設為無讀寫權限時,系統是否會Crash2.當目錄修復完成後資料是否有辦法從其他的node,reconvert回來3.當該某個正在運行的節點,所有目錄都無法讀取時,系統是否會正常運行1234567891011121314151617181920212223242526272829303132333435##將測試檔案存放到HDFS,並設定副本數為4hadoop fs -Ddfs.replication=4 -put /opt/staff_1.csv /user/hadoop/data##查詢block Id 及block儲存位置ssh dna1 &quot;find /bgdt -name &quot;blk_&lt;ID&gt;&quot;&quot;##模擬故障sudo chown -R root:root /bgdt/hadoop-2.7.2/tmpsudo chmod -R 000 /bgdt/hadoop-2.7.2/tmphadoop balancer##Web UI上Datanode Volume Failures會出現有問題的目錄##balencer執行完成後,會將原本在/tmp目錄下的資料搬移到 /bgdt/data/data2##此時若再把另外一個目錄也模擬成無權限狀態sudo chown -R root:root /bgdt/datasudo chmod -R 000 /bgdt/data##該node的Status會變成Dead,並且DataNode的process會停止##此時資料的Repl雖然一樣是4,但實際上的資料份數是3##接著將/bgdt/data目錄設回原本權限,並將資料刪除sudo chown -R hadoop:hadoop /bgdt/datasudo chmod -R 755 /bgdt/datarm -rf /bgdt/data##並將/bgdt/data目錄的資料刪除後,重啟datanodehadoop-daemon.sh start datanode##重啟成功後,進行balancerhadoop balancer##看到所有檔案的副本數與與資料又恢復正常","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"Hadoop Cluster相關環境建置筆記(持續修正中......)","slug":"[Install]BigData","date":"2017-01-08T10:00:00.000Z","updated":"2020-07-22T08:34:56.174Z","comments":true,"path":"2017/01/08/[Install]BigData/","link":"","permalink":"http://yoursite.com/2017/01/08/[Install]BigData/","excerpt":"","text":"安裝前準備停止iptables服務 1234## 先使用root登入Linux OSservice iptables stop #停止iptables服務vi /etc/rc.localservice iptables stop #停止iptables服務,存檔後離開 新增hadoop User,並賦予hadoop root的權限 123456adduser hadooppasswd hadoopvisudohadoop ALL=(ALL) ALL #加入這一行,存檔後離開(以上步驟每台電腦都要做) 進行SSH相關的設定,透過ssh做無密碼連線測試 123456789su - hadoopssh-keygenssh-copy-id hadoop@mna1 ##別忘記自己也要copyssh-copy-id hadoop@dna1 ##有幾台datanode就要做幾次ssh-copy-id hadoop@dna2 ##有幾台datanode就要做幾次##ssh連線測試ssh mna1ssh dna1 系統時間同步 1sudo date [MMddhhmm.SS];hwclocl -w 建立安裝目錄 123sudo mkdir -p /bgdt/install_srcsudo mkdir -p /bgdt/javasudo chown -R hadoop:hadoop /bgdt 將相關的安裝檔案上傳到 hadoop-master /bget/install_src目錄下 12345678910使用WinSCP上傳比較方便jdk-8u101-linux-x64.tar.gzhadoop-2.7.2.tar.gzspark-2.0.0-bin-hadoop2.7.tgzapache-hive-2.1.0-bin.tar.gzapache-hive-2.1.0-src.tar.gzAnaconda3-4.2.0-Linux-x86_64.shoozie-4.3.0-distro.tar.gzmysql-connector-java-5.1.39-bin.jarext-2.2.zip 安裝JDK 123456789101112step1. [CMD]tar -zxvf /bgdt/jdk-8u101-linux-x64.tar.gz -C /usr/localstep2. [CMD]vi ~/.bashrc加入以下兩行export JAVA_HOME=/usr/local/jdk1.8.0_101export PATH=$JAVA_HOME/bin:$PATHstpe3. [CMD]source ~/.bashrcstep4. [CMD]java -version(以上步驟在每台Datanode上也需要安裝喔!!) Hadoop Cluster安裝(2.7.2),每台電腦皆要安裝解壓縮並設定相關環境變數 123456789101112step1. [CMD]tar -zxvf /bgdt/hadoop-2.7.2.tar.gz -C /bgdtstep2. [CMD]vi ~/.bashrc加入以下變數export HADOOP_HOME=/bgdt/hadoop-2.7.2export HADOOP_CONF_DIR=/bgdt/hadoop-2.7.2/etc/hadoopexport CLASSPATH=$CLASSPATH:$HADOOP_HOME/lib/*:.export CLASSPATH=$CLASSPATH:$HADOOP_HOME/share/hadoop/common/*:.export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATHstep3. [CMD]source ~/.bashrcstep4. [CMD]java -version 修改設定檔修改設定檔(/bgdt/hadoop-2.7.2/etc/hadoop/core-site.xml) 123456789101112131415161718&lt;configuration&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mna1:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/bgdt/hadoop-2.7.2/tmp&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改設定檔(/bgdt/hadoop-2.7.2/etc/hadoop/hdfs-site.xml) 12345678910111213141516171819202122&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;mna1:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/bgdt/hadoop-2.7.2/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/bgdt/hadoop-2.7.2/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.block.size&lt;/name&gt; &lt;value&gt;64M&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改設定檔(/bgdt/hadoop-2.7.2/etc/hadoop/mapred-site.xml) 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;mna1:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;mna1:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改設定檔(/bgdt/hadoop-2.7.2/etc/hadoop/yarn-site.xml) 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;mna1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://mna1:19888/jobhistory/logs&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 修改設定檔(/bgdt/hadoop-2.7.2/etc/hadoop/slaves) 1234dna1dna2dna3dna4 修改設定檔(/bgdt/hadoop-2.7.2/etc/hadoop/capacity-scheduler.xml) 1234567僅修改以下屬性值(0.1--&gt;0.5).......&lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;/name&gt; &lt;value&gt;0.5&lt;/value&gt; &lt;/property&gt;...... 將檔案分送到其他Datanode 12345678910##將目錄壓縮tar -zcf /bgdt/hadoop.master.tar.gz /bgdt/hadoop-2.7.2##將壓縮後的檔案,透過scp指令傳送到其他主機(Datanode)scp /bgdt/hadoop.master.tar.gz dna1:/bgdt/install_srcscp /bgdt/hadoop.master.tar.gz dna2:/bgdt/install_srcscp /bgdt/hadoop.master.tar.gz dna3:/bgdt/install_srcscp /bgdt/hadoop.master.tar.gz dna4:/bgdt/install_src##各個Datanode將Hadoop解壓縮至/bgdt,解完後設定相關Haoop變數如第一大項 格式化HDFS系統 12hadoop namenode -formathadoop dfsadmin -report 啟動dfs與yarn 1234567##啟動服務start-dfs.shstart-yarn.sh##關閉服務stop-yarn.shstop-dfs.sh 單台Datanode啟動程式 123ssh dna1/bgdt/hadoop-2.7.2/sbin/hadoop-daemon.sh start datanode/bgdt/hadoop-2.7.2/sbin/yarn-daemon.sh start nodemanager HDFS WebUI (hadoop-master:50070) yarn WebUI (hadoop-master:8088) Spark安裝(2.0.0),僅安裝於NameNodeDownload spark 2.0.0安裝程式 下載後會產生一個 “spark-2.0.0-bin-hadoop2.7.tgz”檔案 或使用wget下載,下載前請先確定是否可以連網路囉!! 12wget -P \\/bgdt/install_src http://archive.apache.org/dist/spark/spark-2.0.0/spark-2.0.0-bin-hadoop2.7.tgz 解壓縮下載檔案 12tar -zxvf /bgdt/install_src/spark-2.0.0-bin-hadoop2.7.tgz -C /bgdtmv /bgdt/spark-2.0.0-bin-hadoop2.7 /bgdt/spark-2.0.0 ##目錄名稱太長了,所以把目錄名稱改短 修改~/.bashrc 1234export SPARK_HOME=/bgdt/spark-2.0.0將檔案儲存後,記得下source,讓設定生效source ~/.bashrc 測試(spark-shell)進入spark-shell CLI 12345##進入spark-shell/bgdt/spark-2.0.0/bin/sparl-shell............scala&gt; Spark Scala Shell Test 123[CTRL+l] &lt;--清除shell畫面scala&gt;scscala&gt;spark Spark Example for Scala-建立檔案內容並上傳至HDFS 12345678hadoop fs -mkdir -p /user/hadoop/cccecho &quot;1,\\&quot;test1\\&quot;,10000&quot; &gt;&gt; test1.csvecho &quot;2,\\&quot;test2\\&quot;,20000&quot; &gt;&gt; test1.csvecho &quot;3,\\&quot;test3\\&quot;,30000&quot; &gt;&gt; test1.csvecho &quot;4,\\&quot;test4\\&quot;,40000&quot; &gt;&gt; test1.csvecho &quot;5,\\&quot;test5\\&quot;,50000&quot; &gt;&gt; test1.csvcat test1.csvhadoop fs -put ~/test1.csv /user/hadoop/ccc Spark Example for Scala-spark for scala程式撰寫 123scala&gt;val distFile = sc.textFile(&quot;/user/hadoop/ccc/test1.csv&quot;)scala&gt;distFile.count()scala&gt;distFile.collect() 範例執行結果 測試(spark-submit)啟動Spark History Server(Port:18080) 12hadoop fs -mkdir -p /tmp/history$SPARK_HOME/sbin/start-history-server.sh hdfs://hadoop-master:8020/tmp/history 啟動Job History Server(Port:19888) 1mr-jobhistory-daemon.sh start historyserver MariaDB安裝(10.1.17),僅安裝於NameNode,目前須使用root權限安裝下載MariaDB安裝檔(目前最新版本為10.1.20)Download mariadb-10.1.17-linux-x86_64 關閉iptables service 123先以root權限登入系統並關閉iptables服務service iptables stopservice iptables status 建立mysql帳號 12groupadd mysqluseradd -g mysql mysql 安裝MariaDB 1234567891011121314151617cd /usr/localtar -zxvf /bgdt/install_src/mariadb-10.1.17-linux-x86_64.tar.gz -C /usr/localln -s mariadb-10.1.17-linux-x86_64 mysqlcd mysql./scripts/mysql_install_db --user=mysqlchown -R root .chown -R mysql datacd /usr/local/mysql./bin/mysqld --user=root &amp;cat &gt; /etc/rc.local &lt;&lt;EOF#!/bin/sh -ecd $&#123;mariadb_install_dist&#125;/mysql./bin/mysqld --user=root &amp;exit 0EOF&#125; 啟動MariaDB(修正中…) 1234nohup ./bin/mysqld --user=root &amp;./bin/mysqladmin -u root password &apos;!QAZxsw2&apos; --socket=/var/lib/mysql/mysql.sock ./bin/mysql -p --socket=/var/lib/mysql/mysql.sock輸入密碼:!QAZxsw2 設定權限 1234mariadb&gt;create database metastore_db;mariadb&gt;SELECT User, Host FROM mysql.user WHERE Host &lt;&gt; &apos;localhost&apos;; mariadb&gt;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;!QAZxsw2&apos; WITH GRANT OPTION;mariadb&gt;FLUSH PRIVILEGES; 確認”3306” Port是否有開啟？並且關閉確認關閉iptables Hive安裝(2.1.1),僅安裝於NameNode安裝/設定/啟動 Hive CLI下載 Hive安裝檔/原始碼Download Hive 安裝檔,apache-hive-2.1.1-binDownload Hive 原始碼,apache-hive-2.1.1-src 解壓縮檔案 12tar -zxvf /bgdt/install_src/apache-hive-2.1.1-bin.tar.gz -C /bgdtmv /bgdt/apache-hive-2.1.1-bin /bgdt/hive-2.1.1 修改~/.bashrc 12345export HIVE_HOME=/bgdt/hive-2.1.1export PATH= $HIVE_HOME\\bin:$PATH儲存檔案source ~/.bashrc 將mysql-connector-java-5.1.39-bin.jar copy to /bgdt/hive-2.1.1/lib目錄下建立Hive Warehouse 12hadoop fs -mkdir -p /user/hive/warehousehadoop fs -mkdir -p /tmp/hive Modify hive-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.XXX.XXX:3306/metastore_db&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;XXXXXXXXX&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt; &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: $&#123;hive.exec.scratchdir&#125;/&amp;lt;username&amp;gt; is created, with $&#123;hive.scratch.dir.permission&#125;.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/tmp&lt;/value&gt; &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/tmp&lt;/value&gt; &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.scratch.dir.permission&lt;/name&gt; &lt;value&gt;775&lt;/value&gt; &lt;description&gt;The permission for the user specific scratch directories that get created.&lt;/description&gt;&lt;/property&gt; create metastore_db 相關Table 12##此動作會在mariadb的metastore_db中建立Hive用到的系統表格(以下動作會將meta資料重建)schematool -dbType mysql -initSchema 在命令列中執行Hive CLI 1234hive............hive&gt; 在hive的CLI下建立Database及Table 12345hive&gt;create database test1;hive&gt;create table test1.staff(id int, name string, salary double) hive&gt;row format delimited fields terminated by &apos;,&apos;;hive&gt;LOAD DATA INPATH &apos;/user/hadoop/ccc/test1.csv&apos; overwrite into table test1.staff;hive&gt;select * from test1.staff; 以上命令執行結果 HWI安裝與設定(選裝)將已下載的Hive Souce code解壓縮至/bgdt 1234tar -zxvf /bgdt/install_src/apache-hive-2.1.0-src.tar.gz -C /bgdtcd /bgdt/apache-hive-2.1.0-src/hwijar cfM hive-hwi-2.1.0.war -C web .cp hive-hwi-2.1.0.war /bgdt/hive-2.1.0/lib/* 修改hive-site.xml 123456789101112131415&lt;property&gt; &lt;name&gt;hive.hwi.listen.host&lt;/name&gt; &lt;value&gt;hadoop-master&lt;/value&gt; &lt;description&gt;This is the host address the Hive Web Interface will listen on&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.hwi.listen.port&lt;/name&gt; &lt;value&gt;9999&lt;/value&gt; &lt;description&gt;This is the port the Hive Web Interface will listen on&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.hwi.war.file&lt;/name&gt; &lt;value&gt;lib/hive-hwi-2.1.0.war&lt;/value&gt; &lt;description&gt;This sets the path to the HWI war file, relative to $&#123;HIVE_HOME&#125;. &lt;/description&gt;&lt;/property&gt; Start HWI Service 123##HWI啟動Port(9999)nohup hive --service hwi &amp;netstat -tnl | grep &quot;9999&quot; HWI介面(請連到http://hadoop-master:9999/hwi) Hive for BeelineStart hiveserver2123##啟動Hive JDBC Port(port:10000)nohup hive --service hiveserver2 &amp;netstat -tnl | grep &quot;10000&quot; 使用beeline連線Hive(須連線HiveServer2)123beelinebeeline&gt; !connect jdbc:hive2://192.168.11.96:10000/default OOZIE安裝(4.3.0),僅安裝於NameNodeOOZIE安裝與設定從Github下載oozie原始碼,編譯oozie,並產生”oozie-4.3.0.tar.gz”檔案解壓縮oozie-4.3.0.tar.gz並變更目錄名稱 12sudo tar -zxvf /bgdt/install_src/oozie-4.3.0-distro.tar.gz -C /bgdtsudo chown -R hadoop:hadoop /bgdt/oozie-4.3.0 搬移相關lib至libext目錄 12345mkdir -p /bgdt/oozie-4.3.0/libextcp /bgdt/hadoop-2.7.2/share/hadoop/*/*.jar /bgdt/oozie-4.3.0/libext/cp /bgdt/hadoop-2.7.2/share/hadoop/*/lib/*.jar /bgdt/oozie-4.3.0/libext/rm -rf /bgdt/oozie-4.3.0/libext/jsp-api-2.1.jarcp /bgdt/install_src/ext-2.2.zip /bgdt/oozie-4.3.0/libext 修改oozie-site.xml 123456789101112131415161718192021cat &gt; /bgdt/oozie-4.3.0/conf/oozie-site.xml &lt;&lt;EOF&lt;?xml version=&quot;1.0&quot;?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;oozie.service.WorkflowAppService.system.libpath&lt;/name&gt; &lt;value&gt;/user/hadoop/share/lib&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;oozie.service.HadoopAccessorService.hadoop.configurations&lt;/name&gt; &lt;value&gt;*=/bgdt/hadoop-2.7.2/etc/hadoop&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;oozie.use.system.libpath&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;oozie.service.SparkConfigurationService.spark.configurations&lt;/name&gt; &lt;value&gt;*=/bgdt/spark-2.0.0/conf&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;EOF oozie4.3.0版需要再額外處理oozie.war檔中有hadoop-2.6相關jar檔衝突問題(在執行oozie時,會產生noSuchFile HAOOP_CLASSPATH的錯誤訊息) 12345#cd $&#123;install_dist&#125;/oozie-$&#123;OOZIE_VERSION&#125;/oozie#/bgdt/java/jdk1.8.0_101/bin/jar -xvf ../oozie.war#rm -rf ./WEB-INF/lib/hadoop-*-2.6.0.jar#/bgdt/java/jdk1.8.0_101/bin/jar cvf ../oozie.war *.* .#rm -rf $&#123;install_dist&#125;/oozie-$&#123;OOZIE_VERSION&#125;/oozie 安裝相關Server,createdb 12345cd /bgdt/oozie-4.3.0./bin/oozie-setup.sh prepare-war./bin/oozie-setup.sh db create -run./bin/oozied.sh start./bin/oozie admin -oozie http://localhost:11000/oozie -status OOZIE WEB UI: 1http://hadoop-master:11000/oozie 安裝sharelib複製相關jar檔 123456789cd /bgdt/oozie-4.3.0tar -zxvf oozie-sharelib-4.3.0-SNAPSHOT.tar.gzrm -rf /bgdt/oozie-4.3.0/share/lib/sparkmkdir -p /bgdt/oozie-4.3.0/share/lib/sparkcp /bgdt/spark/jars/* /bgdt/oozie-4.3.0/share/lib/sparkcp oozie-sharelib-oozie-4.3.0-SNAPSHOT.jar /bgdt/oozie-4.3.0/share/lib/sparkcp oozie-sharelib-spark-4.3.0-SNAPSHOT.jar /bgdt/oozie-4.3.0/share/lib/sparkcp /bgdt/hive-2.1.1/conf/hive-site.xml /bgdt/oozie-4.3.0/share/lib/sparkcp /bgdt/install_src/mysql-connector-java-5.1.39-bin.jar /bgdt/oozie-4.3.0/share/lib/spark 在HDFS上建立sharelib相關目錄 12hadoop fs -mkdir -p /user/hadoop/share/libhadoop fs -put /bgdt/oozie-4.3.0/share/lib/* /user/hadoop/share/lib 重啟oozie 123cd /bgdt/oozie-4.3.0./bin/oozied.sh stop./bin/oozied.sh start 執行oozie提供相關的測試範例:上傳oozie examples相關設定到 hdfs 123tar -zxvf oozie-examples.tar.gzhadoop fs -mkdir -p /user/hadoop/oozie/exampleshadoop fs -put -f /bgdt/oozie-4.3.0/examples/* /user/hadoop/oozie/examples 修改job.properties 1234567891011cd /bgdt/oozie-4.3.0/examples/apps/sparkvi ./job.properties##修改以下內容nameNode=hdfs://hadoop-master:8020jobTracker=hadoop-master:8032master=local[*]queueName=defaultexamplesRoot=oozie/examplesoozie.use.system.libpath=trueoozie.wf.application.path=/user/hadoop/oozie/examples/apps/spark 執行oozie example 123456cd /bgdt/oozie-4.3.0./bin/oozie job -oozie http://localhost:11000/oozie \\-config ./examples/apps/spark/job.properties \\-run 到WebUI觀察執行情形 1http://hadoop-master:8088 Anaconda3安裝(4.2.0),選裝,僅安裝於NameNodeDownload Anaconda3-4.2.0-Linux-x86.shhttps://repo.continuum.io/archive/https://www.continuum.io/downloads Upload Anaconda3-4.2.0-Linux-x86.sh to HostInstall bzip2(if not bzip2) 1sudo yum install bzip2 -y Install Anaconda3 12345bash /opt/Anaconda3-4.2.0-Linux-x86.sh[Enter]yes/opt/anaconda3yes Modify ~/.bashrc 12# added by Anaconda3 4.2.0 installerexport PATH=&quot;/opt/anaconda3/bin:$PATH&quot; Modify spark-env.sh 12345vi /usr/local/spark/conf/spark/spark-env.shexport PYSPARK_PYTHON=/opt/anaconda3/bin/python3export PYSPARK_DRIVER_PYTHON=jupyterexport PYSPARK_DRIVER_PYTHON_OPTS=&quot;notebook --NotebookApp.open_browser=False --NotebookApp.ip=&apos;*&apos; --NotebookApp.port=8880&quot; 重新進入Putty後,啟動Hadoop相關服務 123start-dfs.sh ##start Hadoopstart-yarn.sh ##start yarnnohup /usr/local/spark/bin/pyspark &amp; 啟動完成後,打開Web Browser即可看到Jupyter Web UI 以下編寫測試範例進入ipython Web UI 透過SparkContext取得文字檔 12rdd = sc.textFile(&quot;hdfs://XXX.XXX.XXX.XXX:8020/OOO/readme.txt&quot;)rdd.count() 操作DataFrame範例 123l = [(&apos;Alice&apos;, 1)]spark.createDataFrame(l).collect()spark.createDataFrame(l, [&apos;name&apos;, &apos;age&apos;]).collect() 使用Spark SQL取得資料 12ds = spark.sql(&quot;select * from students&quot;)ds.take(5) 相關參考參考文件(Link)Spark on Ubuntu 離線安裝(第一次就上手)Spark Standalone Cluster 練習 Big Data use default port Hadoop use port 1234568020 &lt;-- HDFS Portocol port(NameNode metadata service)(fs.defaultFS)50070 &lt;-- HDFS Web Browser HTTP Port,NameNode WebUI(dfs.http.address)(Web)50075 &lt;-- DataNode WebUI to access the status, logs etc(dfs.datanode.http.address)(Web)50090 &lt;-- Secondary NameNode Port(dfs.secondary.http.address)50020 &lt;-- The datanode ipc server address and port.(dfs.datanode.ipc.address)50010 &lt;-- The datanode server address and port for data transfer.(dfs.datanode.address) Yarn cluster use port 12345678088 &lt;-- Yarn-Cluster Web UI Port(yarn.resourcemanager.webapp.address)(web)8030 &lt;-- yarn.resourcemanager.scheduler.address8031 &lt;-- yarn.resourcemanager.resource-tracker.address8032 &lt;-- Yarn resourcemnager port(yarn.resourcemanager.address)(JobTracker port for Hadoop2.X)8033 &lt;-- yarn.resourcemanager.admin.address8040 &lt;-- Address where the localizer IPC is.(yarn.nodemanager.localizer.address)8042 &lt;-- NM Webapp address.(yarn.nodemanager.webapp.address) Hive use port 1210000 &lt;-- Hive Server2 Port(Hive JDBC Use)(ENV Variable HIVE_PORT)10002 &lt;-- Hive Server2 Web UI Oozie use port 1211000 &lt;-- OOZIE Web UI Port(OOZIE_HTTP_PORT in oozie_env.sh)(Web)11001 &lt;-- The admin port Oozie server runs(OOZIE_ADMIN_PORT in oozie_env.sh)(OOZIE admin Port) Spark use port 124040 &lt;-- Spark Web UI(spark)18080 &lt;-- Spark History port(Web) MR-Job History use port 12319888 &lt;-- MR Job History port(Web)10020 &lt;-- MapReduce JobHistory server address(mapreduce.jobhistory.address)10033 &lt;-- The address of the History server admin interface.(mapreduce.jobhistory.admin.address) Hadoop Cluster相關環境HEAP SIZE設定hadoop-env.sh 12export HADOOP_HEAPSIZE=1024 ##hadoop namenode&amp;datanode heap size(default 1000)export HADOOP_JOB_HISTORYSERVER_HEAPSIZE=1024 ##yarn job history heap size(default 1000) yarn-env.sh 12export YARN_RESOURCEMANAGER_HEAPSIZE=1024 ##Resource Manager heap size(default 1000)export YARN_NODEMANAGER_HEAPSIZE=512 ##Node Manager heap size(default 1000) Hadoop Cluster相關環境啟動與停止程序整理 1234567891011121314151617Hadoop Cluster相關服務啟動順序------------------------------start-dfs.shstart-yarn.shmr-jobhistory-daemon.sh start historyserver/bgdt/spark-2.0.0/sbin/start-history-server.shnohup hive --service hiveserver2 &amp; (使用&quot;netstat -tnl | grep &apos;10000&apos;&quot;驗證10000 port是否有啟動)/bgdt/oozie-4.3.0/bin/oozied.sh startHadoop Cluster相關服務停止順序------------------------------/bgdt/oozie-4.3.0/bin/oozied.sh stopmr-jobhistory-daemon.sh stop historyserver/bgdt/spark-2.0.0/sbin/stop-history-server.shstop-yarn.shstop-dfs.shhiveserver2需要用&quot;kill -9 PID&quot;方式停止,目前沒有停止指令 Hadoop Cluster相關環境啟動程序名稱","categories":[{"name":"BigData","slug":"BigData","permalink":"http://yoursite.com/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","permalink":"http://yoursite.com/categories/BigData/Hadoop/"}],"tags":[]},{"title":"常用Linux 命令筆記(for Oracle Linux)","slug":"[Linux]Linux","date":"2016-12-07T14:05:00.000Z","updated":"2020-07-22T08:37:54.402Z","comments":true,"path":"2016/12/07/[Linux]Linux/","link":"","permalink":"http://yoursite.com/2016/12/07/[Linux]Linux/","excerpt":"","text":"常用指令新增使用者及密碼修改 12345678[root@hadoop-master]$useradd hadoop[root@hadoop-master]$passwd hadoop[root@hadoop-master]$visudo #如果須要讓 &quot;hadoop&quot;使用者暫時具有管理者(root)權限的話,須使用此指令編輯檔案......root ALL=(ALL) ALL ##在此行後面加入......hadoop ALL=(ALL) ALL...... 系統關機或重啟命令(非”Root”使用者,須加入sudo) 1234567[hadoop@hadoop-master ~]$sudo shutdown -h now ##系統立刻關機 [hadoop@hadoop-master ~]$sudo shutdown -r now ##系統立刻重新開機 [hadoop@hadoop-master ~]$sudo shutdown -h 20:30 ##系統在今天的 20:30 分關機 [hadoop@hadoop-master ~]$sudo shutdown -h +10 ##系統在 10 分鐘後關機[hadoop@hadoop-master ~]$sudo sync;sync;sync;reboot ##重新開機指令,配合寫入緩衝資料的sync指令動作[hadoop@hadoop-master ~]$sudo init 0 ##關機[hadoop@hadoop-master ~]$sudo init 6 ##重新啟動","categories":[{"name":"OS","slug":"OS","permalink":"http://yoursite.com/categories/OS/"},{"name":"Linux","slug":"OS/Linux","permalink":"http://yoursite.com/categories/OS/Linux/"}],"tags":[]}]}